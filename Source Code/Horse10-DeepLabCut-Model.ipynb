{"cells":[{"cell_type":"markdown","metadata":{"id":"RK255E7YoEIt"},"source":["# DeepLabCut Toolbox - Colab for standard (single animal) projects!\n","https://github.com/DeepLabCut/DeepLabCut\n","\n","This notebook illustrates how to use the cloud to:\n","- create a training set\n","- train a network\n","- evaluate a network\n","- create simple quality check plots\n","- analyze novel videos!\n","\n","###This notebook assumes you already have a project folder with labeled data!\n","\n","This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n","\n","This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n","\n","Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n","\n","\n","Paper: https://www.nature.com/articles/s41596-019-0176-0\n","\n","Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"]},{"cell_type":"markdown","metadata":{"id":"txoddlM8hLKm"},"source":["## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9834,"status":"ok","timestamp":1687371107791,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-180},"id":"q23BzhA6CXxu","outputId":"814b5f22-917c-4f5e-cdbf-94c072ccdeaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: deeplabcut in /usr/local/lib/python3.10/dist-packages (2.3.5)\n","Requirement already satisfied: dlclibrary in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.0.3)\n","Requirement already satisfied: filterpy>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.4.5)\n","Requirement already satisfied: ruamel.yaml>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.17.32)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.4.0)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.4.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.56.4)\n","Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.7.1)\n","Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.22.4)\n","Requirement already satisfied: pandas!=1.5.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.5.3)\n","Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.19.3)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.2.2)\n","Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.10.1)\n","Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.13.5)\n","Requirement already satisfied: tables>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.8.0)\n","Requirement already satisfied: torch<=1.12 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.12.0)\n","Requirement already satisfied: tensorpack>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.11)\n","Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (4.65.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (6.0)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (8.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.16.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (4.7.0.72)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.25.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (2.8.2)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut) (67.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut) (2022.7.1)\n","Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.15.0->deeplabcut) (0.2.7)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut) (1.4.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut) (3.1.0)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11->deeplabcut) (0.5.3)\n","Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (0.29.34)\n","Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (2.8.4)\n","Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (2.0.0)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (9.0.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (2.3.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (0.8.10)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (1.0.5)\n","Requirement already satisfied: msgpack-numpy>=0.4.4.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (0.4.8)\n","Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (23.2.1)\n","Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (5.9.5)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1.0->deeplabcut) (1.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=1.12->deeplabcut) (4.5.0)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from dlclibrary->deeplabcut) (0.15.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (3.12.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (3.4)\n"]}],"source":["#(this will take a few minutes to install all the dependences!)\n","!pip install deeplabcut"]},{"cell_type":"markdown","metadata":{"id":"25wSj6TlVclR"},"source":["**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)**"]},{"cell_type":"markdown","metadata":{"id":"cQ-nlTkri4HZ"},"source":["## Link your Google Drive (with your labeled data, or the demo data):\n","\n","### First, place your project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29005,"status":"ok","timestamp":1687371136793,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-180},"id":"KS4Q4UkR9rgG","outputId":"f942e1cc-1333-4bcb-8a5c-a3647d4dfc17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n","#(We recommend putting a copy of the github repo in your google drive if you are using the demo \"examples\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Frnj1RVDyEqs"},"source":["YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n","\n","Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":530,"status":"ok","timestamp":1681997785736,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-120},"id":"vhENAlQnFENJ","outputId":"ded5a8b1-dbe2-429a-8068-8e0b8c3224a0"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/Horses-Byron-2019-05-08/videos/']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["#Setup your project variables:\n","# PLEASE EDIT THESE:\n","\n","ProjectFolderName = 'Horses-Byron-2019-05-08'\n","VideoType = 'mp4'\n","\n","#don't edit these:\n","videofile_path = ['/content/drive/MyDrive/'+ProjectFolderName+'/videos/'] #Enter the list of videos or folder to analyze.\n","videofile_path"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9564,"status":"ok","timestamp":1687371146355,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-180},"id":"3K9Ndy1beyfG","outputId":"12765a87-2265-4dd6-b443-17b40bdd1d10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading DLC 2.3.5...\n","DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"]}],"source":["import deeplabcut"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1687371146356,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-180},"id":"o4orkg9QTHKK","outputId":"22ddd569-2305-4c44-d21c-90207c05d2db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.3.5'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["deeplabcut.__version__"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1687371156198,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-180},"id":"Z7ZlDr3wV4D1","outputId":"daca9ddf-ff00-4238-e694-0a1a42ed5883"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/config.yaml'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["#This creates a path variable that links to your google drive copy\n","#No need to edit this, as you set it up before:\n","path_config_file = '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/config.yaml'\n","path_config_file"]},{"cell_type":"markdown","metadata":{"id":"xNi9s1dboEJN"},"source":["## Create a training dataset:\n","### You must do this step inside of Colab:\n","After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n","\n","This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n","\n","Now it is the time to start training the network!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1560267,"status":"ok","timestamp":1681917516265,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"eMeUwgxPoEJP","outputId":"5ffdc527-9b7e-4983-f429-f5c486c40007","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"]},{"data":{"text/plain":["[(0.05,\n","  1,\n","  (array([3612, 7129, 4232, 3931, 7035, 7734,  980, 5796, 1060,  563, 4578,\n","          5415, 3767, 2969, 3125, 5484, 1329, 6927, 3224,  266, 6459,  693,\n","          7374, 4431, 4085, 4080, 6253, 1939, 4144,  957, 1369, 2867, 2053,\n","          6770,  354, 8055, 3709, 3409, 6542, 5997,  121,  539,  881,  810,\n","          6369, 7295, 7296, 3858, 5664, 6488, 6520,   16, 5170, 5427, 3833,\n","          4363,  138,  270,  949, 7850, 5998, 4281,  648, 2369, 4863, 7211,\n","            64, 7838, 6435, 2173, 1059, 8052, 1665, 4501, 6392, 7690, 5232,\n","          7272, 6311, 4481, 5661, 5896, 6700, 2714, 5048, 7411,  187,  595,\n","          4297, 5612, 1189, 2574, 8040, 7729, 7135, 3855, 4623,  372, 1473,\n","          2699, 7093, 5766, 8026, 2890, 3703,  899, 5882, 3945, 4881, 4075,\n","          2311, 3679, 3239, 3712, 1630, 5049, 1261, 3105,   95, 5505, 5240,\n","          4200, 7105, 6048, 3318,  233, 4724, 4326, 2922, 3735, 5332, 5920,\n","          4094,  185, 1202, 7822, 4723, 7947, 6357, 4869, 4152, 1511, 5726,\n","          5526, 5598, 2110,  420, 6734, 2774, 1235, 2193, 4223, 5303, 7957,\n","          7198,  618,  128, 4307, 6671, 6241,  440, 6095, 5977,  273, 3500,\n","          5286,  288, 4866, 4570,  737, 1163, 3323, 7610, 3356,   15, 2069,\n","          4811, 2212, 2884, 7983, 4639, 5125,  689, 6381, 7223, 4048,  419,\n","          2794, 6312, 5559, 2836, 4059,  908, 6408, 4770, 1121, 2293, 1794,\n","          6662,  571, 5034, 7147, 3647, 6901, 6104, 5682, 1130, 1997, 2779,\n","          3630, 1394,  427, 5429, 3204, 5713, 6785, 5541,  993, 5396, 6708,\n","           186,  211, 2848, 5288, 5561, 6074, 7124, 5488, 8107, 6306, 7851,\n","          7266, 8009, 5618, 4589, 1506, 7646, 7514,  529, 1709, 7444, 1905,\n","          4561,  867, 4966, 7674, 7604, 5450, 5188, 3010, 3504, 6203, 2696,\n","          3953, 5663,   67, 3111, 3281, 1469, 4861, 1532,  856, 2115, 6594,\n","          5318, 7723, 6853, 5159, 7989, 1916, 5019, 4019, 3877, 6389, 5066,\n","          4656, 6318, 1218, 2072,  423, 7824, 6189, 3777, 7256, 5137, 6608,\n","          4745, 3616, 1529, 3283, 3419,   12, 7814, 3203, 2975, 4877, 2348,\n","          7878, 2183, 1453, 7811, 6983, 2933, 3152, 5960, 4196, 1069, 1961,\n","          3223, 3885, 3074, 2006, 6781, 1503,  156, 6305, 7494, 7917,  244,\n","          6021,   86, 6362,  104, 2672, 3446, 6092, 4276, 4759, 4130, 2793,\n","          6465,  808, 2684, 4544, 5228, 4429,  954,  289, 7891, 6533, 1747,\n","          2067, 6763, 7971, 6816, 1200, 6933, 7901, 4143,  476,  180, 1872,\n","          1734, 2213, 2449, 4973, 8088, 2034, 3951, 1595, 7504,  645, 1071,\n","          7751, 2886, 2666,  117, 4971, 4294, 5358, 6588, 5594, 5377, 1313,\n","          6256, 7045, 4160, 1017, 3793, 2997, 1818, 7168, 2390, 2055,  585,\n","          3470, 2720, 4634, 4826, 1904, 5683, 2585, 5101, 4782, 2630, 1256,\n","          8089, 2569, 3337, 2486, 1964, 5302, 7986, 7218, 4539]),\n","   array([3015, 2663, 4041, ...,  451, 4664, 5312]))),\n"," (0.1,\n","  1,\n","  (array([2506, 1768, 2703,  959, 6579, 2446, 4633,  844, 2471, 1787, 5673,\n","          1129, 7521, 3463, 1151, 2078, 5253, 5650, 1467, 1633, 1826, 6766,\n","          1743, 3271, 6993, 3704, 7815, 5875,  609, 6337, 7701,  660, 7510,\n","           733, 2556, 6474, 5439, 1316, 7638, 2857, 3795, 5248, 3160, 2922,\n","          1844, 7489, 7877,  141, 4000, 1453, 7216, 6877, 3361, 4385, 1723,\n","          7928,  511, 3844, 5547, 4348, 1489, 3686, 1952, 2266,  223, 1035,\n","          7422, 5709, 1851, 7990, 4605, 1108, 3946, 2455, 4409, 7063, 1245,\n","          6545, 6779, 7183, 6000, 2115, 2935, 3718, 3207, 1432, 8051,  213,\n","          1933, 5113, 5624,    1, 6228, 2863, 6798, 2724, 3200, 6524,  930,\n","          4104, 3070, 2338, 4673, 2130,  653, 6104,  698, 6057, 7177, 1282,\n","          4496, 2941, 1589, 6308, 3504, 8026, 2978, 1189,  458, 5237, 2017,\n","          7509, 7257, 5731,   30, 2684, 4111,  681, 4928, 1062, 1979, 6098,\n","          4630, 2630, 2887, 5166, 2954, 6079,  657, 3672, 4864, 8012, 7436,\n","          7558, 2449, 3266,  473, 4264,  559, 5330, 3836, 7957, 4137, 5708,\n","          4894, 6768, 4515, 5840, 2445, 5480, 3606, 7189, 4875, 1905, 3894,\n","          2893, 8037, 1156, 7756, 5635, 7078, 7367, 1763, 5081, 2848, 1048,\n","          1157, 6534, 3041,  242, 1925, 1690, 3455,   50, 4649,  184, 1383,\n","          2485, 1119, 2248, 6584, 5657, 5785, 7328, 1849, 4766, 2325, 4921,\n","          3371, 2337, 4836, 5897, 7971, 4898,  614, 4662, 1732, 3821, 3009,\n","          2051, 3601,  351, 2701, 1999, 6003, 5463, 3556, 4554, 2508, 7230,\n","          3235, 1056, 1514, 3081, 5522, 6592, 4851, 4990, 7334, 4446,  875,\n","          6123, 3814, 5423, 4612, 4688, 3237, 2227, 2243,  494, 6205, 5607,\n","          5102, 8079, 4636, 4174, 1437, 2084, 6916, 7397, 7888, 7431, 3485,\n","          4537, 7931, 5465, 2492, 3924, 5772, 3165, 7108, 3572, 4436, 1290,\n","           577, 5345, 6328, 7933, 2920, 3525,  283, 6254,  381, 5609, 1367,\n","           482, 3957, 4380, 1677, 4917, 7394, 2943, 3903, 4950, 5694, 2585,\n","          2536, 4508, 1193, 3801, 6691, 4555, 2174, 1283, 3611, 6239, 4511,\n","          6026, 7456, 6846, 6180, 6495, 3395,   89, 8003, 3006, 5831, 4740,\n","           822,  628,  131, 5880, 5123, 5900, 5555, 6062, 7904, 7039, 1802,\n","          2836, 4848, 1830, 6157, 3464, 5089, 4697, 4193, 1186, 3392, 5492,\n","          4513, 2695, 5882, 1018,  682, 5021, 6581, 4232,  216, 5136, 2880,\n","          4749, 6001, 1935, 2399, 6091, 3837, 6441, 6310, 7601, 2652, 4050,\n","          1776, 1960, 6031, 6570, 1969, 7555, 1243, 6652, 5803, 6673, 5705,\n","          3432, 4124, 3906, 4754, 3416, 2382, 3286, 8022, 1896, 4681, 2554,\n","          4171, 1832, 1474, 6119,  777, 2167, 8038, 4929, 7788, 1622,  588,\n","           193, 2027, 2927, 7633, 6148,  444, 6099,  121, 3759, 1511, 6561,\n","          2948, 4305,  608, 1494, 7112, 4073, 6232, 7778,  688, 7120, 6436,\n","          4789, 7080,  957,  418, 3761, 6496, 6059, 3729,  658, 2219, 4938,\n","          1394, 7091, 4074, 3228, 5930, 1721, 5006, 2489, 3881, 2300, 1657,\n","          4025, 3898, 1628, 4739, 4308,  796,  249,  846, 7311, 4764, 3458,\n","          4817, 5111, 4221, 2713, 5852, 3356, 2054, 8102, 6044, 1263, 7545,\n","          6389, 6027,  108,  417, 5310, 3542, 7407, 4332, 2657, 5366, 1954,\n","          4859, 4384,  714, 6202, 5038,  979, 1789, 6086, 5614, 7304,   73,\n","          5862, 2493, 6279, 2009, 3038, 7290, 2034, 5890, 5932, 2718, 5019,\n","          5513, 4200,  600, 7781, 5804, 2222, 1021,  616,  259, 2604, 1106,\n","          1458,  142, 7901,  568, 2188, 5921, 2008, 4661, 3246, 6704, 3434,\n","          6974,  374, 8018, 6050, 1418, 2184, 6296, 7579, 1819, 6100, 7843,\n","          6603, 3590, 4796, 2470,  932, 6229, 8089, 3734, 3841, 5993, 5040,\n","          3110, 3150, 6973, 5068, 7449, 6878, 5885, 5342, 8044, 3768, 4286,\n","          6506, 1711,  759, 3575, 1762,  583, 3642, 7960, 3661, 3860, 3693,\n","          3131, 6013, 2466, 8046, 2530,   53,  345, 6264, 3119, 2040, 7765,\n","          3760, 4979, 4430, 6512, 2381, 6678, 7359, 2505, 7548, 2050,  592,\n","          6251, 2904, 6854, 4876, 5256, 6491, 7168, 7252, 5769,  279, 2335,\n","          6017, 6469, 2392, 4991, 3646,  571, 5392, 1565, 6552, 7772, 2028,\n","          8111, 4635,  515, 3378, 5224, 1650, 1811, 6382, 3054, 3048, 5781,\n","          4646, 5602, 1822, 6333, 6117, 6371, 2673,  104, 4502, 5597, 4347,\n","          6516, 3882, 7130, 2851, 1584, 2458, 7697, 4199, 7661, 3934, 3402,\n","          3905,  101, 4709, 4763, 1357,  798, 4447, 3398, 7707, 4531, 3742,\n","          4961, 1161, 4694, 7932,  189, 5400, 2158, 7211, 5938, 5144, 3792,\n","          1132,  973, 4253, 2905, 3963, 2142,  896, 7664, 3670,  684, 2480,\n","          3891, 4403, 3913, 7924, 4369, 1155, 6549, 5987, 4133,  375, 6405,\n","          3039, 7277, 5520, 5231,  829, 7353, 3052, 6250, 6305, 2333, 5714,\n","          1336, 4671, 1653, 3322, 2144, 3521, 5898, 6690, 7859,  622, 2785,\n","          2840, 4482, 2686, 3890, 3690, 1700, 2293, 7587, 3279, 5067, 6868,\n","          6640, 6415,  541,   98, 5913, 7857, 2298, 6089, 2309, 2063,  939,\n","            19, 4270,  191, 7072, 1544, 6962, 6849, 2251,  755, 4128, 2346,\n","          3947,  262, 6861, 4100, 2885, 2601,  361, 1513, 2123, 3408, 2774,\n","          2849, 3111, 1015, 5197, 4773,  330, 5751, 7042, 1055, 6960, 5937,\n","           989, 5322, 5359, 2547, 4324, 5766, 2705, 6126, 8024, 7749, 1172,\n","          2729, 3486, 6411, 1767, 1630, 6268, 1928, 4626, 1478, 1604, 4354,\n","          6865, 2988, 6351, 6744, 7947, 7245, 5746,  793, 5619, 6663, 3527,\n","          6526, 4882, 2094, 1858, 3816, 2367, 5391, 2114, 6711, 7128, 3159,\n","          7576, 3621, 7278, 5108, 4089, 3922, 1587, 4892, 7603, 6225, 2075,\n","          1311, 5615, 5281,  527, 3191, 1023, 1081, 4405]),\n","   array([6520, 4852, 1731, ..., 1212, 4082,  324]))),\n"," (0.2,\n","  1,\n","  (array([3747, 5928, 1372, ..., 4260, 4253, 5236]),\n","   array([7323, 1905, 4764, ..., 1525, 5511, 6079]))),\n"," (0.5,\n","  1,\n","  (array([2353, 5039, 2797, ..., 7300, 6172,  683]),\n","   array([2329, 6434, 4004, ..., 6663, 6821, 6183]))),\n"," (0.9,\n","  1,\n","  (array([2253, 6446,   27, ..., 7494, 4335, 1452]),\n","   array([7729, 1058, 7623,  466, 7757, 3260, 1818,  943, 4425, 3682,  927,\n","          7013, 6774, 4071,  456, 2897, 2408,  192, 5470, 2058, 2960, 3441,\n","           384,  791, 2307,  733,  612, 6764, 4203, 5135, 7458, 4102, 7806,\n","          3192, 5587,  290, 5907,  174,  938, 5290, 5826, 5148, 5569, 3583,\n","          5939, 1328, 1389,  951, 5222, 2822, 1938, 6445,  719, 4592, 2110,\n","           889, 7687, 2719, 6757, 7261, 1561, 4615, 4493, 1347, 5502, 5872,\n","           453, 1796, 3475, 3206, 5203, 5863, 3307, 1617, 4398, 6238, 2631,\n","          7668, 1002, 4161, 6804, 4880, 7541,  412, 2138, 2969,  890,  292,\n","          5804, 1450, 1476, 1687, 5333, 6316, 4644, 3804, 5440, 7797, 1826,\n","          5491, 3558, 6890, 2230, 3289, 4856, 6967, 5877, 4674, 1036, 6554,\n","          3516, 2263, 5789,   69, 7862, 7161,  870, 4813, 5106, 1316, 4903,\n","          5227,  852, 6222, 5759, 5102, 3985,  919, 2967, 2012, 1931,  601,\n","          2241, 1937, 5836, 1747, 5228, 6261, 5963, 4920, 4011, 7332, 6633,\n","          7738, 7518, 6150,  979, 7216, 5077, 5471, 6416, 5573, 1371, 2825,\n","          6180, 4635, 2361, 1529, 2397,  754, 4015,  757, 4696, 4359, 2313,\n","          2267, 3908, 5259, 4368, 5758, 7227, 6385, 1253,  158,   10, 5990,\n","          3196, 3675, 7169, 3398, 6308, 5810, 6982, 2510, 3041, 5651, 1228,\n","          6375, 3598, 1564, 5489, 1244, 6260, 3707, 1692, 4224,  803, 3060,\n","            91, 4448, 4065, 5052, 6038, 4851, 2334, 6895,  671,  110,  875,\n","          7718,  143, 7706, 1317,  455, 2047, 6153,  586, 3817, 6830, 4541,\n","          1270, 2638, 7044, 4512, 4975, 8088, 6820, 1575, 6078,  577, 2542,\n","          2526, 3338, 1336, 4026, 5090, 1427, 7087, 7598, 4662, 6001, 2023,\n","          1929, 3357, 1417, 1312, 5262, 7698,  727, 1251, 2238, 8095, 2358,\n","          4698, 3215, 6801, 4469, 5172, 7200, 6227, 2896, 7012,   59, 3873,\n","          4034, 5241, 3889, 3057, 1947, 2913, 2431, 3849,  531, 8024,   30,\n","          1044, 4697, 6992, 6016, 1569, 7126, 6861, 3575,   47, 6161, 1157,\n","          1110, 1342, 7189,  543, 7996,  883, 2583, 2815, 1589, 6529, 5561,\n","          3800,  487, 2929, 2934, 2245, 4395, 4294, 6988, 2534, 4351, 5774,\n","           354, 4553, 6759, 2720, 5154, 1527, 7559, 8047, 1300,  155, 7438,\n","          2580, 2107, 1854, 5706, 8101, 2772, 5707, 6364, 6263, 2009, 5693,\n","          3833, 7744, 7356,  689, 4438, 4566, 1233,  969, 2679, 6092, 2963,\n","          3679,   83, 3155, 4090, 7689, 1033, 5927, 3202, 5325, 1813, 3262,\n","          3217, 6937, 3087, 7106, 2995, 2466, 4822, 6292, 2628,  469, 7296,\n","          2932, 7850,  319,  327, 7364, 6647,  361, 2714, 4434, 1654, 2696,\n","          3706, 3513, 7038, 8066, 1199, 3503, 3470,  127, 5185, 5288, 7102,\n","          7160, 6949, 5252, 7769, 6008, 4446, 3784, 1502, 7379, 6304, 4155,\n","          4397, 5631,  934, 3731, 6198, 4754, 3859, 6317, 3793,  179, 5336,\n","          4695, 4843, 6386, 6749, 7560, 6939, 3144, 7527, 1820, 2792, 4296,\n","          2879,  541,  789, 7755, 2752, 3365, 6646, 5127, 6750, 1340, 7609,\n","          6579, 3919, 3223, 5679,  804, 7550, 2687, 1723, 4524,  677, 7975,\n","          3596, 3117, 7898, 1011, 1260, 2373, 3500, 1041, 1886, 7329, 1020,\n","          3035, 2403, 6408, 3782, 5903, 6929, 7605, 8055, 5533, 2294, 4681,\n","           352, 4243, 2503,  641, 5662, 7986, 6491, 6457,   82, 6374, 2378,\n","          3116, 6996, 2443,  736, 6628, 4847, 4354, 6484, 4625, 4485, 3806,\n","          3825, 3739,  359, 7268, 6917, 1373, 5652, 4054, 1995, 2486, 4646,\n","          7429, 8020, 5431, 6860, 7067,  742, 1167, 2430, 7184,  651, 4821,\n","          7388, 6950, 5065, 6731,  392, 6570, 4053, 4143,  509, 2722, 4451,\n","          1797, 4394, 7111,  168, 7873, 1131, 2033, 2726, 5655, 3900, 1361,\n","          3099, 3020,   42, 4521, 3780, 7563, 1183, 2439,  238, 4317, 6010,\n","          6894, 2462, 7557, 5098, 7555, 2946, 4962, 6997, 4807, 6531,  836,\n","          5426, 8057, 6302, 1548, 3961, 4926, 6636, 7346, 3669, 4098, 3284,\n","          5053,  537, 4251,  356, 2128, 2120, 6025, 3484, 1577, 2813, 5027,\n","          8026, 7347, 3349, 1591, 5581, 4478,  572, 1154, 5967, 7780, 6896,\n","          3630, 1489, 3114, 4818,  907, 7880, 4083, 3725, 1563, 5437, 2006,\n","          1542, 2159, 5801, 4709, 6657, 2193,  785, 6756, 3193, 5323,  333,\n","          6552, 3445,  705, 6442, 4078, 6597, 5487, 6449, 4403,  516, 2921,\n","          3164, 4720, 1150, 4835, 5910, 6972, 6209, 7325, 4568, 3795, 3149,\n","          6127, 3998, 5660, 3298, 3321, 5284,  508, 5516, 1265, 4263, 3381,\n","          2682, 3431, 2424, 1197, 2976, 4223, 6891, 6330, 2808, 4964, 2170,\n","          1944, 4477, 7077, 2648, 1205, 8045,  408, 6790, 7299, 3079,  796,\n","          3855, 3121, 4520, 4746, 3348,  306, 2841, 3552, 6905, 5505, 6454,\n","          6721, 1959, 4050, 2589, 3075, 1988, 1496, 1181, 3634, 7568, 3642,\n","          6589, 6511, 4812, 3281, 4949, 5088, 6567, 5645, 3836, 8019, 6098,\n","          5174, 6616, 3951, 4277, 2086, 4614, 2575, 2789, 2540, 4022, 5210,\n","          5351, 4466, 2619, 1026, 6181, 2524, 5433, 4599, 2309, 7620, 1865,\n","          6252, 3996, 2184, 6162, 1821,  949, 3478, 8064, 2746, 7639,  924,\n","          2395, 1610, 1766,  984, 2340, 2256, 6699, 4832, 7495, 5949, 1964,\n","          7781, 6512, 7530, 3332, 8109, 1523, 2618, 4606, 2482, 4921,  325,\n","          1948, 1713, 1804, 4981, 5975, 5398, 7121, 3095, 1262,  229, 7565,\n","           643, 5769, 4723, 2156, 6387, 6948, 4417, 1050, 8093,  659,  721,\n","          7401, 7306, 2582, 5201, 1696, 5871, 2992, 4413, 6195, 6336, 8011,\n","          2027, 7322, 1999, 4786, 5578,  894, 5938, 3878, 5005, 6185, 6119,\n","          3712, 2025, 2419, 6311, 6049, 7766, 1148, 3392, 3426, 5171, 1609,\n","          6405, 5969,  691, 4961, 5040, 2512,   22, 2537,  566])))]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Note: if you are using the demo data (i.e. examples/Reaching-Mackenzie-2018-08-30/), first delete the folder called dlc-models!\n","#Then, run this cell. There are many more functions you can set here, including which netowkr to use!\n","#check the docstring for full options you can do!\n","deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"]},{"cell_type":"markdown","metadata":{"id":"c4FczXGDoEJU"},"source":["## Start training:\n","This function trains the network for a specific shuffle of the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_pOvDq_2oEJW","outputId":"c128a077-9278-4b5f-f3fb-8fe295965890","executionInfo":{"status":"error","timestamp":1682270077678,"user_tz":-120,"elapsed":5326076,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting single-animal trainer\n"]},{"output_type":"stream","name":"stderr","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21]],\n"," 'all_joints_names': ['Nose',\n","                      'Eye',\n","                      'Nearknee',\n","                      'Nearfrontfetlock',\n","                      'Nearfrontfoot',\n","                      'Offknee',\n","                      'Offfrontfetlock',\n","                      'Offfrontfoot',\n","                      'Shoulder',\n","                      'Midshoulder',\n","                      'Elbow',\n","                      'Girth',\n","                      'Wither',\n","                      'Nearhindhock',\n","                      'Nearhindfetlock',\n","                      'Nearhindfoot',\n","                      'Hip',\n","                      'Stifle',\n","                      'Offhindhock',\n","                      'Offhindfetlock',\n","                      'Offhindfoot',\n","                      'Ischium'],\n"," 'alpha_r': 0.02,\n"," 'apply_prob': 0.5,\n"," 'batch_size': 1,\n"," 'contrast': {'clahe': True,\n","              'claheratio': 0.1,\n","              'histeq': True,\n","              'histeqratio': 0.1},\n"," 'convolution': {'edge': False,\n","                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n","                 'embossratio': 0.1,\n","                 'sharpen': False,\n","                 'sharpenratio': 0.3},\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_HorsesMay8/Horses_Byron5shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'decay_steps': 30000,\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1/train/snapshot-105000',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'lr_init': 0.0005,\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_HorsesMay8/Documentation_data-Horses_5shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'mirror': False,\n"," 'multi_stage': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 22,\n"," 'optimizer': 'sgd',\n"," 'pairwise_huber_loss': False,\n"," 'pairwise_predict': False,\n"," 'partaffinityfield_predict': False,\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08',\n"," 'regularize': False,\n"," 'rotation': 25,\n"," 'rotratio': 0.4,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"]},{"output_type":"stream","name":"stdout","text":["Batch Size is 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["Loading already trained DLC with backbone: resnet_50\n","Display_iters overwritten as 10\n","Save_iters overwritten as 500\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21]], 'all_joints_names': ['Nose', 'Eye', 'Nearknee', 'Nearfrontfetlock', 'Nearfrontfoot', 'Offknee', 'Offfrontfetlock', 'Offfrontfoot', 'Shoulder', 'Midshoulder', 'Elbow', 'Girth', 'Wither', 'Nearhindhock', 'Nearhindfetlock', 'Nearhindfoot', 'Hip', 'Stifle', 'Offhindhock', 'Offhindfetlock', 'Offhindfoot', 'Ischium'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_HorsesMay8/Horses_Byron5shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1/train/snapshot-105000', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_HorsesMay8/Documentation_data-Horses_5shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 22, 'pos_dist_thresh': 17, 'project_path': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n","Starting training....\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","iteration: 155610 loss: 0.0107 lr: 0.02\n","iteration: 155620 loss: 0.0088 lr: 0.02\n","iteration: 155630 loss: 0.0124 lr: 0.02\n","iteration: 155640 loss: 0.0102 lr: 0.02\n","iteration: 155650 loss: 0.0150 lr: 0.02\n","iteration: 155660 loss: 0.0113 lr: 0.02\n","iteration: 155670 loss: 0.0098 lr: 0.02\n","iteration: 155680 loss: 0.0092 lr: 0.02\n","iteration: 155690 loss: 0.0130 lr: 0.02\n","iteration: 155700 loss: 0.0095 lr: 0.02\n","iteration: 155710 loss: 0.0108 lr: 0.02\n","iteration: 155720 loss: 0.0121 lr: 0.02\n","iteration: 155730 loss: 0.0115 lr: 0.02\n","iteration: 155740 loss: 0.0097 lr: 0.02\n","iteration: 155750 loss: 0.0128 lr: 0.02\n","iteration: 155760 loss: 0.0155 lr: 0.02\n","iteration: 155770 loss: 0.0078 lr: 0.02\n","iteration: 155780 loss: 0.0125 lr: 0.02\n","iteration: 155790 loss: 0.0101 lr: 0.02\n","iteration: 155800 loss: 0.0091 lr: 0.02\n","iteration: 155810 loss: 0.0074 lr: 0.02\n","iteration: 155820 loss: 0.0129 lr: 0.02\n","iteration: 155830 loss: 0.0110 lr: 0.02\n","iteration: 155840 loss: 0.0108 lr: 0.02\n","iteration: 155850 loss: 0.0188 lr: 0.02\n","iteration: 155860 loss: 0.0093 lr: 0.02\n","iteration: 155870 loss: 0.0095 lr: 0.02\n","iteration: 155880 loss: 0.0100 lr: 0.02\n","iteration: 155890 loss: 0.0080 lr: 0.02\n","iteration: 155900 loss: 0.0092 lr: 0.02\n","iteration: 155910 loss: 0.0112 lr: 0.02\n","iteration: 155920 loss: 0.0114 lr: 0.02\n","iteration: 155930 loss: 0.0097 lr: 0.02\n","iteration: 155940 loss: 0.0141 lr: 0.02\n","iteration: 155950 loss: 0.0113 lr: 0.02\n","iteration: 155960 loss: 0.0106 lr: 0.02\n","iteration: 155970 loss: 0.0097 lr: 0.02\n","iteration: 155980 loss: 0.0117 lr: 0.02\n","iteration: 155990 loss: 0.0092 lr: 0.02\n","iteration: 156000 loss: 0.0106 lr: 0.02\n","iteration: 156010 loss: 0.0118 lr: 0.02\n","iteration: 156020 loss: 0.0097 lr: 0.02\n","iteration: 156030 loss: 0.0067 lr: 0.02\n","iteration: 156040 loss: 0.0082 lr: 0.02\n","iteration: 156050 loss: 0.0098 lr: 0.02\n","iteration: 156060 loss: 0.0094 lr: 0.02\n","iteration: 156070 loss: 0.0096 lr: 0.02\n","iteration: 156080 loss: 0.0090 lr: 0.02\n","iteration: 156090 loss: 0.0107 lr: 0.02\n","iteration: 156100 loss: 0.0112 lr: 0.02\n","iteration: 156110 loss: 0.0093 lr: 0.02\n","iteration: 156120 loss: 0.0117 lr: 0.02\n","iteration: 156130 loss: 0.0085 lr: 0.02\n","iteration: 156140 loss: 0.0093 lr: 0.02\n","iteration: 156150 loss: 0.0076 lr: 0.02\n","iteration: 156160 loss: 0.0098 lr: 0.02\n","iteration: 156170 loss: 0.0089 lr: 0.02\n","iteration: 156180 loss: 0.0152 lr: 0.02\n","iteration: 156190 loss: 0.0078 lr: 0.02\n","iteration: 156200 loss: 0.0120 lr: 0.02\n","iteration: 156210 loss: 0.0134 lr: 0.02\n","iteration: 156220 loss: 0.0079 lr: 0.02\n","iteration: 156230 loss: 0.0119 lr: 0.02\n","iteration: 156240 loss: 0.0200 lr: 0.02\n","iteration: 156250 loss: 0.0147 lr: 0.02\n","iteration: 156260 loss: 0.0110 lr: 0.02\n","iteration: 156270 loss: 0.0125 lr: 0.02\n","iteration: 156280 loss: 0.0142 lr: 0.02\n","iteration: 156290 loss: 0.0109 lr: 0.02\n","iteration: 156300 loss: 0.0129 lr: 0.02\n","iteration: 156310 loss: 0.0092 lr: 0.02\n","iteration: 156320 loss: 0.0119 lr: 0.02\n","iteration: 156330 loss: 0.0088 lr: 0.02\n","iteration: 156340 loss: 0.0122 lr: 0.02\n","iteration: 156350 loss: 0.0125 lr: 0.02\n","iteration: 156360 loss: 0.0100 lr: 0.02\n","iteration: 156370 loss: 0.0113 lr: 0.02\n","iteration: 156380 loss: 0.0132 lr: 0.02\n","iteration: 156390 loss: 0.0114 lr: 0.02\n","iteration: 156400 loss: 0.0148 lr: 0.02\n","iteration: 156410 loss: 0.0161 lr: 0.02\n","iteration: 156420 loss: 0.0103 lr: 0.02\n","iteration: 156430 loss: 0.0115 lr: 0.02\n","iteration: 156440 loss: 0.0112 lr: 0.02\n","iteration: 156450 loss: 0.0086 lr: 0.02\n","iteration: 156460 loss: 0.0098 lr: 0.02\n","iteration: 156470 loss: 0.0088 lr: 0.02\n","iteration: 156480 loss: 0.0095 lr: 0.02\n","iteration: 156490 loss: 0.0090 lr: 0.02\n","iteration: 156500 loss: 0.0079 lr: 0.02\n","iteration: 156510 loss: 0.0117 lr: 0.02\n","iteration: 156520 loss: 0.0094 lr: 0.02\n","iteration: 156530 loss: 0.0081 lr: 0.02\n","iteration: 156540 loss: 0.0099 lr: 0.02\n","iteration: 156550 loss: 0.0092 lr: 0.02\n","iteration: 156560 loss: 0.0085 lr: 0.02\n","iteration: 156570 loss: 0.0076 lr: 0.02\n","iteration: 156580 loss: 0.0112 lr: 0.02\n","iteration: 156590 loss: 0.0134 lr: 0.02\n","iteration: 156600 loss: 0.0126 lr: 0.02\n","iteration: 156610 loss: 0.0099 lr: 0.02\n","iteration: 156620 loss: 0.0117 lr: 0.02\n","iteration: 156630 loss: 0.0104 lr: 0.02\n","iteration: 156640 loss: 0.0099 lr: 0.02\n","iteration: 156650 loss: 0.0082 lr: 0.02\n","iteration: 156660 loss: 0.0089 lr: 0.02\n","iteration: 156670 loss: 0.0095 lr: 0.02\n","iteration: 156680 loss: 0.0099 lr: 0.02\n","iteration: 156690 loss: 0.0130 lr: 0.02\n","iteration: 156700 loss: 0.0109 lr: 0.02\n","iteration: 156710 loss: 0.0090 lr: 0.02\n","iteration: 156720 loss: 0.0099 lr: 0.02\n","iteration: 156730 loss: 0.0162 lr: 0.02\n","iteration: 156740 loss: 0.0102 lr: 0.02\n","iteration: 156750 loss: 0.0103 lr: 0.02\n","iteration: 156760 loss: 0.0080 lr: 0.02\n","iteration: 156770 loss: 0.0090 lr: 0.02\n","iteration: 156780 loss: 0.0130 lr: 0.02\n","iteration: 156790 loss: 0.0085 lr: 0.02\n","iteration: 156800 loss: 0.0107 lr: 0.02\n","iteration: 156810 loss: 0.0098 lr: 0.02\n","iteration: 156820 loss: 0.0119 lr: 0.02\n","iteration: 156830 loss: 0.0170 lr: 0.02\n","iteration: 156840 loss: 0.0092 lr: 0.02\n","iteration: 156850 loss: 0.0089 lr: 0.02\n","iteration: 156860 loss: 0.0078 lr: 0.02\n","iteration: 156870 loss: 0.0087 lr: 0.02\n","iteration: 156880 loss: 0.0118 lr: 0.02\n","iteration: 156890 loss: 0.0071 lr: 0.02\n","iteration: 156900 loss: 0.0107 lr: 0.02\n","iteration: 156910 loss: 0.0085 lr: 0.02\n","iteration: 156920 loss: 0.0094 lr: 0.02\n","iteration: 156930 loss: 0.0106 lr: 0.02\n","iteration: 156940 loss: 0.0074 lr: 0.02\n","iteration: 156950 loss: 0.0085 lr: 0.02\n","iteration: 156960 loss: 0.0075 lr: 0.02\n","iteration: 156970 loss: 0.0079 lr: 0.02\n","iteration: 156980 loss: 0.0137 lr: 0.02\n","iteration: 156990 loss: 0.0132 lr: 0.02\n","iteration: 157000 loss: 0.0098 lr: 0.02\n","iteration: 157010 loss: 0.0102 lr: 0.02\n","iteration: 157020 loss: 0.0135 lr: 0.02\n","iteration: 157030 loss: 0.0116 lr: 0.02\n","iteration: 157040 loss: 0.0093 lr: 0.02\n","iteration: 157050 loss: 0.0138 lr: 0.02\n","iteration: 157060 loss: 0.0078 lr: 0.02\n","iteration: 157070 loss: 0.0074 lr: 0.02\n","iteration: 157080 loss: 0.0084 lr: 0.02\n","iteration: 157090 loss: 0.0112 lr: 0.02\n","iteration: 157100 loss: 0.0082 lr: 0.02\n","iteration: 157110 loss: 0.0114 lr: 0.02\n","iteration: 157120 loss: 0.0074 lr: 0.02\n","iteration: 157130 loss: 0.0095 lr: 0.02\n","iteration: 157140 loss: 0.0084 lr: 0.02\n","iteration: 157150 loss: 0.0085 lr: 0.02\n","iteration: 157160 loss: 0.0097 lr: 0.02\n","iteration: 157170 loss: 0.0116 lr: 0.02\n","iteration: 157180 loss: 0.0132 lr: 0.02\n","iteration: 157190 loss: 0.0067 lr: 0.02\n","iteration: 157200 loss: 0.0103 lr: 0.02\n","iteration: 157210 loss: 0.0089 lr: 0.02\n","iteration: 157220 loss: 0.0082 lr: 0.02\n","iteration: 157230 loss: 0.0094 lr: 0.02\n","iteration: 157240 loss: 0.0088 lr: 0.02\n","iteration: 157250 loss: 0.0086 lr: 0.02\n","iteration: 157260 loss: 0.0065 lr: 0.02\n","iteration: 157270 loss: 0.0089 lr: 0.02\n","iteration: 157280 loss: 0.0084 lr: 0.02\n","iteration: 157290 loss: 0.0088 lr: 0.02\n","iteration: 157300 loss: 0.0078 lr: 0.02\n","iteration: 157310 loss: 0.0091 lr: 0.02\n","iteration: 157320 loss: 0.0086 lr: 0.02\n","iteration: 157330 loss: 0.0075 lr: 0.02\n","iteration: 157340 loss: 0.0116 lr: 0.02\n","iteration: 157350 loss: 0.0078 lr: 0.02\n","iteration: 157360 loss: 0.0077 lr: 0.02\n","iteration: 157370 loss: 0.0122 lr: 0.02\n","iteration: 157380 loss: 0.0096 lr: 0.02\n","iteration: 157390 loss: 0.0090 lr: 0.02\n","iteration: 157400 loss: 0.0139 lr: 0.02\n","iteration: 157410 loss: 0.0085 lr: 0.02\n","iteration: 157420 loss: 0.0111 lr: 0.02\n","iteration: 157430 loss: 0.0109 lr: 0.02\n","iteration: 157440 loss: 0.0144 lr: 0.02\n","iteration: 157450 loss: 0.0106 lr: 0.02\n","iteration: 157460 loss: 0.0092 lr: 0.02\n","iteration: 157470 loss: 0.0117 lr: 0.02\n","iteration: 157480 loss: 0.0074 lr: 0.02\n","iteration: 157490 loss: 0.0132 lr: 0.02\n","iteration: 157500 loss: 0.0098 lr: 0.02\n","iteration: 157510 loss: 0.0079 lr: 0.02\n","iteration: 157520 loss: 0.0093 lr: 0.02\n","iteration: 157530 loss: 0.0093 lr: 0.02\n","iteration: 157540 loss: 0.0089 lr: 0.02\n","iteration: 157550 loss: 0.0094 lr: 0.02\n","iteration: 157560 loss: 0.0105 lr: 0.02\n","iteration: 157570 loss: 0.0095 lr: 0.02\n","iteration: 157580 loss: 0.0106 lr: 0.02\n","iteration: 157590 loss: 0.0101 lr: 0.02\n","iteration: 157600 loss: 0.0086 lr: 0.02\n","iteration: 157610 loss: 0.0095 lr: 0.02\n","iteration: 157620 loss: 0.0106 lr: 0.02\n","iteration: 157630 loss: 0.0082 lr: 0.02\n","iteration: 157640 loss: 0.0077 lr: 0.02\n","iteration: 157650 loss: 0.0066 lr: 0.02\n","iteration: 157660 loss: 0.0106 lr: 0.02\n","iteration: 157670 loss: 0.0087 lr: 0.02\n","iteration: 157680 loss: 0.0081 lr: 0.02\n","iteration: 157690 loss: 0.0136 lr: 0.02\n","iteration: 157700 loss: 0.0081 lr: 0.02\n","iteration: 157710 loss: 0.0116 lr: 0.02\n","iteration: 157720 loss: 0.0141 lr: 0.02\n","iteration: 157730 loss: 0.0088 lr: 0.02\n","iteration: 157740 loss: 0.0123 lr: 0.02\n","iteration: 157750 loss: 0.0087 lr: 0.02\n","iteration: 157760 loss: 0.0115 lr: 0.02\n","iteration: 157770 loss: 0.0093 lr: 0.02\n","iteration: 157780 loss: 0.0107 lr: 0.02\n","iteration: 157790 loss: 0.0106 lr: 0.02\n","iteration: 157800 loss: 0.0113 lr: 0.02\n","iteration: 157810 loss: 0.0131 lr: 0.02\n","iteration: 157820 loss: 0.0102 lr: 0.02\n","iteration: 157830 loss: 0.0103 lr: 0.02\n","iteration: 157840 loss: 0.0117 lr: 0.02\n","iteration: 157850 loss: 0.0124 lr: 0.02\n","iteration: 157860 loss: 0.0126 lr: 0.02\n","iteration: 157870 loss: 0.0085 lr: 0.02\n","iteration: 157880 loss: 0.0105 lr: 0.02\n","iteration: 157890 loss: 0.0094 lr: 0.02\n","iteration: 157900 loss: 0.0104 lr: 0.02\n","iteration: 157910 loss: 0.0137 lr: 0.02\n","iteration: 157920 loss: 0.0116 lr: 0.02\n","iteration: 157930 loss: 0.0082 lr: 0.02\n","iteration: 157940 loss: 0.0099 lr: 0.02\n","iteration: 157950 loss: 0.0101 lr: 0.02\n","iteration: 157960 loss: 0.0076 lr: 0.02\n","iteration: 157970 loss: 0.0116 lr: 0.02\n","iteration: 157980 loss: 0.0099 lr: 0.02\n","iteration: 157990 loss: 0.0091 lr: 0.02\n","iteration: 158000 loss: 0.0158 lr: 0.02\n","iteration: 158010 loss: 0.0201 lr: 0.02\n","iteration: 158020 loss: 0.0110 lr: 0.02\n","iteration: 158030 loss: 0.0143 lr: 0.02\n","iteration: 158040 loss: 0.0127 lr: 0.02\n","iteration: 158050 loss: 0.0106 lr: 0.02\n","iteration: 158060 loss: 0.0123 lr: 0.02\n","iteration: 158070 loss: 0.0115 lr: 0.02\n","iteration: 158080 loss: 0.0126 lr: 0.02\n","iteration: 158090 loss: 0.0158 lr: 0.02\n","iteration: 158100 loss: 0.0197 lr: 0.02\n","iteration: 158110 loss: 0.0176 lr: 0.02\n","iteration: 158120 loss: 0.0124 lr: 0.02\n","iteration: 158130 loss: 0.0104 lr: 0.02\n","iteration: 158140 loss: 0.0117 lr: 0.02\n","iteration: 158150 loss: 0.0125 lr: 0.02\n","iteration: 158160 loss: 0.0110 lr: 0.02\n","iteration: 158170 loss: 0.0081 lr: 0.02\n","iteration: 158180 loss: 0.0088 lr: 0.02\n","iteration: 158190 loss: 0.0093 lr: 0.02\n","iteration: 158200 loss: 0.0092 lr: 0.02\n","iteration: 158210 loss: 0.0099 lr: 0.02\n","iteration: 158220 loss: 0.0131 lr: 0.02\n","iteration: 158230 loss: 0.0102 lr: 0.02\n","iteration: 158240 loss: 0.0121 lr: 0.02\n","iteration: 158250 loss: 0.0092 lr: 0.02\n","iteration: 158260 loss: 0.0110 lr: 0.02\n","iteration: 158270 loss: 0.0089 lr: 0.02\n","iteration: 158280 loss: 0.0101 lr: 0.02\n","iteration: 158290 loss: 0.0102 lr: 0.02\n","iteration: 158300 loss: 0.0096 lr: 0.02\n","iteration: 158310 loss: 0.0101 lr: 0.02\n","iteration: 158320 loss: 0.0097 lr: 0.02\n","iteration: 158330 loss: 0.0101 lr: 0.02\n","iteration: 158340 loss: 0.0092 lr: 0.02\n","iteration: 158350 loss: 0.0132 lr: 0.02\n","iteration: 158360 loss: 0.0098 lr: 0.02\n","iteration: 158370 loss: 0.0086 lr: 0.02\n","iteration: 158380 loss: 0.0092 lr: 0.02\n","iteration: 158390 loss: 0.0100 lr: 0.02\n","iteration: 158400 loss: 0.0105 lr: 0.02\n","iteration: 158410 loss: 0.0089 lr: 0.02\n","iteration: 158420 loss: 0.0138 lr: 0.02\n","iteration: 158430 loss: 0.0105 lr: 0.02\n","iteration: 158440 loss: 0.0095 lr: 0.02\n","iteration: 158450 loss: 0.0101 lr: 0.02\n","iteration: 158460 loss: 0.0126 lr: 0.02\n","iteration: 158470 loss: 0.0115 lr: 0.02\n","iteration: 158480 loss: 0.0097 lr: 0.02\n","iteration: 158490 loss: 0.0135 lr: 0.02\n","iteration: 158500 loss: 0.0104 lr: 0.02\n","iteration: 158510 loss: 0.0154 lr: 0.02\n","iteration: 158520 loss: 0.0100 lr: 0.02\n","iteration: 158530 loss: 0.0093 lr: 0.02\n","iteration: 158540 loss: 0.0104 lr: 0.02\n","iteration: 158550 loss: 0.0083 lr: 0.02\n","iteration: 158560 loss: 0.0134 lr: 0.02\n","iteration: 158570 loss: 0.0158 lr: 0.02\n","iteration: 158580 loss: 0.0089 lr: 0.02\n","iteration: 158590 loss: 0.0094 lr: 0.02\n","iteration: 158600 loss: 0.0123 lr: 0.02\n","iteration: 158610 loss: 0.0119 lr: 0.02\n","iteration: 158620 loss: 0.0105 lr: 0.02\n","iteration: 158630 loss: 0.0099 lr: 0.02\n","iteration: 158640 loss: 0.0111 lr: 0.02\n","iteration: 158650 loss: 0.0086 lr: 0.02\n","iteration: 158660 loss: 0.0126 lr: 0.02\n","iteration: 158670 loss: 0.0119 lr: 0.02\n","iteration: 158680 loss: 0.0071 lr: 0.02\n","iteration: 158690 loss: 0.0127 lr: 0.02\n","iteration: 158700 loss: 0.0146 lr: 0.02\n","iteration: 158710 loss: 0.0109 lr: 0.02\n","iteration: 158720 loss: 0.0097 lr: 0.02\n","iteration: 158730 loss: 0.0132 lr: 0.02\n","iteration: 158740 loss: 0.0171 lr: 0.02\n","iteration: 158750 loss: 0.0132 lr: 0.02\n","iteration: 158760 loss: 0.0111 lr: 0.02\n","iteration: 158770 loss: 0.0141 lr: 0.02\n","iteration: 158780 loss: 0.0113 lr: 0.02\n","iteration: 158790 loss: 0.0135 lr: 0.02\n","iteration: 158800 loss: 0.0147 lr: 0.02\n","iteration: 158810 loss: 0.0083 lr: 0.02\n","iteration: 158820 loss: 0.0096 lr: 0.02\n","iteration: 158830 loss: 0.0130 lr: 0.02\n","iteration: 158840 loss: 0.0078 lr: 0.02\n","iteration: 158850 loss: 0.0103 lr: 0.02\n","iteration: 158860 loss: 0.0109 lr: 0.02\n","iteration: 158870 loss: 0.0109 lr: 0.02\n","iteration: 158880 loss: 0.0106 lr: 0.02\n","iteration: 158890 loss: 0.0097 lr: 0.02\n","iteration: 158900 loss: 0.0087 lr: 0.02\n","iteration: 158910 loss: 0.0118 lr: 0.02\n","iteration: 158920 loss: 0.0099 lr: 0.02\n","iteration: 158930 loss: 0.0076 lr: 0.02\n","iteration: 158940 loss: 0.0101 lr: 0.02\n","iteration: 158950 loss: 0.0088 lr: 0.02\n","iteration: 158960 loss: 0.0119 lr: 0.02\n","iteration: 158970 loss: 0.0093 lr: 0.02\n","iteration: 158980 loss: 0.0096 lr: 0.02\n","iteration: 158990 loss: 0.0094 lr: 0.02\n","iteration: 159000 loss: 0.0122 lr: 0.02\n","iteration: 159010 loss: 0.0122 lr: 0.02\n","iteration: 159020 loss: 0.0083 lr: 0.02\n","iteration: 159030 loss: 0.0078 lr: 0.02\n","iteration: 159040 loss: 0.0104 lr: 0.02\n","iteration: 159050 loss: 0.0118 lr: 0.02\n","iteration: 159060 loss: 0.0093 lr: 0.02\n","iteration: 159070 loss: 0.0075 lr: 0.02\n","iteration: 159080 loss: 0.0112 lr: 0.02\n","iteration: 159090 loss: 0.0093 lr: 0.02\n","iteration: 159100 loss: 0.0136 lr: 0.02\n","iteration: 159110 loss: 0.0065 lr: 0.02\n","iteration: 159120 loss: 0.0095 lr: 0.02\n","iteration: 159130 loss: 0.0081 lr: 0.02\n","iteration: 159140 loss: 0.0102 lr: 0.02\n","iteration: 159150 loss: 0.0099 lr: 0.02\n","iteration: 159160 loss: 0.0118 lr: 0.02\n","iteration: 159170 loss: 0.0101 lr: 0.02\n","iteration: 159180 loss: 0.0125 lr: 0.02\n","iteration: 159190 loss: 0.0102 lr: 0.02\n","iteration: 159200 loss: 0.0104 lr: 0.02\n","iteration: 159210 loss: 0.0151 lr: 0.02\n","iteration: 159220 loss: 0.0104 lr: 0.02\n","iteration: 159230 loss: 0.0093 lr: 0.02\n","iteration: 159240 loss: 0.0104 lr: 0.02\n","iteration: 159250 loss: 0.0100 lr: 0.02\n","iteration: 159260 loss: 0.0105 lr: 0.02\n","iteration: 159270 loss: 0.0098 lr: 0.02\n","iteration: 159280 loss: 0.0131 lr: 0.02\n","iteration: 159290 loss: 0.0095 lr: 0.02\n","iteration: 159300 loss: 0.0120 lr: 0.02\n","iteration: 159310 loss: 0.0112 lr: 0.02\n","iteration: 159320 loss: 0.0095 lr: 0.02\n","iteration: 159330 loss: 0.0102 lr: 0.02\n","iteration: 159340 loss: 0.0087 lr: 0.02\n","iteration: 159350 loss: 0.0123 lr: 0.02\n","iteration: 159360 loss: 0.0124 lr: 0.02\n","iteration: 159370 loss: 0.0095 lr: 0.02\n","iteration: 159380 loss: 0.0084 lr: 0.02\n","iteration: 159390 loss: 0.0141 lr: 0.02\n","iteration: 159400 loss: 0.0119 lr: 0.02\n","iteration: 159410 loss: 0.0108 lr: 0.02\n","iteration: 159420 loss: 0.0112 lr: 0.02\n","iteration: 159430 loss: 0.0109 lr: 0.02\n","iteration: 159440 loss: 0.0097 lr: 0.02\n","iteration: 159450 loss: 0.0107 lr: 0.02\n","iteration: 159460 loss: 0.0119 lr: 0.02\n","iteration: 159470 loss: 0.0126 lr: 0.02\n","iteration: 159480 loss: 0.0120 lr: 0.02\n","iteration: 159490 loss: 0.0104 lr: 0.02\n","iteration: 159500 loss: 0.0086 lr: 0.02\n","iteration: 159510 loss: 0.0100 lr: 0.02\n","iteration: 159520 loss: 0.0086 lr: 0.02\n","iteration: 159530 loss: 0.0146 lr: 0.02\n","iteration: 159540 loss: 0.0092 lr: 0.02\n","iteration: 159550 loss: 0.0106 lr: 0.02\n","iteration: 159560 loss: 0.0100 lr: 0.02\n","iteration: 159570 loss: 0.0107 lr: 0.02\n","iteration: 159580 loss: 0.0102 lr: 0.02\n","iteration: 159590 loss: 0.0110 lr: 0.02\n","iteration: 159600 loss: 0.0106 lr: 0.02\n","iteration: 159610 loss: 0.0097 lr: 0.02\n","iteration: 159620 loss: 0.0133 lr: 0.02\n","iteration: 159630 loss: 0.0097 lr: 0.02\n","iteration: 159640 loss: 0.0099 lr: 0.02\n","iteration: 159650 loss: 0.0127 lr: 0.02\n","iteration: 159660 loss: 0.0124 lr: 0.02\n","iteration: 159670 loss: 0.0084 lr: 0.02\n","iteration: 159680 loss: 0.0123 lr: 0.02\n","iteration: 159690 loss: 0.0125 lr: 0.02\n","iteration: 159700 loss: 0.0099 lr: 0.02\n","iteration: 159710 loss: 0.0084 lr: 0.02\n","iteration: 159720 loss: 0.0120 lr: 0.02\n","iteration: 159730 loss: 0.0135 lr: 0.02\n","iteration: 159740 loss: 0.0093 lr: 0.02\n","iteration: 159750 loss: 0.0124 lr: 0.02\n","iteration: 159760 loss: 0.0076 lr: 0.02\n","iteration: 159770 loss: 0.0090 lr: 0.02\n","iteration: 159780 loss: 0.0100 lr: 0.02\n","iteration: 159790 loss: 0.0074 lr: 0.02\n","iteration: 159800 loss: 0.0092 lr: 0.02\n","iteration: 159810 loss: 0.0090 lr: 0.02\n","iteration: 159820 loss: 0.0105 lr: 0.02\n","iteration: 159830 loss: 0.0101 lr: 0.02\n","iteration: 159840 loss: 0.0108 lr: 0.02\n","iteration: 159850 loss: 0.0115 lr: 0.02\n","iteration: 159860 loss: 0.0127 lr: 0.02\n","iteration: 159870 loss: 0.0105 lr: 0.02\n","iteration: 159880 loss: 0.0075 lr: 0.02\n","iteration: 159890 loss: 0.0094 lr: 0.02\n","iteration: 159900 loss: 0.0107 lr: 0.02\n","iteration: 159910 loss: 0.0141 lr: 0.02\n","iteration: 159920 loss: 0.0127 lr: 0.02\n","iteration: 159930 loss: 0.0125 lr: 0.02\n","iteration: 159940 loss: 0.0090 lr: 0.02\n","iteration: 159950 loss: 0.0090 lr: 0.02\n","iteration: 159960 loss: 0.0094 lr: 0.02\n","iteration: 159970 loss: 0.0116 lr: 0.02\n","iteration: 159980 loss: 0.0096 lr: 0.02\n","iteration: 159990 loss: 0.0095 lr: 0.02\n","iteration: 160000 loss: 0.0129 lr: 0.02\n","iteration: 160010 loss: 0.0106 lr: 0.02\n","iteration: 160020 loss: 0.0080 lr: 0.02\n","iteration: 160030 loss: 0.0157 lr: 0.02\n","iteration: 160040 loss: 0.0088 lr: 0.02\n","iteration: 160050 loss: 0.0078 lr: 0.02\n","iteration: 160060 loss: 0.0170 lr: 0.02\n","iteration: 160070 loss: 0.0110 lr: 0.02\n","iteration: 160080 loss: 0.0078 lr: 0.02\n","iteration: 160090 loss: 0.0103 lr: 0.02\n","iteration: 160100 loss: 0.0121 lr: 0.02\n","iteration: 160110 loss: 0.0106 lr: 0.02\n","iteration: 160120 loss: 0.0096 lr: 0.02\n","iteration: 160130 loss: 0.0074 lr: 0.02\n","iteration: 160140 loss: 0.0133 lr: 0.02\n","iteration: 160150 loss: 0.0089 lr: 0.02\n","iteration: 160160 loss: 0.0096 lr: 0.02\n","iteration: 160170 loss: 0.0099 lr: 0.02\n","iteration: 160180 loss: 0.0091 lr: 0.02\n","iteration: 160190 loss: 0.0096 lr: 0.02\n","iteration: 160200 loss: 0.0113 lr: 0.02\n","iteration: 160210 loss: 0.0091 lr: 0.02\n","iteration: 160220 loss: 0.0125 lr: 0.02\n","iteration: 160230 loss: 0.0134 lr: 0.02\n","iteration: 160240 loss: 0.0109 lr: 0.02\n","iteration: 160250 loss: 0.0123 lr: 0.02\n","iteration: 160260 loss: 0.0088 lr: 0.02\n","iteration: 160270 loss: 0.0124 lr: 0.02\n","iteration: 160280 loss: 0.0111 lr: 0.02\n","iteration: 160290 loss: 0.0114 lr: 0.02\n","iteration: 160300 loss: 0.0120 lr: 0.02\n","iteration: 160310 loss: 0.0064 lr: 0.02\n","iteration: 160320 loss: 0.0148 lr: 0.02\n","iteration: 160330 loss: 0.0073 lr: 0.02\n","iteration: 160340 loss: 0.0178 lr: 0.02\n","iteration: 160350 loss: 0.0148 lr: 0.02\n","iteration: 160360 loss: 0.0119 lr: 0.02\n","iteration: 160370 loss: 0.0090 lr: 0.02\n","iteration: 160380 loss: 0.0096 lr: 0.02\n","iteration: 160390 loss: 0.0113 lr: 0.02\n","iteration: 160400 loss: 0.0104 lr: 0.02\n","iteration: 160410 loss: 0.0092 lr: 0.02\n","iteration: 160420 loss: 0.0135 lr: 0.02\n","iteration: 160430 loss: 0.0107 lr: 0.02\n","iteration: 160440 loss: 0.0140 lr: 0.02\n","iteration: 160450 loss: 0.0112 lr: 0.02\n","iteration: 160460 loss: 0.0132 lr: 0.02\n","iteration: 160470 loss: 0.0143 lr: 0.02\n","iteration: 160480 loss: 0.0093 lr: 0.02\n","iteration: 160490 loss: 0.0144 lr: 0.02\n","iteration: 160500 loss: 0.0123 lr: 0.02\n","iteration: 160510 loss: 0.0133 lr: 0.02\n","iteration: 160520 loss: 0.0117 lr: 0.02\n","iteration: 160530 loss: 0.0087 lr: 0.02\n","iteration: 160540 loss: 0.0113 lr: 0.02\n","iteration: 160550 loss: 0.0075 lr: 0.02\n","iteration: 160560 loss: 0.0105 lr: 0.02\n","iteration: 160570 loss: 0.0120 lr: 0.02\n","iteration: 160580 loss: 0.0088 lr: 0.02\n","iteration: 160590 loss: 0.0090 lr: 0.02\n","iteration: 160600 loss: 0.0097 lr: 0.02\n","iteration: 160610 loss: 0.0133 lr: 0.02\n","iteration: 160620 loss: 0.0091 lr: 0.02\n","iteration: 160630 loss: 0.0115 lr: 0.02\n","iteration: 160640 loss: 0.0097 lr: 0.02\n","iteration: 160650 loss: 0.0099 lr: 0.02\n","iteration: 160660 loss: 0.0078 lr: 0.02\n","iteration: 160670 loss: 0.0127 lr: 0.02\n","iteration: 160680 loss: 0.0110 lr: 0.02\n","iteration: 160690 loss: 0.0110 lr: 0.02\n","iteration: 160700 loss: 0.0104 lr: 0.02\n","iteration: 160710 loss: 0.0119 lr: 0.02\n","iteration: 160720 loss: 0.0086 lr: 0.02\n","iteration: 160730 loss: 0.0076 lr: 0.02\n","iteration: 160740 loss: 0.0092 lr: 0.02\n","iteration: 160750 loss: 0.0094 lr: 0.02\n","iteration: 160760 loss: 0.0099 lr: 0.02\n","iteration: 160770 loss: 0.0071 lr: 0.02\n","iteration: 160780 loss: 0.0109 lr: 0.02\n","iteration: 160790 loss: 0.0106 lr: 0.02\n","iteration: 160800 loss: 0.0095 lr: 0.02\n","iteration: 160810 loss: 0.0100 lr: 0.02\n","iteration: 160820 loss: 0.0088 lr: 0.02\n","iteration: 160830 loss: 0.0123 lr: 0.02\n","iteration: 160840 loss: 0.0103 lr: 0.02\n","iteration: 160850 loss: 0.0093 lr: 0.02\n","iteration: 160860 loss: 0.0128 lr: 0.02\n","iteration: 160870 loss: 0.0091 lr: 0.02\n","iteration: 160880 loss: 0.0102 lr: 0.02\n","iteration: 160890 loss: 0.0113 lr: 0.02\n","iteration: 160900 loss: 0.0091 lr: 0.02\n","iteration: 160910 loss: 0.0121 lr: 0.02\n","iteration: 160920 loss: 0.0123 lr: 0.02\n","iteration: 160930 loss: 0.0138 lr: 0.02\n","iteration: 160940 loss: 0.0098 lr: 0.02\n","iteration: 160950 loss: 0.0083 lr: 0.02\n","iteration: 160960 loss: 0.0086 lr: 0.02\n","iteration: 160970 loss: 0.0115 lr: 0.02\n","iteration: 160980 loss: 0.0143 lr: 0.02\n","iteration: 160990 loss: 0.0071 lr: 0.02\n","iteration: 161000 loss: 0.0108 lr: 0.02\n","iteration: 161010 loss: 0.0158 lr: 0.02\n","iteration: 161020 loss: 0.0135 lr: 0.02\n","iteration: 161030 loss: 0.0116 lr: 0.02\n","iteration: 161040 loss: 0.0107 lr: 0.02\n","iteration: 161050 loss: 0.0075 lr: 0.02\n","iteration: 161060 loss: 0.0086 lr: 0.02\n","iteration: 161070 loss: 0.0099 lr: 0.02\n","iteration: 161080 loss: 0.0098 lr: 0.02\n","iteration: 161090 loss: 0.0106 lr: 0.02\n","iteration: 161100 loss: 0.0110 lr: 0.02\n","iteration: 161110 loss: 0.0085 lr: 0.02\n","iteration: 161120 loss: 0.0087 lr: 0.02\n","iteration: 161130 loss: 0.0112 lr: 0.02\n","iteration: 161140 loss: 0.0098 lr: 0.02\n","iteration: 161150 loss: 0.0091 lr: 0.02\n","iteration: 161160 loss: 0.0106 lr: 0.02\n","iteration: 161170 loss: 0.0114 lr: 0.02\n","iteration: 161180 loss: 0.0124 lr: 0.02\n","iteration: 161190 loss: 0.0100 lr: 0.02\n","iteration: 161200 loss: 0.0093 lr: 0.02\n","iteration: 161210 loss: 0.0067 lr: 0.02\n","iteration: 161220 loss: 0.0131 lr: 0.02\n","iteration: 161230 loss: 0.0093 lr: 0.02\n","iteration: 161240 loss: 0.0079 lr: 0.02\n","iteration: 161250 loss: 0.0112 lr: 0.02\n","iteration: 161260 loss: 0.0106 lr: 0.02\n","iteration: 161270 loss: 0.0137 lr: 0.02\n","iteration: 161280 loss: 0.0102 lr: 0.02\n","iteration: 161290 loss: 0.0094 lr: 0.02\n","iteration: 161300 loss: 0.0077 lr: 0.02\n","iteration: 161310 loss: 0.0129 lr: 0.02\n","iteration: 161320 loss: 0.0113 lr: 0.02\n","iteration: 161330 loss: 0.0083 lr: 0.02\n","iteration: 161340 loss: 0.0078 lr: 0.02\n","iteration: 161350 loss: 0.0133 lr: 0.02\n","iteration: 161360 loss: 0.0129 lr: 0.02\n","iteration: 161370 loss: 0.0082 lr: 0.02\n","iteration: 161380 loss: 0.0090 lr: 0.02\n","iteration: 161390 loss: 0.0097 lr: 0.02\n","iteration: 161400 loss: 0.0089 lr: 0.02\n","iteration: 161410 loss: 0.0103 lr: 0.02\n","iteration: 161420 loss: 0.0142 lr: 0.02\n","iteration: 161430 loss: 0.0106 lr: 0.02\n","iteration: 161440 loss: 0.0118 lr: 0.02\n","iteration: 161450 loss: 0.0075 lr: 0.02\n","iteration: 161460 loss: 0.0081 lr: 0.02\n","iteration: 161470 loss: 0.0082 lr: 0.02\n","iteration: 161480 loss: 0.0082 lr: 0.02\n","iteration: 161490 loss: 0.0138 lr: 0.02\n","iteration: 161500 loss: 0.0081 lr: 0.02\n","iteration: 161510 loss: 0.0113 lr: 0.02\n","iteration: 161520 loss: 0.0086 lr: 0.02\n","iteration: 161530 loss: 0.0082 lr: 0.02\n","iteration: 161540 loss: 0.0101 lr: 0.02\n","iteration: 161550 loss: 0.0100 lr: 0.02\n","iteration: 161560 loss: 0.0112 lr: 0.02\n","iteration: 161570 loss: 0.0103 lr: 0.02\n","iteration: 161580 loss: 0.0086 lr: 0.02\n","iteration: 161590 loss: 0.0106 lr: 0.02\n","iteration: 161600 loss: 0.0098 lr: 0.02\n","iteration: 161610 loss: 0.0097 lr: 0.02\n","iteration: 161620 loss: 0.0105 lr: 0.02\n","iteration: 161630 loss: 0.0091 lr: 0.02\n","iteration: 161640 loss: 0.0123 lr: 0.02\n","iteration: 161650 loss: 0.0086 lr: 0.02\n","iteration: 161660 loss: 0.0126 lr: 0.02\n","iteration: 161670 loss: 0.0106 lr: 0.02\n","iteration: 161680 loss: 0.0096 lr: 0.02\n","iteration: 161690 loss: 0.0120 lr: 0.02\n","iteration: 161700 loss: 0.0084 lr: 0.02\n","iteration: 161710 loss: 0.0146 lr: 0.02\n","iteration: 161720 loss: 0.0086 lr: 0.02\n","iteration: 161730 loss: 0.0130 lr: 0.02\n","iteration: 161740 loss: 0.0086 lr: 0.02\n","iteration: 161750 loss: 0.0107 lr: 0.02\n","iteration: 161760 loss: 0.0108 lr: 0.02\n","iteration: 161770 loss: 0.0139 lr: 0.02\n","iteration: 161780 loss: 0.0096 lr: 0.02\n","iteration: 161790 loss: 0.0102 lr: 0.02\n","iteration: 161800 loss: 0.0093 lr: 0.02\n","iteration: 161810 loss: 0.0108 lr: 0.02\n","iteration: 161820 loss: 0.0096 lr: 0.02\n","iteration: 161830 loss: 0.0107 lr: 0.02\n","iteration: 161840 loss: 0.0097 lr: 0.02\n","iteration: 161850 loss: 0.0138 lr: 0.02\n","iteration: 161860 loss: 0.0120 lr: 0.02\n","iteration: 161870 loss: 0.0143 lr: 0.02\n","iteration: 161880 loss: 0.0087 lr: 0.02\n","iteration: 161890 loss: 0.0112 lr: 0.02\n","iteration: 161900 loss: 0.0112 lr: 0.02\n","iteration: 161910 loss: 0.0102 lr: 0.02\n","iteration: 161920 loss: 0.0114 lr: 0.02\n","iteration: 161930 loss: 0.0098 lr: 0.02\n","iteration: 161940 loss: 0.0083 lr: 0.02\n","iteration: 161950 loss: 0.0108 lr: 0.02\n","iteration: 161960 loss: 0.0081 lr: 0.02\n","iteration: 161970 loss: 0.0097 lr: 0.02\n","iteration: 161980 loss: 0.0102 lr: 0.02\n","iteration: 161990 loss: 0.0115 lr: 0.02\n","iteration: 162000 loss: 0.0137 lr: 0.02\n","iteration: 162010 loss: 0.0078 lr: 0.02\n","iteration: 162020 loss: 0.0102 lr: 0.02\n","iteration: 162030 loss: 0.0117 lr: 0.02\n","iteration: 162040 loss: 0.0100 lr: 0.02\n","iteration: 162050 loss: 0.0129 lr: 0.02\n","iteration: 162060 loss: 0.0147 lr: 0.02\n","iteration: 162070 loss: 0.0084 lr: 0.02\n","iteration: 162080 loss: 0.0071 lr: 0.02\n","iteration: 162090 loss: 0.0074 lr: 0.02\n","iteration: 162100 loss: 0.0114 lr: 0.02\n","iteration: 162110 loss: 0.0095 lr: 0.02\n","iteration: 162120 loss: 0.0085 lr: 0.02\n","iteration: 162130 loss: 0.0118 lr: 0.02\n","iteration: 162140 loss: 0.0065 lr: 0.02\n","iteration: 162150 loss: 0.0117 lr: 0.02\n","iteration: 162160 loss: 0.0077 lr: 0.02\n","iteration: 162170 loss: 0.0087 lr: 0.02\n","iteration: 162180 loss: 0.0085 lr: 0.02\n","iteration: 162190 loss: 0.0102 lr: 0.02\n","iteration: 162200 loss: 0.0088 lr: 0.02\n","iteration: 162210 loss: 0.0107 lr: 0.02\n","iteration: 162220 loss: 0.0109 lr: 0.02\n","iteration: 162230 loss: 0.0081 lr: 0.02\n","iteration: 162240 loss: 0.0086 lr: 0.02\n","iteration: 162250 loss: 0.0110 lr: 0.02\n","iteration: 162260 loss: 0.0104 lr: 0.02\n","iteration: 162270 loss: 0.0081 lr: 0.02\n","iteration: 162280 loss: 0.0102 lr: 0.02\n","iteration: 162290 loss: 0.0117 lr: 0.02\n","iteration: 162300 loss: 0.0103 lr: 0.02\n","iteration: 162310 loss: 0.0082 lr: 0.02\n","iteration: 162320 loss: 0.0086 lr: 0.02\n","iteration: 162330 loss: 0.0088 lr: 0.02\n","iteration: 162340 loss: 0.0083 lr: 0.02\n","iteration: 162350 loss: 0.0084 lr: 0.02\n","iteration: 162360 loss: 0.0120 lr: 0.02\n","iteration: 162370 loss: 0.0100 lr: 0.02\n","iteration: 162380 loss: 0.0104 lr: 0.02\n","iteration: 162390 loss: 0.0079 lr: 0.02\n","iteration: 162400 loss: 0.0066 lr: 0.02\n","iteration: 162410 loss: 0.0109 lr: 0.02\n","iteration: 162420 loss: 0.0078 lr: 0.02\n","iteration: 162430 loss: 0.0113 lr: 0.02\n","iteration: 162440 loss: 0.0081 lr: 0.02\n","iteration: 162450 loss: 0.0128 lr: 0.02\n","iteration: 162460 loss: 0.0088 lr: 0.02\n","iteration: 162470 loss: 0.0084 lr: 0.02\n","iteration: 162480 loss: 0.0102 lr: 0.02\n","iteration: 162490 loss: 0.0131 lr: 0.02\n","iteration: 162500 loss: 0.0221 lr: 0.02\n","iteration: 162510 loss: 0.0126 lr: 0.02\n","iteration: 162520 loss: 0.0145 lr: 0.02\n","iteration: 162530 loss: 0.0121 lr: 0.02\n","iteration: 162540 loss: 0.0112 lr: 0.02\n","iteration: 162550 loss: 0.0104 lr: 0.02\n","iteration: 162560 loss: 0.0097 lr: 0.02\n","iteration: 162570 loss: 0.0092 lr: 0.02\n","iteration: 162580 loss: 0.0091 lr: 0.02\n","iteration: 162590 loss: 0.0099 lr: 0.02\n","iteration: 162600 loss: 0.0137 lr: 0.02\n","iteration: 162610 loss: 0.0107 lr: 0.02\n","iteration: 162620 loss: 0.0108 lr: 0.02\n","iteration: 162630 loss: 0.0108 lr: 0.02\n","iteration: 162640 loss: 0.0116 lr: 0.02\n","iteration: 162650 loss: 0.0093 lr: 0.02\n","iteration: 162660 loss: 0.0143 lr: 0.02\n","iteration: 162670 loss: 0.0100 lr: 0.02\n","iteration: 162680 loss: 0.0113 lr: 0.02\n","iteration: 162690 loss: 0.0102 lr: 0.02\n","iteration: 162700 loss: 0.0093 lr: 0.02\n","iteration: 162710 loss: 0.0143 lr: 0.02\n","iteration: 162720 loss: 0.0139 lr: 0.02\n","iteration: 162730 loss: 0.0128 lr: 0.02\n","iteration: 162740 loss: 0.0074 lr: 0.02\n","iteration: 162750 loss: 0.0140 lr: 0.02\n","iteration: 162760 loss: 0.0092 lr: 0.02\n","iteration: 162770 loss: 0.0144 lr: 0.02\n","iteration: 162780 loss: 0.0098 lr: 0.02\n","iteration: 162790 loss: 0.0101 lr: 0.02\n","iteration: 162800 loss: 0.0091 lr: 0.02\n","iteration: 162810 loss: 0.0095 lr: 0.02\n","iteration: 162820 loss: 0.0109 lr: 0.02\n","iteration: 162830 loss: 0.0098 lr: 0.02\n","iteration: 162840 loss: 0.0092 lr: 0.02\n","iteration: 162850 loss: 0.0111 lr: 0.02\n","iteration: 162860 loss: 0.0126 lr: 0.02\n","iteration: 162870 loss: 0.0084 lr: 0.02\n","iteration: 162880 loss: 0.0146 lr: 0.02\n","iteration: 162890 loss: 0.0079 lr: 0.02\n","iteration: 162900 loss: 0.0153 lr: 0.02\n","iteration: 162910 loss: 0.0099 lr: 0.02\n","iteration: 162920 loss: 0.0088 lr: 0.02\n","iteration: 162930 loss: 0.0099 lr: 0.02\n","iteration: 162940 loss: 0.0105 lr: 0.02\n","iteration: 162950 loss: 0.0097 lr: 0.02\n","iteration: 162960 loss: 0.0086 lr: 0.02\n","iteration: 162970 loss: 0.0093 lr: 0.02\n","iteration: 162980 loss: 0.0116 lr: 0.02\n","iteration: 162990 loss: 0.0092 lr: 0.02\n","iteration: 163000 loss: 0.0092 lr: 0.02\n","iteration: 163010 loss: 0.0128 lr: 0.02\n","iteration: 163020 loss: 0.0095 lr: 0.02\n","iteration: 163030 loss: 0.0083 lr: 0.02\n","iteration: 163040 loss: 0.0098 lr: 0.02\n","iteration: 163050 loss: 0.0115 lr: 0.02\n","iteration: 163060 loss: 0.0092 lr: 0.02\n","iteration: 163070 loss: 0.0074 lr: 0.02\n","iteration: 163080 loss: 0.0125 lr: 0.02\n","iteration: 163090 loss: 0.0097 lr: 0.02\n","iteration: 163100 loss: 0.0103 lr: 0.02\n","iteration: 163110 loss: 0.0089 lr: 0.02\n","iteration: 163120 loss: 0.0108 lr: 0.02\n","iteration: 163130 loss: 0.0090 lr: 0.02\n","iteration: 163140 loss: 0.0076 lr: 0.02\n","iteration: 163150 loss: 0.0102 lr: 0.02\n","iteration: 163160 loss: 0.0086 lr: 0.02\n","iteration: 163170 loss: 0.0103 lr: 0.02\n","iteration: 163180 loss: 0.0105 lr: 0.02\n","iteration: 163190 loss: 0.0097 lr: 0.02\n","iteration: 163200 loss: 0.0099 lr: 0.02\n","iteration: 163210 loss: 0.0137 lr: 0.02\n","iteration: 163220 loss: 0.0090 lr: 0.02\n","iteration: 163230 loss: 0.0108 lr: 0.02\n","iteration: 163240 loss: 0.0133 lr: 0.02\n","iteration: 163250 loss: 0.0100 lr: 0.02\n","iteration: 163260 loss: 0.0095 lr: 0.02\n","iteration: 163270 loss: 0.0085 lr: 0.02\n","iteration: 163280 loss: 0.0101 lr: 0.02\n","iteration: 163290 loss: 0.0110 lr: 0.02\n","iteration: 163300 loss: 0.0099 lr: 0.02\n","iteration: 163310 loss: 0.0091 lr: 0.02\n","iteration: 163320 loss: 0.0086 lr: 0.02\n","iteration: 163330 loss: 0.0122 lr: 0.02\n","iteration: 163340 loss: 0.0101 lr: 0.02\n","iteration: 163350 loss: 0.0106 lr: 0.02\n","iteration: 163360 loss: 0.0092 lr: 0.02\n","iteration: 163370 loss: 0.0097 lr: 0.02\n","iteration: 163380 loss: 0.0096 lr: 0.02\n","iteration: 163390 loss: 0.0101 lr: 0.02\n","iteration: 163400 loss: 0.0099 lr: 0.02\n","iteration: 163410 loss: 0.0096 lr: 0.02\n","iteration: 163420 loss: 0.0107 lr: 0.02\n","iteration: 163430 loss: 0.0130 lr: 0.02\n","iteration: 163440 loss: 0.0091 lr: 0.02\n","iteration: 163450 loss: 0.0156 lr: 0.02\n","iteration: 163460 loss: 0.0077 lr: 0.02\n","iteration: 163470 loss: 0.0094 lr: 0.02\n","iteration: 163480 loss: 0.0097 lr: 0.02\n","iteration: 163490 loss: 0.0103 lr: 0.02\n","iteration: 163500 loss: 0.0119 lr: 0.02\n","iteration: 163510 loss: 0.0089 lr: 0.02\n","iteration: 163520 loss: 0.0080 lr: 0.02\n","iteration: 163530 loss: 0.0104 lr: 0.02\n","iteration: 163540 loss: 0.0118 lr: 0.02\n","iteration: 163550 loss: 0.0116 lr: 0.02\n","iteration: 163560 loss: 0.0219 lr: 0.02\n","iteration: 163570 loss: 0.0141 lr: 0.02\n","iteration: 163580 loss: 0.0182 lr: 0.02\n","iteration: 163590 loss: 0.0095 lr: 0.02\n","iteration: 163600 loss: 0.0083 lr: 0.02\n","iteration: 163610 loss: 0.0113 lr: 0.02\n","iteration: 163620 loss: 0.0081 lr: 0.02\n","iteration: 163630 loss: 0.0121 lr: 0.02\n","iteration: 163640 loss: 0.0115 lr: 0.02\n","iteration: 163650 loss: 0.0135 lr: 0.02\n","iteration: 163660 loss: 0.0092 lr: 0.02\n","iteration: 163670 loss: 0.0107 lr: 0.02\n","iteration: 163680 loss: 0.0087 lr: 0.02\n","iteration: 163690 loss: 0.0105 lr: 0.02\n","iteration: 163700 loss: 0.0087 lr: 0.02\n","iteration: 163710 loss: 0.0077 lr: 0.02\n","iteration: 163720 loss: 0.0125 lr: 0.02\n","iteration: 163730 loss: 0.0107 lr: 0.02\n","iteration: 163740 loss: 0.0111 lr: 0.02\n","iteration: 163750 loss: 0.0185 lr: 0.02\n","iteration: 163760 loss: 0.0097 lr: 0.02\n","iteration: 163770 loss: 0.0068 lr: 0.02\n","iteration: 163780 loss: 0.0119 lr: 0.02\n","iteration: 163790 loss: 0.0109 lr: 0.02\n","iteration: 163800 loss: 0.0076 lr: 0.02\n","iteration: 163810 loss: 0.0094 lr: 0.02\n","iteration: 163820 loss: 0.0087 lr: 0.02\n","iteration: 163830 loss: 0.0093 lr: 0.02\n","iteration: 163840 loss: 0.0079 lr: 0.02\n","iteration: 163850 loss: 0.0095 lr: 0.02\n","iteration: 163860 loss: 0.0110 lr: 0.02\n","iteration: 163870 loss: 0.0114 lr: 0.02\n","iteration: 163880 loss: 0.0096 lr: 0.02\n","iteration: 163890 loss: 0.0079 lr: 0.02\n","iteration: 163900 loss: 0.0086 lr: 0.02\n","iteration: 163910 loss: 0.0106 lr: 0.02\n","iteration: 163920 loss: 0.0086 lr: 0.02\n","iteration: 163930 loss: 0.0085 lr: 0.02\n","iteration: 163940 loss: 0.0113 lr: 0.02\n","iteration: 163950 loss: 0.0081 lr: 0.02\n","iteration: 163960 loss: 0.0166 lr: 0.02\n","iteration: 163970 loss: 0.0093 lr: 0.02\n","iteration: 163980 loss: 0.0096 lr: 0.02\n","iteration: 163990 loss: 0.0103 lr: 0.02\n","iteration: 164000 loss: 0.0085 lr: 0.02\n","iteration: 164010 loss: 0.0121 lr: 0.02\n","iteration: 164020 loss: 0.0090 lr: 0.02\n","iteration: 164030 loss: 0.0152 lr: 0.02\n","iteration: 164040 loss: 0.0092 lr: 0.02\n","iteration: 164050 loss: 0.0069 lr: 0.02\n","iteration: 164060 loss: 0.0115 lr: 0.02\n","iteration: 164070 loss: 0.0119 lr: 0.02\n","iteration: 164080 loss: 0.0117 lr: 0.02\n","iteration: 164090 loss: 0.0113 lr: 0.02\n","iteration: 164100 loss: 0.0076 lr: 0.02\n","iteration: 164110 loss: 0.0091 lr: 0.02\n","iteration: 164120 loss: 0.0122 lr: 0.02\n","iteration: 164130 loss: 0.0109 lr: 0.02\n","iteration: 164140 loss: 0.0104 lr: 0.02\n","iteration: 164150 loss: 0.0131 lr: 0.02\n","iteration: 164160 loss: 0.0084 lr: 0.02\n","iteration: 164170 loss: 0.0072 lr: 0.02\n","iteration: 164180 loss: 0.0100 lr: 0.02\n","iteration: 164190 loss: 0.0092 lr: 0.02\n","iteration: 164200 loss: 0.0083 lr: 0.02\n","iteration: 164210 loss: 0.0094 lr: 0.02\n","iteration: 164220 loss: 0.0105 lr: 0.02\n","iteration: 164230 loss: 0.0107 lr: 0.02\n","iteration: 164240 loss: 0.0107 lr: 0.02\n","iteration: 164250 loss: 0.0109 lr: 0.02\n","iteration: 164260 loss: 0.0104 lr: 0.02\n","iteration: 164270 loss: 0.0088 lr: 0.02\n","iteration: 164280 loss: 0.0067 lr: 0.02\n","iteration: 164290 loss: 0.0066 lr: 0.02\n","iteration: 164300 loss: 0.0110 lr: 0.02\n","iteration: 164310 loss: 0.0105 lr: 0.02\n","iteration: 164320 loss: 0.0185 lr: 0.02\n","iteration: 164330 loss: 0.0121 lr: 0.02\n","iteration: 164340 loss: 0.0093 lr: 0.02\n","iteration: 164350 loss: 0.0147 lr: 0.02\n","iteration: 164360 loss: 0.0119 lr: 0.02\n","iteration: 164370 loss: 0.0089 lr: 0.02\n","iteration: 164380 loss: 0.0108 lr: 0.02\n","iteration: 164390 loss: 0.0105 lr: 0.02\n","iteration: 164400 loss: 0.0122 lr: 0.02\n","iteration: 164410 loss: 0.0087 lr: 0.02\n","iteration: 164420 loss: 0.0114 lr: 0.02\n","iteration: 164430 loss: 0.0118 lr: 0.02\n","iteration: 164440 loss: 0.0104 lr: 0.02\n","iteration: 164450 loss: 0.0089 lr: 0.02\n","iteration: 164460 loss: 0.0132 lr: 0.02\n","iteration: 164470 loss: 0.0081 lr: 0.02\n","iteration: 164480 loss: 0.0087 lr: 0.02\n","iteration: 164490 loss: 0.0107 lr: 0.02\n","iteration: 164500 loss: 0.0093 lr: 0.02\n","iteration: 164510 loss: 0.0099 lr: 0.02\n","iteration: 164520 loss: 0.0093 lr: 0.02\n","iteration: 164530 loss: 0.0096 lr: 0.02\n","iteration: 164540 loss: 0.0126 lr: 0.02\n","iteration: 164550 loss: 0.0076 lr: 0.02\n","iteration: 164560 loss: 0.0093 lr: 0.02\n","iteration: 164570 loss: 0.0089 lr: 0.02\n","iteration: 164580 loss: 0.0109 lr: 0.02\n","iteration: 164590 loss: 0.0110 lr: 0.02\n","iteration: 164600 loss: 0.0080 lr: 0.02\n","iteration: 164610 loss: 0.0124 lr: 0.02\n","iteration: 164620 loss: 0.0111 lr: 0.02\n","iteration: 164630 loss: 0.0111 lr: 0.02\n","iteration: 164640 loss: 0.0082 lr: 0.02\n","iteration: 164650 loss: 0.0143 lr: 0.02\n","iteration: 164660 loss: 0.0097 lr: 0.02\n","iteration: 164670 loss: 0.0099 lr: 0.02\n","iteration: 164680 loss: 0.0099 lr: 0.02\n","iteration: 164690 loss: 0.0093 lr: 0.02\n","iteration: 164700 loss: 0.0103 lr: 0.02\n","iteration: 164710 loss: 0.0074 lr: 0.02\n","iteration: 164720 loss: 0.0097 lr: 0.02\n","iteration: 164730 loss: 0.0092 lr: 0.02\n","iteration: 164740 loss: 0.0099 lr: 0.02\n","iteration: 164750 loss: 0.0094 lr: 0.02\n","iteration: 164760 loss: 0.0081 lr: 0.02\n","iteration: 164770 loss: 0.0093 lr: 0.02\n","iteration: 164780 loss: 0.0085 lr: 0.02\n","iteration: 164790 loss: 0.0074 lr: 0.02\n","iteration: 164800 loss: 0.0096 lr: 0.02\n","iteration: 164810 loss: 0.0085 lr: 0.02\n","iteration: 164820 loss: 0.0127 lr: 0.02\n","iteration: 164830 loss: 0.0105 lr: 0.02\n","iteration: 164840 loss: 0.0074 lr: 0.02\n","iteration: 164850 loss: 0.0086 lr: 0.02\n","iteration: 164860 loss: 0.0103 lr: 0.02\n","iteration: 164870 loss: 0.0098 lr: 0.02\n","iteration: 164880 loss: 0.0101 lr: 0.02\n","iteration: 164890 loss: 0.0068 lr: 0.02\n","iteration: 164900 loss: 0.0082 lr: 0.02\n","iteration: 164910 loss: 0.0090 lr: 0.02\n","iteration: 164920 loss: 0.0106 lr: 0.02\n","iteration: 164930 loss: 0.0090 lr: 0.02\n","iteration: 164940 loss: 0.0098 lr: 0.02\n","iteration: 164950 loss: 0.0070 lr: 0.02\n","iteration: 164960 loss: 0.0092 lr: 0.02\n","iteration: 164970 loss: 0.0067 lr: 0.02\n","iteration: 164980 loss: 0.0085 lr: 0.02\n","iteration: 164990 loss: 0.0096 lr: 0.02\n","iteration: 165000 loss: 0.0163 lr: 0.02\n","iteration: 165010 loss: 0.0132 lr: 0.02\n","iteration: 165020 loss: 0.0102 lr: 0.02\n","iteration: 165030 loss: 0.0130 lr: 0.02\n","iteration: 165040 loss: 0.0093 lr: 0.02\n","iteration: 165050 loss: 0.0105 lr: 0.02\n","iteration: 165060 loss: 0.0099 lr: 0.02\n","iteration: 165070 loss: 0.0113 lr: 0.02\n","iteration: 165080 loss: 0.0115 lr: 0.02\n","iteration: 165090 loss: 0.0110 lr: 0.02\n","iteration: 165100 loss: 0.0116 lr: 0.02\n","iteration: 165110 loss: 0.0071 lr: 0.02\n","iteration: 165120 loss: 0.0108 lr: 0.02\n","iteration: 165130 loss: 0.0092 lr: 0.02\n","iteration: 165140 loss: 0.0082 lr: 0.02\n","iteration: 165150 loss: 0.0084 lr: 0.02\n","iteration: 165160 loss: 0.0067 lr: 0.02\n","iteration: 165170 loss: 0.0105 lr: 0.02\n","iteration: 165180 loss: 0.0093 lr: 0.02\n","iteration: 165190 loss: 0.0111 lr: 0.02\n","iteration: 165200 loss: 0.0098 lr: 0.02\n","iteration: 165210 loss: 0.0085 lr: 0.02\n","iteration: 165220 loss: 0.0089 lr: 0.02\n","iteration: 165230 loss: 0.0098 lr: 0.02\n","iteration: 165240 loss: 0.0094 lr: 0.02\n","iteration: 165250 loss: 0.0103 lr: 0.02\n","iteration: 165260 loss: 0.0120 lr: 0.02\n","iteration: 165270 loss: 0.0094 lr: 0.02\n","iteration: 165280 loss: 0.0093 lr: 0.02\n","iteration: 165290 loss: 0.0089 lr: 0.02\n","iteration: 165300 loss: 0.0088 lr: 0.02\n","iteration: 165310 loss: 0.0086 lr: 0.02\n","iteration: 165320 loss: 0.0096 lr: 0.02\n","iteration: 165330 loss: 0.0123 lr: 0.02\n","iteration: 165340 loss: 0.0082 lr: 0.02\n","iteration: 165350 loss: 0.0094 lr: 0.02\n","iteration: 165360 loss: 0.0083 lr: 0.02\n","iteration: 165370 loss: 0.0107 lr: 0.02\n","iteration: 165380 loss: 0.0096 lr: 0.02\n","iteration: 165390 loss: 0.0096 lr: 0.02\n","iteration: 165400 loss: 0.0136 lr: 0.02\n","iteration: 165410 loss: 0.0107 lr: 0.02\n","iteration: 165420 loss: 0.0105 lr: 0.02\n","iteration: 165430 loss: 0.0115 lr: 0.02\n","iteration: 165440 loss: 0.0105 lr: 0.02\n","iteration: 165450 loss: 0.0101 lr: 0.02\n","iteration: 165460 loss: 0.0094 lr: 0.02\n","iteration: 165470 loss: 0.0093 lr: 0.02\n","iteration: 165480 loss: 0.0090 lr: 0.02\n","iteration: 165490 loss: 0.0091 lr: 0.02\n","iteration: 165500 loss: 0.0134 lr: 0.02\n","iteration: 165510 loss: 0.0133 lr: 0.02\n","iteration: 165520 loss: 0.0114 lr: 0.02\n","iteration: 165530 loss: 0.0102 lr: 0.02\n","iteration: 165540 loss: 0.0078 lr: 0.02\n","iteration: 165550 loss: 0.0090 lr: 0.02\n","iteration: 165560 loss: 0.0144 lr: 0.02\n","iteration: 165570 loss: 0.0107 lr: 0.02\n","iteration: 165580 loss: 0.0084 lr: 0.02\n","iteration: 165590 loss: 0.0094 lr: 0.02\n","iteration: 165600 loss: 0.0124 lr: 0.02\n","iteration: 165610 loss: 0.0096 lr: 0.02\n","iteration: 165620 loss: 0.0101 lr: 0.02\n","iteration: 165630 loss: 0.0096 lr: 0.02\n","iteration: 165640 loss: 0.0104 lr: 0.02\n","iteration: 165650 loss: 0.0074 lr: 0.02\n","iteration: 165660 loss: 0.0075 lr: 0.02\n","iteration: 165670 loss: 0.0087 lr: 0.02\n","iteration: 165680 loss: 0.0100 lr: 0.02\n","iteration: 165690 loss: 0.0108 lr: 0.02\n","iteration: 165700 loss: 0.0080 lr: 0.02\n","iteration: 165710 loss: 0.0089 lr: 0.02\n","iteration: 165720 loss: 0.0107 lr: 0.02\n","iteration: 165730 loss: 0.0085 lr: 0.02\n","iteration: 165740 loss: 0.0113 lr: 0.02\n","iteration: 165750 loss: 0.0102 lr: 0.02\n","iteration: 165760 loss: 0.0104 lr: 0.02\n","iteration: 165770 loss: 0.0086 lr: 0.02\n","iteration: 165780 loss: 0.0100 lr: 0.02\n","iteration: 165790 loss: 0.0133 lr: 0.02\n","iteration: 165800 loss: 0.0076 lr: 0.02\n","iteration: 165810 loss: 0.0069 lr: 0.02\n","iteration: 165820 loss: 0.0081 lr: 0.02\n","iteration: 165830 loss: 0.0094 lr: 0.02\n","iteration: 165840 loss: 0.0107 lr: 0.02\n","iteration: 165850 loss: 0.0104 lr: 0.02\n","iteration: 165860 loss: 0.0099 lr: 0.02\n","iteration: 165870 loss: 0.0092 lr: 0.02\n","iteration: 165880 loss: 0.0160 lr: 0.02\n","iteration: 165890 loss: 0.0110 lr: 0.02\n","iteration: 165900 loss: 0.0101 lr: 0.02\n","iteration: 165910 loss: 0.0104 lr: 0.02\n","iteration: 165920 loss: 0.0103 lr: 0.02\n","iteration: 165930 loss: 0.0113 lr: 0.02\n","iteration: 165940 loss: 0.0109 lr: 0.02\n","iteration: 165950 loss: 0.0118 lr: 0.02\n","iteration: 165960 loss: 0.0087 lr: 0.02\n","iteration: 165970 loss: 0.0085 lr: 0.02\n","iteration: 165980 loss: 0.0101 lr: 0.02\n","iteration: 165990 loss: 0.0063 lr: 0.02\n","iteration: 166000 loss: 0.0082 lr: 0.02\n","iteration: 166010 loss: 0.0110 lr: 0.02\n","iteration: 166020 loss: 0.0130 lr: 0.02\n","iteration: 166030 loss: 0.0123 lr: 0.02\n","iteration: 166040 loss: 0.0090 lr: 0.02\n","iteration: 166050 loss: 0.0115 lr: 0.02\n","iteration: 166060 loss: 0.0123 lr: 0.02\n","iteration: 166070 loss: 0.0095 lr: 0.02\n","iteration: 166080 loss: 0.0089 lr: 0.02\n","iteration: 166090 loss: 0.0095 lr: 0.02\n","iteration: 166100 loss: 0.0078 lr: 0.02\n","iteration: 166110 loss: 0.0082 lr: 0.02\n","iteration: 166120 loss: 0.0103 lr: 0.02\n","iteration: 166130 loss: 0.0114 lr: 0.02\n","iteration: 166140 loss: 0.0082 lr: 0.02\n","iteration: 166150 loss: 0.0077 lr: 0.02\n","iteration: 166160 loss: 0.0072 lr: 0.02\n","iteration: 166170 loss: 0.0113 lr: 0.02\n","iteration: 166180 loss: 0.0085 lr: 0.02\n","iteration: 166190 loss: 0.0087 lr: 0.02\n","iteration: 166200 loss: 0.0100 lr: 0.02\n","iteration: 166210 loss: 0.0084 lr: 0.02\n","iteration: 166220 loss: 0.0098 lr: 0.02\n","iteration: 166230 loss: 0.0107 lr: 0.02\n","iteration: 166240 loss: 0.0138 lr: 0.02\n","iteration: 166250 loss: 0.0114 lr: 0.02\n","iteration: 166260 loss: 0.0097 lr: 0.02\n","iteration: 166270 loss: 0.0079 lr: 0.02\n","iteration: 166280 loss: 0.0104 lr: 0.02\n","iteration: 166290 loss: 0.0103 lr: 0.02\n","iteration: 166300 loss: 0.0107 lr: 0.02\n","iteration: 166310 loss: 0.0118 lr: 0.02\n","iteration: 166320 loss: 0.0080 lr: 0.02\n","iteration: 166330 loss: 0.0117 lr: 0.02\n","iteration: 166340 loss: 0.0068 lr: 0.02\n","iteration: 166350 loss: 0.0088 lr: 0.02\n","iteration: 166360 loss: 0.0106 lr: 0.02\n","iteration: 166370 loss: 0.0081 lr: 0.02\n","iteration: 166380 loss: 0.0111 lr: 0.02\n","iteration: 166390 loss: 0.0102 lr: 0.02\n","iteration: 166400 loss: 0.0117 lr: 0.02\n","iteration: 166410 loss: 0.0081 lr: 0.02\n","iteration: 166420 loss: 0.0093 lr: 0.02\n","iteration: 166430 loss: 0.0087 lr: 0.02\n","iteration: 166440 loss: 0.0091 lr: 0.02\n","iteration: 166450 loss: 0.0103 lr: 0.02\n","iteration: 166460 loss: 0.0086 lr: 0.02\n","iteration: 166470 loss: 0.0109 lr: 0.02\n","iteration: 166480 loss: 0.0096 lr: 0.02\n","iteration: 166490 loss: 0.0089 lr: 0.02\n","iteration: 166500 loss: 0.0078 lr: 0.02\n","iteration: 166510 loss: 0.0090 lr: 0.02\n","iteration: 166520 loss: 0.0089 lr: 0.02\n","iteration: 166530 loss: 0.0089 lr: 0.02\n","iteration: 166540 loss: 0.0101 lr: 0.02\n","iteration: 166550 loss: 0.0082 lr: 0.02\n","iteration: 166560 loss: 0.0095 lr: 0.02\n","iteration: 166570 loss: 0.0098 lr: 0.02\n","iteration: 166580 loss: 0.0108 lr: 0.02\n","iteration: 166590 loss: 0.0069 lr: 0.02\n","iteration: 166600 loss: 0.0089 lr: 0.02\n","iteration: 166610 loss: 0.0105 lr: 0.02\n","iteration: 166620 loss: 0.0092 lr: 0.02\n","iteration: 166630 loss: 0.0156 lr: 0.02\n","iteration: 166640 loss: 0.0111 lr: 0.02\n","iteration: 166650 loss: 0.0143 lr: 0.02\n","iteration: 166660 loss: 0.0106 lr: 0.02\n","iteration: 166670 loss: 0.0087 lr: 0.02\n","iteration: 166680 loss: 0.0103 lr: 0.02\n","iteration: 166690 loss: 0.0096 lr: 0.02\n","iteration: 166700 loss: 0.0128 lr: 0.02\n","iteration: 166710 loss: 0.0091 lr: 0.02\n","iteration: 166720 loss: 0.0079 lr: 0.02\n","iteration: 166730 loss: 0.0095 lr: 0.02\n","iteration: 166740 loss: 0.0104 lr: 0.02\n","iteration: 166750 loss: 0.0080 lr: 0.02\n","iteration: 166760 loss: 0.0100 lr: 0.02\n","iteration: 166770 loss: 0.0106 lr: 0.02\n","iteration: 166780 loss: 0.0118 lr: 0.02\n","iteration: 166790 loss: 0.0065 lr: 0.02\n","iteration: 166800 loss: 0.0159 lr: 0.02\n","iteration: 166810 loss: 0.0101 lr: 0.02\n","iteration: 166820 loss: 0.0075 lr: 0.02\n","iteration: 166830 loss: 0.0126 lr: 0.02\n","iteration: 166840 loss: 0.0093 lr: 0.02\n","iteration: 166850 loss: 0.0090 lr: 0.02\n","iteration: 166860 loss: 0.0090 lr: 0.02\n","iteration: 166870 loss: 0.0131 lr: 0.02\n","iteration: 166880 loss: 0.0120 lr: 0.02\n","iteration: 166890 loss: 0.0157 lr: 0.02\n","iteration: 166900 loss: 0.0104 lr: 0.02\n","iteration: 166910 loss: 0.0116 lr: 0.02\n","iteration: 166920 loss: 0.0108 lr: 0.02\n","iteration: 166930 loss: 0.0100 lr: 0.02\n","iteration: 166940 loss: 0.0093 lr: 0.02\n","iteration: 166950 loss: 0.0120 lr: 0.02\n","iteration: 166960 loss: 0.0100 lr: 0.02\n","iteration: 166970 loss: 0.0085 lr: 0.02\n","iteration: 166980 loss: 0.0124 lr: 0.02\n","iteration: 166990 loss: 0.0097 lr: 0.02\n","iteration: 167000 loss: 0.0096 lr: 0.02\n","iteration: 167010 loss: 0.0100 lr: 0.02\n","iteration: 167020 loss: 0.0102 lr: 0.02\n","iteration: 167030 loss: 0.0096 lr: 0.02\n","iteration: 167040 loss: 0.0097 lr: 0.02\n","iteration: 167050 loss: 0.0099 lr: 0.02\n","iteration: 167060 loss: 0.0097 lr: 0.02\n","iteration: 167070 loss: 0.0120 lr: 0.02\n","iteration: 167080 loss: 0.0063 lr: 0.02\n","iteration: 167090 loss: 0.0109 lr: 0.02\n","iteration: 167100 loss: 0.0124 lr: 0.02\n","iteration: 167110 loss: 0.0105 lr: 0.02\n","iteration: 167120 loss: 0.0100 lr: 0.02\n","iteration: 167130 loss: 0.0076 lr: 0.02\n","iteration: 167140 loss: 0.0094 lr: 0.02\n","iteration: 167150 loss: 0.0097 lr: 0.02\n","iteration: 167160 loss: 0.0094 lr: 0.02\n","iteration: 167170 loss: 0.0126 lr: 0.02\n","iteration: 167180 loss: 0.0170 lr: 0.02\n","iteration: 167190 loss: 0.0069 lr: 0.02\n","iteration: 167200 loss: 0.0096 lr: 0.02\n","iteration: 167210 loss: 0.0102 lr: 0.02\n","iteration: 167220 loss: 0.0109 lr: 0.02\n","iteration: 167230 loss: 0.0101 lr: 0.02\n","iteration: 167240 loss: 0.0109 lr: 0.02\n","iteration: 167250 loss: 0.0119 lr: 0.02\n","iteration: 167260 loss: 0.0104 lr: 0.02\n","iteration: 167270 loss: 0.0139 lr: 0.02\n","iteration: 167280 loss: 0.0129 lr: 0.02\n","iteration: 167290 loss: 0.0082 lr: 0.02\n","iteration: 167300 loss: 0.0109 lr: 0.02\n","iteration: 167310 loss: 0.0062 lr: 0.02\n","iteration: 167320 loss: 0.0105 lr: 0.02\n","iteration: 167330 loss: 0.0106 lr: 0.02\n","iteration: 167340 loss: 0.0107 lr: 0.02\n","iteration: 167350 loss: 0.0103 lr: 0.02\n","iteration: 167360 loss: 0.0104 lr: 0.02\n","iteration: 167370 loss: 0.0119 lr: 0.02\n","iteration: 167380 loss: 0.0111 lr: 0.02\n","iteration: 167390 loss: 0.0094 lr: 0.02\n","iteration: 167400 loss: 0.0079 lr: 0.02\n","iteration: 167410 loss: 0.0076 lr: 0.02\n","iteration: 167420 loss: 0.0117 lr: 0.02\n","iteration: 167430 loss: 0.0084 lr: 0.02\n","iteration: 167440 loss: 0.0082 lr: 0.02\n","iteration: 167450 loss: 0.0097 lr: 0.02\n","iteration: 167460 loss: 0.0097 lr: 0.02\n","iteration: 167470 loss: 0.0114 lr: 0.02\n","iteration: 167480 loss: 0.0090 lr: 0.02\n","iteration: 167490 loss: 0.0126 lr: 0.02\n","iteration: 167500 loss: 0.0098 lr: 0.02\n","iteration: 167510 loss: 0.0076 lr: 0.02\n","iteration: 167520 loss: 0.0112 lr: 0.02\n","iteration: 167530 loss: 0.0114 lr: 0.02\n","iteration: 167540 loss: 0.0115 lr: 0.02\n","iteration: 167550 loss: 0.0072 lr: 0.02\n","iteration: 167560 loss: 0.0098 lr: 0.02\n","iteration: 167570 loss: 0.0086 lr: 0.02\n","iteration: 167580 loss: 0.0099 lr: 0.02\n","iteration: 167590 loss: 0.0137 lr: 0.02\n","iteration: 167600 loss: 0.0102 lr: 0.02\n","iteration: 167610 loss: 0.0106 lr: 0.02\n","iteration: 167620 loss: 0.0090 lr: 0.02\n","iteration: 167630 loss: 0.0088 lr: 0.02\n","iteration: 167640 loss: 0.0102 lr: 0.02\n","iteration: 167650 loss: 0.0095 lr: 0.02\n","iteration: 167660 loss: 0.0085 lr: 0.02\n","iteration: 167670 loss: 0.0078 lr: 0.02\n","iteration: 167680 loss: 0.0155 lr: 0.02\n","iteration: 167690 loss: 0.0102 lr: 0.02\n","iteration: 167700 loss: 0.0085 lr: 0.02\n","iteration: 167710 loss: 0.0083 lr: 0.02\n","iteration: 167720 loss: 0.0095 lr: 0.02\n","iteration: 167730 loss: 0.0115 lr: 0.02\n","iteration: 167740 loss: 0.0168 lr: 0.02\n","iteration: 167750 loss: 0.0112 lr: 0.02\n","iteration: 167760 loss: 0.0107 lr: 0.02\n","iteration: 167770 loss: 0.0088 lr: 0.02\n","iteration: 167780 loss: 0.0108 lr: 0.02\n","iteration: 167790 loss: 0.0129 lr: 0.02\n","iteration: 167800 loss: 0.0109 lr: 0.02\n","iteration: 167810 loss: 0.0074 lr: 0.02\n","iteration: 167820 loss: 0.0084 lr: 0.02\n","iteration: 167830 loss: 0.0122 lr: 0.02\n","iteration: 167840 loss: 0.0123 lr: 0.02\n","iteration: 167850 loss: 0.0087 lr: 0.02\n","iteration: 167860 loss: 0.0102 lr: 0.02\n","iteration: 167870 loss: 0.0104 lr: 0.02\n","iteration: 167880 loss: 0.0096 lr: 0.02\n","iteration: 167890 loss: 0.0139 lr: 0.02\n","iteration: 167900 loss: 0.0117 lr: 0.02\n","iteration: 167910 loss: 0.0143 lr: 0.02\n","iteration: 167920 loss: 0.0114 lr: 0.02\n","iteration: 167930 loss: 0.0116 lr: 0.02\n","iteration: 167940 loss: 0.0130 lr: 0.02\n","iteration: 167950 loss: 0.0086 lr: 0.02\n","iteration: 167960 loss: 0.0104 lr: 0.02\n","iteration: 167970 loss: 0.0084 lr: 0.02\n","iteration: 167980 loss: 0.0150 lr: 0.02\n","iteration: 167990 loss: 0.0104 lr: 0.02\n","iteration: 168000 loss: 0.0104 lr: 0.02\n","iteration: 168010 loss: 0.0075 lr: 0.02\n","iteration: 168020 loss: 0.0117 lr: 0.02\n","iteration: 168030 loss: 0.0190 lr: 0.02\n","iteration: 168040 loss: 0.0100 lr: 0.02\n","iteration: 168050 loss: 0.0102 lr: 0.02\n","iteration: 168060 loss: 0.0098 lr: 0.02\n","iteration: 168070 loss: 0.0089 lr: 0.02\n","iteration: 168080 loss: 0.0099 lr: 0.02\n","iteration: 168090 loss: 0.0115 lr: 0.02\n","iteration: 168100 loss: 0.0124 lr: 0.02\n","iteration: 168110 loss: 0.0116 lr: 0.02\n","iteration: 168120 loss: 0.0166 lr: 0.02\n","iteration: 168130 loss: 0.0113 lr: 0.02\n","iteration: 168140 loss: 0.0074 lr: 0.02\n","iteration: 168150 loss: 0.0090 lr: 0.02\n","iteration: 168160 loss: 0.0106 lr: 0.02\n","iteration: 168170 loss: 0.0090 lr: 0.02\n","iteration: 168180 loss: 0.0080 lr: 0.02\n","iteration: 168190 loss: 0.0124 lr: 0.02\n","iteration: 168200 loss: 0.0082 lr: 0.02\n","iteration: 168210 loss: 0.0117 lr: 0.02\n","iteration: 168220 loss: 0.0113 lr: 0.02\n","iteration: 168230 loss: 0.0099 lr: 0.02\n","iteration: 168240 loss: 0.0095 lr: 0.02\n","iteration: 168250 loss: 0.0100 lr: 0.02\n","iteration: 168260 loss: 0.0123 lr: 0.02\n","iteration: 168270 loss: 0.0081 lr: 0.02\n","iteration: 168280 loss: 0.0099 lr: 0.02\n","iteration: 168290 loss: 0.0110 lr: 0.02\n","iteration: 168300 loss: 0.0102 lr: 0.02\n","iteration: 168310 loss: 0.0059 lr: 0.02\n","iteration: 168320 loss: 0.0105 lr: 0.02\n","iteration: 168330 loss: 0.0089 lr: 0.02\n","iteration: 168340 loss: 0.0107 lr: 0.02\n","iteration: 168350 loss: 0.0072 lr: 0.02\n","iteration: 168360 loss: 0.0127 lr: 0.02\n","iteration: 168370 loss: 0.0086 lr: 0.02\n","iteration: 168380 loss: 0.0100 lr: 0.02\n","iteration: 168390 loss: 0.0104 lr: 0.02\n","iteration: 168400 loss: 0.0069 lr: 0.02\n","iteration: 168410 loss: 0.0105 lr: 0.02\n","iteration: 168420 loss: 0.0135 lr: 0.02\n","iteration: 168430 loss: 0.0080 lr: 0.02\n","iteration: 168440 loss: 0.0123 lr: 0.02\n","iteration: 168450 loss: 0.0144 lr: 0.02\n","iteration: 168460 loss: 0.0139 lr: 0.02\n","iteration: 168470 loss: 0.0150 lr: 0.02\n","iteration: 168480 loss: 0.0102 lr: 0.02\n","iteration: 168490 loss: 0.0117 lr: 0.02\n","iteration: 168500 loss: 0.0083 lr: 0.02\n","iteration: 168510 loss: 0.0091 lr: 0.02\n","iteration: 168520 loss: 0.0116 lr: 0.02\n","iteration: 168530 loss: 0.0137 lr: 0.02\n","iteration: 168540 loss: 0.0100 lr: 0.02\n","iteration: 168550 loss: 0.0091 lr: 0.02\n","iteration: 168560 loss: 0.0104 lr: 0.02\n","iteration: 168570 loss: 0.0094 lr: 0.02\n","iteration: 168580 loss: 0.0090 lr: 0.02\n","iteration: 168590 loss: 0.0096 lr: 0.02\n","iteration: 168600 loss: 0.0136 lr: 0.02\n","iteration: 168610 loss: 0.0088 lr: 0.02\n","iteration: 168620 loss: 0.0138 lr: 0.02\n","iteration: 168630 loss: 0.0087 lr: 0.02\n","iteration: 168640 loss: 0.0123 lr: 0.02\n","iteration: 168650 loss: 0.0186 lr: 0.02\n","iteration: 168660 loss: 0.0120 lr: 0.02\n","iteration: 168670 loss: 0.0098 lr: 0.02\n","iteration: 168680 loss: 0.0089 lr: 0.02\n","iteration: 168690 loss: 0.0105 lr: 0.02\n","iteration: 168700 loss: 0.0082 lr: 0.02\n","iteration: 168710 loss: 0.0117 lr: 0.02\n","iteration: 168720 loss: 0.0089 lr: 0.02\n","iteration: 168730 loss: 0.0098 lr: 0.02\n","iteration: 168740 loss: 0.0098 lr: 0.02\n","iteration: 168750 loss: 0.0088 lr: 0.02\n","iteration: 168760 loss: 0.0112 lr: 0.02\n","iteration: 168770 loss: 0.0082 lr: 0.02\n","iteration: 168780 loss: 0.0110 lr: 0.02\n","iteration: 168790 loss: 0.0063 lr: 0.02\n","iteration: 168800 loss: 0.0104 lr: 0.02\n","iteration: 168810 loss: 0.0088 lr: 0.02\n","iteration: 168820 loss: 0.0105 lr: 0.02\n","iteration: 168830 loss: 0.0130 lr: 0.02\n","iteration: 168840 loss: 0.0078 lr: 0.02\n","iteration: 168850 loss: 0.0092 lr: 0.02\n","iteration: 168860 loss: 0.0071 lr: 0.02\n","iteration: 168870 loss: 0.0090 lr: 0.02\n","iteration: 168880 loss: 0.0095 lr: 0.02\n","iteration: 168890 loss: 0.0086 lr: 0.02\n","iteration: 168900 loss: 0.0087 lr: 0.02\n","iteration: 168910 loss: 0.0097 lr: 0.02\n","iteration: 168920 loss: 0.0126 lr: 0.02\n","iteration: 168930 loss: 0.0102 lr: 0.02\n","iteration: 168940 loss: 0.0092 lr: 0.02\n","iteration: 168950 loss: 0.0077 lr: 0.02\n","iteration: 168960 loss: 0.0109 lr: 0.02\n","iteration: 168970 loss: 0.0107 lr: 0.02\n","iteration: 168980 loss: 0.0099 lr: 0.02\n","iteration: 168990 loss: 0.0112 lr: 0.02\n","iteration: 169000 loss: 0.0074 lr: 0.02\n","iteration: 169010 loss: 0.0084 lr: 0.02\n","iteration: 169020 loss: 0.0073 lr: 0.02\n","iteration: 169030 loss: 0.0090 lr: 0.02\n","iteration: 169040 loss: 0.0090 lr: 0.02\n","iteration: 169050 loss: 0.0108 lr: 0.02\n","iteration: 169060 loss: 0.0077 lr: 0.02\n","iteration: 169070 loss: 0.0070 lr: 0.02\n","iteration: 169080 loss: 0.0091 lr: 0.02\n","iteration: 169090 loss: 0.0091 lr: 0.02\n","iteration: 169100 loss: 0.0113 lr: 0.02\n","iteration: 169110 loss: 0.0098 lr: 0.02\n","iteration: 169120 loss: 0.0103 lr: 0.02\n","iteration: 169130 loss: 0.0085 lr: 0.02\n","iteration: 169140 loss: 0.0108 lr: 0.02\n","iteration: 169150 loss: 0.0107 lr: 0.02\n","iteration: 169160 loss: 0.0097 lr: 0.02\n","iteration: 169170 loss: 0.0085 lr: 0.02\n","iteration: 169180 loss: 0.0096 lr: 0.02\n","iteration: 169190 loss: 0.0060 lr: 0.02\n","iteration: 169200 loss: 0.0101 lr: 0.02\n","iteration: 169210 loss: 0.0055 lr: 0.02\n","iteration: 169220 loss: 0.0080 lr: 0.02\n","iteration: 169230 loss: 0.0102 lr: 0.02\n","iteration: 169240 loss: 0.0075 lr: 0.02\n","iteration: 169250 loss: 0.0104 lr: 0.02\n","iteration: 169260 loss: 0.0091 lr: 0.02\n","iteration: 169270 loss: 0.0080 lr: 0.02\n","iteration: 169280 loss: 0.0091 lr: 0.02\n","iteration: 169290 loss: 0.0093 lr: 0.02\n","iteration: 169300 loss: 0.0088 lr: 0.02\n","iteration: 169310 loss: 0.0064 lr: 0.02\n","iteration: 169320 loss: 0.0149 lr: 0.02\n","iteration: 169330 loss: 0.0097 lr: 0.02\n","iteration: 169340 loss: 0.0104 lr: 0.02\n","iteration: 169350 loss: 0.0078 lr: 0.02\n","iteration: 169360 loss: 0.0081 lr: 0.02\n","iteration: 169370 loss: 0.0125 lr: 0.02\n","iteration: 169380 loss: 0.0087 lr: 0.02\n","iteration: 169390 loss: 0.0076 lr: 0.02\n","iteration: 169400 loss: 0.0069 lr: 0.02\n","iteration: 169410 loss: 0.0108 lr: 0.02\n","iteration: 169420 loss: 0.0098 lr: 0.02\n","iteration: 169430 loss: 0.0096 lr: 0.02\n","iteration: 169440 loss: 0.0116 lr: 0.02\n","iteration: 169450 loss: 0.0138 lr: 0.02\n","iteration: 169460 loss: 0.0100 lr: 0.02\n","iteration: 169470 loss: 0.0140 lr: 0.02\n","iteration: 169480 loss: 0.0099 lr: 0.02\n","iteration: 169490 loss: 0.0127 lr: 0.02\n","iteration: 169500 loss: 0.0077 lr: 0.02\n","iteration: 169510 loss: 0.0128 lr: 0.02\n","iteration: 169520 loss: 0.0123 lr: 0.02\n","iteration: 169530 loss: 0.0068 lr: 0.02\n","iteration: 169540 loss: 0.0096 lr: 0.02\n","iteration: 169550 loss: 0.0088 lr: 0.02\n","iteration: 169560 loss: 0.0085 lr: 0.02\n","iteration: 169570 loss: 0.0095 lr: 0.02\n","iteration: 169580 loss: 0.0095 lr: 0.02\n","iteration: 169590 loss: 0.0118 lr: 0.02\n","iteration: 169600 loss: 0.0089 lr: 0.02\n","iteration: 169610 loss: 0.0144 lr: 0.02\n","iteration: 169620 loss: 0.0109 lr: 0.02\n","iteration: 169630 loss: 0.0086 lr: 0.02\n","iteration: 169640 loss: 0.0117 lr: 0.02\n","iteration: 169650 loss: 0.0075 lr: 0.02\n","iteration: 169660 loss: 0.0119 lr: 0.02\n","iteration: 169670 loss: 0.0096 lr: 0.02\n","iteration: 169680 loss: 0.0088 lr: 0.02\n","iteration: 169690 loss: 0.0127 lr: 0.02\n","iteration: 169700 loss: 0.0167 lr: 0.02\n","iteration: 169710 loss: 0.0106 lr: 0.02\n","iteration: 169720 loss: 0.0102 lr: 0.02\n","iteration: 169730 loss: 0.0080 lr: 0.02\n","iteration: 169740 loss: 0.0115 lr: 0.02\n","iteration: 169750 loss: 0.0099 lr: 0.02\n","iteration: 169760 loss: 0.0082 lr: 0.02\n","iteration: 169770 loss: 0.0170 lr: 0.02\n","iteration: 169780 loss: 0.0104 lr: 0.02\n","iteration: 169790 loss: 0.0081 lr: 0.02\n","iteration: 169800 loss: 0.0114 lr: 0.02\n","iteration: 169810 loss: 0.0142 lr: 0.02\n","iteration: 169820 loss: 0.0089 lr: 0.02\n","iteration: 169830 loss: 0.0084 lr: 0.02\n","iteration: 169840 loss: 0.0120 lr: 0.02\n","iteration: 169850 loss: 0.0124 lr: 0.02\n","iteration: 169860 loss: 0.0112 lr: 0.02\n","iteration: 169870 loss: 0.0099 lr: 0.02\n","iteration: 169880 loss: 0.0097 lr: 0.02\n","iteration: 169890 loss: 0.0165 lr: 0.02\n","iteration: 169900 loss: 0.0160 lr: 0.02\n","iteration: 169910 loss: 0.0143 lr: 0.02\n","iteration: 169920 loss: 0.0150 lr: 0.02\n","iteration: 169930 loss: 0.0207 lr: 0.02\n","iteration: 169940 loss: 0.0162 lr: 0.02\n","iteration: 169950 loss: 0.0173 lr: 0.02\n","iteration: 169960 loss: 0.0114 lr: 0.02\n","iteration: 169970 loss: 0.0132 lr: 0.02\n","iteration: 169980 loss: 0.0093 lr: 0.02\n","iteration: 169990 loss: 0.0119 lr: 0.02\n","iteration: 170000 loss: 0.0089 lr: 0.02\n","iteration: 170010 loss: 0.0098 lr: 0.02\n","iteration: 170020 loss: 0.0139 lr: 0.02\n","iteration: 170030 loss: 0.0107 lr: 0.02\n","iteration: 170040 loss: 0.0130 lr: 0.02\n","iteration: 170050 loss: 0.0173 lr: 0.02\n","iteration: 170060 loss: 0.0123 lr: 0.02\n","iteration: 170070 loss: 0.0077 lr: 0.02\n","iteration: 170080 loss: 0.0106 lr: 0.02\n","iteration: 170090 loss: 0.0095 lr: 0.02\n","iteration: 170100 loss: 0.0108 lr: 0.02\n","iteration: 170110 loss: 0.0094 lr: 0.02\n","iteration: 170120 loss: 0.0095 lr: 0.02\n","iteration: 170130 loss: 0.0089 lr: 0.02\n","iteration: 170140 loss: 0.0126 lr: 0.02\n","iteration: 170150 loss: 0.0117 lr: 0.02\n","iteration: 170160 loss: 0.0129 lr: 0.02\n","iteration: 170170 loss: 0.0171 lr: 0.02\n","iteration: 170180 loss: 0.0144 lr: 0.02\n","iteration: 170190 loss: 0.0098 lr: 0.02\n","iteration: 170200 loss: 0.0109 lr: 0.02\n","iteration: 170210 loss: 0.0096 lr: 0.02\n","iteration: 170220 loss: 0.0106 lr: 0.02\n","iteration: 170230 loss: 0.0098 lr: 0.02\n","iteration: 170240 loss: 0.0127 lr: 0.02\n","iteration: 170250 loss: 0.0101 lr: 0.02\n","iteration: 170260 loss: 0.0096 lr: 0.02\n","iteration: 170270 loss: 0.0110 lr: 0.02\n","iteration: 170280 loss: 0.0100 lr: 0.02\n","iteration: 170290 loss: 0.0084 lr: 0.02\n","iteration: 170300 loss: 0.0138 lr: 0.02\n","iteration: 170310 loss: 0.0119 lr: 0.02\n","iteration: 170320 loss: 0.0109 lr: 0.02\n","iteration: 170330 loss: 0.0123 lr: 0.02\n","iteration: 170340 loss: 0.0181 lr: 0.02\n","iteration: 170350 loss: 0.0130 lr: 0.02\n","iteration: 170360 loss: 0.0109 lr: 0.02\n","iteration: 170370 loss: 0.0121 lr: 0.02\n","iteration: 170380 loss: 0.0122 lr: 0.02\n","iteration: 170390 loss: 0.0170 lr: 0.02\n","iteration: 170400 loss: 0.0120 lr: 0.02\n","iteration: 170410 loss: 0.0109 lr: 0.02\n","iteration: 170420 loss: 0.0123 lr: 0.02\n","iteration: 170430 loss: 0.0101 lr: 0.02\n","iteration: 170440 loss: 0.0123 lr: 0.02\n","iteration: 170450 loss: 0.0118 lr: 0.02\n","iteration: 170460 loss: 0.0132 lr: 0.02\n","iteration: 170470 loss: 0.0094 lr: 0.02\n","iteration: 170480 loss: 0.0116 lr: 0.02\n","iteration: 170490 loss: 0.0087 lr: 0.02\n","iteration: 170500 loss: 0.0087 lr: 0.02\n","iteration: 170510 loss: 0.0132 lr: 0.02\n","iteration: 170520 loss: 0.0114 lr: 0.02\n","iteration: 170530 loss: 0.0088 lr: 0.02\n","iteration: 170540 loss: 0.0106 lr: 0.02\n","iteration: 170550 loss: 0.0137 lr: 0.02\n","iteration: 170560 loss: 0.0113 lr: 0.02\n","iteration: 170570 loss: 0.0094 lr: 0.02\n","iteration: 170580 loss: 0.0123 lr: 0.02\n","iteration: 170590 loss: 0.0112 lr: 0.02\n","iteration: 170600 loss: 0.0098 lr: 0.02\n","iteration: 170610 loss: 0.0089 lr: 0.02\n","iteration: 170620 loss: 0.0109 lr: 0.02\n","iteration: 170630 loss: 0.0082 lr: 0.02\n","iteration: 170640 loss: 0.0108 lr: 0.02\n","iteration: 170650 loss: 0.0114 lr: 0.02\n","iteration: 170660 loss: 0.0109 lr: 0.02\n","iteration: 170670 loss: 0.0114 lr: 0.02\n","iteration: 170680 loss: 0.0102 lr: 0.02\n","iteration: 170690 loss: 0.0081 lr: 0.02\n","iteration: 170700 loss: 0.0096 lr: 0.02\n","iteration: 170710 loss: 0.0102 lr: 0.02\n","iteration: 170720 loss: 0.0098 lr: 0.02\n","iteration: 170730 loss: 0.0080 lr: 0.02\n","iteration: 170740 loss: 0.0076 lr: 0.02\n","iteration: 170750 loss: 0.0091 lr: 0.02\n","iteration: 170760 loss: 0.0089 lr: 0.02\n","iteration: 170770 loss: 0.0112 lr: 0.02\n","iteration: 170780 loss: 0.0133 lr: 0.02\n","iteration: 170790 loss: 0.0093 lr: 0.02\n","iteration: 170800 loss: 0.0105 lr: 0.02\n","iteration: 170810 loss: 0.0097 lr: 0.02\n","iteration: 170820 loss: 0.0106 lr: 0.02\n","iteration: 170830 loss: 0.0095 lr: 0.02\n","iteration: 170840 loss: 0.0106 lr: 0.02\n","iteration: 170850 loss: 0.0074 lr: 0.02\n","iteration: 170860 loss: 0.0099 lr: 0.02\n","iteration: 170870 loss: 0.0087 lr: 0.02\n","iteration: 170880 loss: 0.0139 lr: 0.02\n","iteration: 170890 loss: 0.0109 lr: 0.02\n","iteration: 170900 loss: 0.0087 lr: 0.02\n","iteration: 170910 loss: 0.0064 lr: 0.02\n","iteration: 170920 loss: 0.0085 lr: 0.02\n","iteration: 170930 loss: 0.0076 lr: 0.02\n","iteration: 170940 loss: 0.0112 lr: 0.02\n","iteration: 170950 loss: 0.0077 lr: 0.02\n","iteration: 170960 loss: 0.0108 lr: 0.02\n","iteration: 170970 loss: 0.0089 lr: 0.02\n","iteration: 170980 loss: 0.0154 lr: 0.02\n","iteration: 170990 loss: 0.0049 lr: 0.02\n","iteration: 171000 loss: 0.0097 lr: 0.02\n","iteration: 171010 loss: 0.0092 lr: 0.02\n","iteration: 171020 loss: 0.0132 lr: 0.02\n","iteration: 171030 loss: 0.0099 lr: 0.02\n","iteration: 171040 loss: 0.0098 lr: 0.02\n","iteration: 171050 loss: 0.0080 lr: 0.02\n","iteration: 171060 loss: 0.0111 lr: 0.02\n","iteration: 171070 loss: 0.0074 lr: 0.02\n","iteration: 171080 loss: 0.0143 lr: 0.02\n","iteration: 171090 loss: 0.0091 lr: 0.02\n","iteration: 171100 loss: 0.0134 lr: 0.02\n","iteration: 171110 loss: 0.0152 lr: 0.02\n","iteration: 171120 loss: 0.0118 lr: 0.02\n","iteration: 171130 loss: 0.0093 lr: 0.02\n","iteration: 171140 loss: 0.0075 lr: 0.02\n","iteration: 171150 loss: 0.0112 lr: 0.02\n","iteration: 171160 loss: 0.0075 lr: 0.02\n","iteration: 171170 loss: 0.0087 lr: 0.02\n","iteration: 171180 loss: 0.0099 lr: 0.02\n","iteration: 171190 loss: 0.0094 lr: 0.02\n","iteration: 171200 loss: 0.0137 lr: 0.02\n","iteration: 171210 loss: 0.0078 lr: 0.02\n","iteration: 171220 loss: 0.0075 lr: 0.02\n","iteration: 171230 loss: 0.0096 lr: 0.02\n","iteration: 171240 loss: 0.0082 lr: 0.02\n","iteration: 171250 loss: 0.0104 lr: 0.02\n","iteration: 171260 loss: 0.0091 lr: 0.02\n","iteration: 171270 loss: 0.0094 lr: 0.02\n","iteration: 171280 loss: 0.0111 lr: 0.02\n","iteration: 171290 loss: 0.0073 lr: 0.02\n","iteration: 171300 loss: 0.0098 lr: 0.02\n","iteration: 171310 loss: 0.0101 lr: 0.02\n","iteration: 171320 loss: 0.0087 lr: 0.02\n","iteration: 171330 loss: 0.0097 lr: 0.02\n","iteration: 171340 loss: 0.0083 lr: 0.02\n","iteration: 171350 loss: 0.0110 lr: 0.02\n","iteration: 171360 loss: 0.0098 lr: 0.02\n","iteration: 171370 loss: 0.0112 lr: 0.02\n","iteration: 171380 loss: 0.0093 lr: 0.02\n","iteration: 171390 loss: 0.0091 lr: 0.02\n","iteration: 171400 loss: 0.0116 lr: 0.02\n","iteration: 171410 loss: 0.0098 lr: 0.02\n","iteration: 171420 loss: 0.0074 lr: 0.02\n","iteration: 171430 loss: 0.0119 lr: 0.02\n","iteration: 171440 loss: 0.0092 lr: 0.02\n","iteration: 171450 loss: 0.0097 lr: 0.02\n","iteration: 171460 loss: 0.0083 lr: 0.02\n","iteration: 171470 loss: 0.0057 lr: 0.02\n","iteration: 171480 loss: 0.0100 lr: 0.02\n","iteration: 171490 loss: 0.0078 lr: 0.02\n","iteration: 171500 loss: 0.0083 lr: 0.02\n","iteration: 171510 loss: 0.0075 lr: 0.02\n","iteration: 171520 loss: 0.0083 lr: 0.02\n","iteration: 171530 loss: 0.0091 lr: 0.02\n","iteration: 171540 loss: 0.0075 lr: 0.02\n","iteration: 171550 loss: 0.0106 lr: 0.02\n","iteration: 171560 loss: 0.0069 lr: 0.02\n","iteration: 171570 loss: 0.0094 lr: 0.02\n","iteration: 171580 loss: 0.0074 lr: 0.02\n","iteration: 171590 loss: 0.0102 lr: 0.02\n","iteration: 171600 loss: 0.0096 lr: 0.02\n","iteration: 171610 loss: 0.0084 lr: 0.02\n","iteration: 171620 loss: 0.0105 lr: 0.02\n","iteration: 171630 loss: 0.0113 lr: 0.02\n","iteration: 171640 loss: 0.0103 lr: 0.02\n","iteration: 171650 loss: 0.0090 lr: 0.02\n","iteration: 171660 loss: 0.0095 lr: 0.02\n","iteration: 171670 loss: 0.0081 lr: 0.02\n","iteration: 171680 loss: 0.0091 lr: 0.02\n","iteration: 171690 loss: 0.0133 lr: 0.02\n","iteration: 171700 loss: 0.0160 lr: 0.02\n","iteration: 171710 loss: 0.0148 lr: 0.02\n","iteration: 171720 loss: 0.0085 lr: 0.02\n","iteration: 171730 loss: 0.0103 lr: 0.02\n","iteration: 171740 loss: 0.0104 lr: 0.02\n","iteration: 171750 loss: 0.0160 lr: 0.02\n","iteration: 171760 loss: 0.0090 lr: 0.02\n","iteration: 171770 loss: 0.0123 lr: 0.02\n","iteration: 171780 loss: 0.0118 lr: 0.02\n","iteration: 171790 loss: 0.0111 lr: 0.02\n","iteration: 171800 loss: 0.0116 lr: 0.02\n","iteration: 171810 loss: 0.0091 lr: 0.02\n","iteration: 171820 loss: 0.0117 lr: 0.02\n","iteration: 171830 loss: 0.0107 lr: 0.02\n","iteration: 171840 loss: 0.0110 lr: 0.02\n","iteration: 171850 loss: 0.0105 lr: 0.02\n","iteration: 171860 loss: 0.0066 lr: 0.02\n","iteration: 171870 loss: 0.0085 lr: 0.02\n","iteration: 171880 loss: 0.0094 lr: 0.02\n","iteration: 171890 loss: 0.0121 lr: 0.02\n","iteration: 171900 loss: 0.0119 lr: 0.02\n","iteration: 171910 loss: 0.0131 lr: 0.02\n","iteration: 171920 loss: 0.0102 lr: 0.02\n","iteration: 171930 loss: 0.0117 lr: 0.02\n","iteration: 171940 loss: 0.0122 lr: 0.02\n","iteration: 171950 loss: 0.0103 lr: 0.02\n","iteration: 171960 loss: 0.0108 lr: 0.02\n","iteration: 171970 loss: 0.0101 lr: 0.02\n","iteration: 171980 loss: 0.0104 lr: 0.02\n","iteration: 171990 loss: 0.0091 lr: 0.02\n","iteration: 172000 loss: 0.0097 lr: 0.02\n","iteration: 172010 loss: 0.0083 lr: 0.02\n","iteration: 172020 loss: 0.0089 lr: 0.02\n","iteration: 172030 loss: 0.0095 lr: 0.02\n","iteration: 172040 loss: 0.0139 lr: 0.02\n","iteration: 172050 loss: 0.0085 lr: 0.02\n","iteration: 172060 loss: 0.0074 lr: 0.02\n","iteration: 172070 loss: 0.0070 lr: 0.02\n","iteration: 172080 loss: 0.0102 lr: 0.02\n","iteration: 172090 loss: 0.0100 lr: 0.02\n","iteration: 172100 loss: 0.0108 lr: 0.02\n","iteration: 172110 loss: 0.0116 lr: 0.02\n","iteration: 172120 loss: 0.0102 lr: 0.02\n","iteration: 172130 loss: 0.0124 lr: 0.02\n","iteration: 172140 loss: 0.0085 lr: 0.02\n","iteration: 172150 loss: 0.0075 lr: 0.02\n","iteration: 172160 loss: 0.0091 lr: 0.02\n","iteration: 172170 loss: 0.0115 lr: 0.02\n","iteration: 172180 loss: 0.0084 lr: 0.02\n","iteration: 172190 loss: 0.0082 lr: 0.02\n","iteration: 172200 loss: 0.0107 lr: 0.02\n","iteration: 172210 loss: 0.0141 lr: 0.02\n","iteration: 172220 loss: 0.0122 lr: 0.02\n","iteration: 172230 loss: 0.0103 lr: 0.02\n","iteration: 172240 loss: 0.0113 lr: 0.02\n","iteration: 172250 loss: 0.0100 lr: 0.02\n","iteration: 172260 loss: 0.0109 lr: 0.02\n","iteration: 172270 loss: 0.0081 lr: 0.02\n","iteration: 172280 loss: 0.0102 lr: 0.02\n","iteration: 172290 loss: 0.0131 lr: 0.02\n","iteration: 172300 loss: 0.0098 lr: 0.02\n","iteration: 172310 loss: 0.0083 lr: 0.02\n","iteration: 172320 loss: 0.0126 lr: 0.02\n","iteration: 172330 loss: 0.0066 lr: 0.02\n","iteration: 172340 loss: 0.0098 lr: 0.02\n","iteration: 172350 loss: 0.0108 lr: 0.02\n","iteration: 172360 loss: 0.0088 lr: 0.02\n","iteration: 172370 loss: 0.0070 lr: 0.02\n","iteration: 172380 loss: 0.0120 lr: 0.02\n","iteration: 172390 loss: 0.0103 lr: 0.02\n","iteration: 172400 loss: 0.0112 lr: 0.02\n","iteration: 172410 loss: 0.0074 lr: 0.02\n","iteration: 172420 loss: 0.0066 lr: 0.02\n","iteration: 172430 loss: 0.0090 lr: 0.02\n","iteration: 172440 loss: 0.0109 lr: 0.02\n","iteration: 172450 loss: 0.0092 lr: 0.02\n","iteration: 172460 loss: 0.0092 lr: 0.02\n","iteration: 172470 loss: 0.0087 lr: 0.02\n","iteration: 172480 loss: 0.0097 lr: 0.02\n","iteration: 172490 loss: 0.0093 lr: 0.02\n","iteration: 172500 loss: 0.0065 lr: 0.02\n","iteration: 172510 loss: 0.0090 lr: 0.02\n","iteration: 172520 loss: 0.0096 lr: 0.02\n","iteration: 172530 loss: 0.0101 lr: 0.02\n","iteration: 172540 loss: 0.0103 lr: 0.02\n","iteration: 172550 loss: 0.0121 lr: 0.02\n","iteration: 172560 loss: 0.0112 lr: 0.02\n","iteration: 172570 loss: 0.0089 lr: 0.02\n","iteration: 172580 loss: 0.0088 lr: 0.02\n","iteration: 172590 loss: 0.0096 lr: 0.02\n","iteration: 172600 loss: 0.0111 lr: 0.02\n","iteration: 172610 loss: 0.0089 lr: 0.02\n","iteration: 172620 loss: 0.0072 lr: 0.02\n","iteration: 172630 loss: 0.0127 lr: 0.02\n","iteration: 172640 loss: 0.0092 lr: 0.02\n","iteration: 172650 loss: 0.0088 lr: 0.02\n","iteration: 172660 loss: 0.0089 lr: 0.02\n","iteration: 172670 loss: 0.0074 lr: 0.02\n","iteration: 172680 loss: 0.0065 lr: 0.02\n","iteration: 172690 loss: 0.0121 lr: 0.02\n","iteration: 172700 loss: 0.0086 lr: 0.02\n","iteration: 172710 loss: 0.0095 lr: 0.02\n","iteration: 172720 loss: 0.0104 lr: 0.02\n","iteration: 172730 loss: 0.0110 lr: 0.02\n","iteration: 172740 loss: 0.0105 lr: 0.02\n","iteration: 172750 loss: 0.0097 lr: 0.02\n","iteration: 172760 loss: 0.0095 lr: 0.02\n","iteration: 172770 loss: 0.0086 lr: 0.02\n","iteration: 172780 loss: 0.0100 lr: 0.02\n","iteration: 172790 loss: 0.0090 lr: 0.02\n","iteration: 172800 loss: 0.0117 lr: 0.02\n","iteration: 172810 loss: 0.0081 lr: 0.02\n","iteration: 172820 loss: 0.0089 lr: 0.02\n","iteration: 172830 loss: 0.0082 lr: 0.02\n","iteration: 172840 loss: 0.0093 lr: 0.02\n","iteration: 172850 loss: 0.0084 lr: 0.02\n","iteration: 172860 loss: 0.0109 lr: 0.02\n","iteration: 172870 loss: 0.0118 lr: 0.02\n","iteration: 172880 loss: 0.0106 lr: 0.02\n","iteration: 172890 loss: 0.0095 lr: 0.02\n","iteration: 172900 loss: 0.0101 lr: 0.02\n","iteration: 172910 loss: 0.0095 lr: 0.02\n","iteration: 172920 loss: 0.0098 lr: 0.02\n","iteration: 172930 loss: 0.0098 lr: 0.02\n","iteration: 172940 loss: 0.0123 lr: 0.02\n","iteration: 172950 loss: 0.0102 lr: 0.02\n","iteration: 172960 loss: 0.0081 lr: 0.02\n","iteration: 172970 loss: 0.0111 lr: 0.02\n","iteration: 172980 loss: 0.0096 lr: 0.02\n","iteration: 172990 loss: 0.0075 lr: 0.02\n","iteration: 173000 loss: 0.0108 lr: 0.02\n","iteration: 173010 loss: 0.0087 lr: 0.02\n","iteration: 173020 loss: 0.0081 lr: 0.02\n","iteration: 173030 loss: 0.0076 lr: 0.02\n","iteration: 173040 loss: 0.0127 lr: 0.02\n","iteration: 173050 loss: 0.0088 lr: 0.02\n","iteration: 173060 loss: 0.0119 lr: 0.02\n","iteration: 173070 loss: 0.0099 lr: 0.02\n","iteration: 173080 loss: 0.0117 lr: 0.02\n","iteration: 173090 loss: 0.0096 lr: 0.02\n","iteration: 173100 loss: 0.0107 lr: 0.02\n","iteration: 173110 loss: 0.0114 lr: 0.02\n","iteration: 173120 loss: 0.0085 lr: 0.02\n","iteration: 173130 loss: 0.0090 lr: 0.02\n","iteration: 173140 loss: 0.0098 lr: 0.02\n","iteration: 173150 loss: 0.0101 lr: 0.02\n","iteration: 173160 loss: 0.0084 lr: 0.02\n","iteration: 173170 loss: 0.0087 lr: 0.02\n","iteration: 173180 loss: 0.0122 lr: 0.02\n","iteration: 173190 loss: 0.0115 lr: 0.02\n","iteration: 173200 loss: 0.0140 lr: 0.02\n","iteration: 173210 loss: 0.0090 lr: 0.02\n","iteration: 173220 loss: 0.0089 lr: 0.02\n","iteration: 173230 loss: 0.0121 lr: 0.02\n","iteration: 173240 loss: 0.0086 lr: 0.02\n","iteration: 173250 loss: 0.0102 lr: 0.02\n","iteration: 173260 loss: 0.0088 lr: 0.02\n","iteration: 173270 loss: 0.0097 lr: 0.02\n","iteration: 173280 loss: 0.0113 lr: 0.02\n","iteration: 173290 loss: 0.0136 lr: 0.02\n","iteration: 173300 loss: 0.0114 lr: 0.02\n","iteration: 173310 loss: 0.0107 lr: 0.02\n","iteration: 173320 loss: 0.0083 lr: 0.02\n","iteration: 173330 loss: 0.0068 lr: 0.02\n","iteration: 173340 loss: 0.0093 lr: 0.02\n","iteration: 173350 loss: 0.0082 lr: 0.02\n","iteration: 173360 loss: 0.0181 lr: 0.02\n","iteration: 173370 loss: 0.0121 lr: 0.02\n","iteration: 173380 loss: 0.0092 lr: 0.02\n","iteration: 173390 loss: 0.0119 lr: 0.02\n","iteration: 173400 loss: 0.0107 lr: 0.02\n","iteration: 173410 loss: 0.0094 lr: 0.02\n","iteration: 173420 loss: 0.0078 lr: 0.02\n","iteration: 173430 loss: 0.0073 lr: 0.02\n","iteration: 173440 loss: 0.0090 lr: 0.02\n","iteration: 173450 loss: 0.0127 lr: 0.02\n","iteration: 173460 loss: 0.0107 lr: 0.02\n","iteration: 173470 loss: 0.0105 lr: 0.02\n","iteration: 173480 loss: 0.0103 lr: 0.02\n","iteration: 173490 loss: 0.0079 lr: 0.02\n","iteration: 173500 loss: 0.0127 lr: 0.02\n","iteration: 173510 loss: 0.0107 lr: 0.02\n","iteration: 173520 loss: 0.0080 lr: 0.02\n","iteration: 173530 loss: 0.0129 lr: 0.02\n","iteration: 173540 loss: 0.0111 lr: 0.02\n","iteration: 173550 loss: 0.0096 lr: 0.02\n","iteration: 173560 loss: 0.0080 lr: 0.02\n","iteration: 173570 loss: 0.0090 lr: 0.02\n","iteration: 173580 loss: 0.0123 lr: 0.02\n","iteration: 173590 loss: 0.0160 lr: 0.02\n","iteration: 173600 loss: 0.0121 lr: 0.02\n","iteration: 173610 loss: 0.0089 lr: 0.02\n","iteration: 173620 loss: 0.0114 lr: 0.02\n","iteration: 173630 loss: 0.0091 lr: 0.02\n","iteration: 173640 loss: 0.0142 lr: 0.02\n","iteration: 173650 loss: 0.0138 lr: 0.02\n","iteration: 173660 loss: 0.0092 lr: 0.02\n","iteration: 173670 loss: 0.0073 lr: 0.02\n","iteration: 173680 loss: 0.0105 lr: 0.02\n","iteration: 173690 loss: 0.0088 lr: 0.02\n","iteration: 173700 loss: 0.0107 lr: 0.02\n","iteration: 173710 loss: 0.0086 lr: 0.02\n","iteration: 173720 loss: 0.0102 lr: 0.02\n","iteration: 173730 loss: 0.0083 lr: 0.02\n","iteration: 173740 loss: 0.0101 lr: 0.02\n","iteration: 173750 loss: 0.0110 lr: 0.02\n","iteration: 173760 loss: 0.0098 lr: 0.02\n","iteration: 173770 loss: 0.0088 lr: 0.02\n","iteration: 173780 loss: 0.0085 lr: 0.02\n","iteration: 173790 loss: 0.0115 lr: 0.02\n","iteration: 173800 loss: 0.0086 lr: 0.02\n","iteration: 173810 loss: 0.0065 lr: 0.02\n","iteration: 173820 loss: 0.0121 lr: 0.02\n","iteration: 173830 loss: 0.0114 lr: 0.02\n","iteration: 173840 loss: 0.0097 lr: 0.02\n","iteration: 173850 loss: 0.0096 lr: 0.02\n","iteration: 173860 loss: 0.0114 lr: 0.02\n","iteration: 173870 loss: 0.0084 lr: 0.02\n","iteration: 173880 loss: 0.0089 lr: 0.02\n","iteration: 173890 loss: 0.0080 lr: 0.02\n","iteration: 173900 loss: 0.0096 lr: 0.02\n","iteration: 173910 loss: 0.0087 lr: 0.02\n","iteration: 173920 loss: 0.0092 lr: 0.02\n","iteration: 173930 loss: 0.0107 lr: 0.02\n","iteration: 173940 loss: 0.0106 lr: 0.02\n","iteration: 173950 loss: 0.0093 lr: 0.02\n","iteration: 173960 loss: 0.0096 lr: 0.02\n","iteration: 173970 loss: 0.0087 lr: 0.02\n","iteration: 173980 loss: 0.0080 lr: 0.02\n","iteration: 173990 loss: 0.0087 lr: 0.02\n","iteration: 174000 loss: 0.0112 lr: 0.02\n","iteration: 174010 loss: 0.0109 lr: 0.02\n","iteration: 174020 loss: 0.0098 lr: 0.02\n","iteration: 174030 loss: 0.0085 lr: 0.02\n","iteration: 174040 loss: 0.0125 lr: 0.02\n","iteration: 174050 loss: 0.0100 lr: 0.02\n","iteration: 174060 loss: 0.0084 lr: 0.02\n","iteration: 174070 loss: 0.0100 lr: 0.02\n","iteration: 174080 loss: 0.0096 lr: 0.02\n","iteration: 174090 loss: 0.0086 lr: 0.02\n","iteration: 174100 loss: 0.0103 lr: 0.02\n","iteration: 174110 loss: 0.0118 lr: 0.02\n","iteration: 174120 loss: 0.0079 lr: 0.02\n","iteration: 174130 loss: 0.0077 lr: 0.02\n","iteration: 174140 loss: 0.0134 lr: 0.02\n","iteration: 174150 loss: 0.0127 lr: 0.02\n","iteration: 174160 loss: 0.0082 lr: 0.02\n","iteration: 174170 loss: 0.0091 lr: 0.02\n","iteration: 174180 loss: 0.0089 lr: 0.02\n","iteration: 174190 loss: 0.0103 lr: 0.02\n","iteration: 174200 loss: 0.0103 lr: 0.02\n","iteration: 174210 loss: 0.0098 lr: 0.02\n","iteration: 174220 loss: 0.0091 lr: 0.02\n","iteration: 174230 loss: 0.0087 lr: 0.02\n","iteration: 174240 loss: 0.0102 lr: 0.02\n","iteration: 174250 loss: 0.0098 lr: 0.02\n","iteration: 174260 loss: 0.0065 lr: 0.02\n","iteration: 174270 loss: 0.0088 lr: 0.02\n","iteration: 174280 loss: 0.0089 lr: 0.02\n","iteration: 174290 loss: 0.0083 lr: 0.02\n","iteration: 174300 loss: 0.0111 lr: 0.02\n","iteration: 174310 loss: 0.0092 lr: 0.02\n","iteration: 174320 loss: 0.0086 lr: 0.02\n","iteration: 174330 loss: 0.0057 lr: 0.02\n","iteration: 174340 loss: 0.0095 lr: 0.02\n","iteration: 174350 loss: 0.0105 lr: 0.02\n","iteration: 174360 loss: 0.0113 lr: 0.02\n","iteration: 174370 loss: 0.0099 lr: 0.02\n","iteration: 174380 loss: 0.0077 lr: 0.02\n","iteration: 174390 loss: 0.0071 lr: 0.02\n","iteration: 174400 loss: 0.0081 lr: 0.02\n","iteration: 174410 loss: 0.0074 lr: 0.02\n","iteration: 174420 loss: 0.0082 lr: 0.02\n","iteration: 174430 loss: 0.0076 lr: 0.02\n","iteration: 174440 loss: 0.0092 lr: 0.02\n","iteration: 174450 loss: 0.0091 lr: 0.02\n","iteration: 174460 loss: 0.0117 lr: 0.02\n","iteration: 174470 loss: 0.0065 lr: 0.02\n","iteration: 174480 loss: 0.0118 lr: 0.02\n","iteration: 174490 loss: 0.0075 lr: 0.02\n","iteration: 174500 loss: 0.0116 lr: 0.02\n","iteration: 174510 loss: 0.0110 lr: 0.02\n","iteration: 174520 loss: 0.0102 lr: 0.02\n","iteration: 174530 loss: 0.0080 lr: 0.02\n","iteration: 174540 loss: 0.0083 lr: 0.02\n","iteration: 174550 loss: 0.0104 lr: 0.02\n","iteration: 174560 loss: 0.0093 lr: 0.02\n","iteration: 174570 loss: 0.0114 lr: 0.02\n","iteration: 174580 loss: 0.0067 lr: 0.02\n","iteration: 174590 loss: 0.0081 lr: 0.02\n","iteration: 174600 loss: 0.0077 lr: 0.02\n","iteration: 174610 loss: 0.0105 lr: 0.02\n","iteration: 174620 loss: 0.0103 lr: 0.02\n","iteration: 174630 loss: 0.0110 lr: 0.02\n","iteration: 174640 loss: 0.0134 lr: 0.02\n","iteration: 174650 loss: 0.0083 lr: 0.02\n","iteration: 174660 loss: 0.0106 lr: 0.02\n","iteration: 174670 loss: 0.0099 lr: 0.02\n","iteration: 174680 loss: 0.0125 lr: 0.02\n","iteration: 174690 loss: 0.0144 lr: 0.02\n","iteration: 174700 loss: 0.0105 lr: 0.02\n","iteration: 174710 loss: 0.0102 lr: 0.02\n","iteration: 174720 loss: 0.0148 lr: 0.02\n","iteration: 174730 loss: 0.0111 lr: 0.02\n","iteration: 174740 loss: 0.0091 lr: 0.02\n","iteration: 174750 loss: 0.0062 lr: 0.02\n","iteration: 174760 loss: 0.0105 lr: 0.02\n","iteration: 174770 loss: 0.0082 lr: 0.02\n","iteration: 174780 loss: 0.0070 lr: 0.02\n","iteration: 174790 loss: 0.0073 lr: 0.02\n","iteration: 174800 loss: 0.0111 lr: 0.02\n","iteration: 174810 loss: 0.0105 lr: 0.02\n","iteration: 174820 loss: 0.0095 lr: 0.02\n","iteration: 174830 loss: 0.0077 lr: 0.02\n","iteration: 174840 loss: 0.0107 lr: 0.02\n","iteration: 174850 loss: 0.0090 lr: 0.02\n","iteration: 174860 loss: 0.0081 lr: 0.02\n","iteration: 174870 loss: 0.0130 lr: 0.02\n","iteration: 174880 loss: 0.0070 lr: 0.02\n","iteration: 174890 loss: 0.0124 lr: 0.02\n","iteration: 174900 loss: 0.0079 lr: 0.02\n","iteration: 174910 loss: 0.0122 lr: 0.02\n","iteration: 174920 loss: 0.0108 lr: 0.02\n","iteration: 174930 loss: 0.0109 lr: 0.02\n","iteration: 174940 loss: 0.0069 lr: 0.02\n","iteration: 174950 loss: 0.0081 lr: 0.02\n","iteration: 174960 loss: 0.0093 lr: 0.02\n","iteration: 174970 loss: 0.0091 lr: 0.02\n","iteration: 174980 loss: 0.0081 lr: 0.02\n","iteration: 174990 loss: 0.0111 lr: 0.02\n","iteration: 175000 loss: 0.0101 lr: 0.02\n","iteration: 175010 loss: 0.0121 lr: 0.02\n","iteration: 175020 loss: 0.0088 lr: 0.02\n","iteration: 175030 loss: 0.0103 lr: 0.02\n","iteration: 175040 loss: 0.0115 lr: 0.02\n","iteration: 175050 loss: 0.0074 lr: 0.02\n","iteration: 175060 loss: 0.0100 lr: 0.02\n","iteration: 175070 loss: 0.0098 lr: 0.02\n","iteration: 175080 loss: 0.0099 lr: 0.02\n","iteration: 175090 loss: 0.0116 lr: 0.02\n","iteration: 175100 loss: 0.0118 lr: 0.02\n","iteration: 175110 loss: 0.0114 lr: 0.02\n","iteration: 175120 loss: 0.0110 lr: 0.02\n","iteration: 175130 loss: 0.0093 lr: 0.02\n","iteration: 175140 loss: 0.0097 lr: 0.02\n","iteration: 175150 loss: 0.0102 lr: 0.02\n","iteration: 175160 loss: 0.0104 lr: 0.02\n","iteration: 175170 loss: 0.0098 lr: 0.02\n","iteration: 175180 loss: 0.0108 lr: 0.02\n","iteration: 175190 loss: 0.0089 lr: 0.02\n","iteration: 175200 loss: 0.0083 lr: 0.02\n","iteration: 175210 loss: 0.0152 lr: 0.02\n","iteration: 175220 loss: 0.0122 lr: 0.02\n","iteration: 175230 loss: 0.0089 lr: 0.02\n","iteration: 175240 loss: 0.0085 lr: 0.02\n","iteration: 175250 loss: 0.0089 lr: 0.02\n","iteration: 175260 loss: 0.0081 lr: 0.02\n","iteration: 175270 loss: 0.0094 lr: 0.02\n","iteration: 175280 loss: 0.0095 lr: 0.02\n","iteration: 175290 loss: 0.0101 lr: 0.02\n","iteration: 175300 loss: 0.0111 lr: 0.02\n","iteration: 175310 loss: 0.0108 lr: 0.02\n","iteration: 175320 loss: 0.0102 lr: 0.02\n","iteration: 175330 loss: 0.0095 lr: 0.02\n","iteration: 175340 loss: 0.0082 lr: 0.02\n","iteration: 175350 loss: 0.0138 lr: 0.02\n","iteration: 175360 loss: 0.0133 lr: 0.02\n","iteration: 175370 loss: 0.0082 lr: 0.02\n","iteration: 175380 loss: 0.0085 lr: 0.02\n","iteration: 175390 loss: 0.0104 lr: 0.02\n","iteration: 175400 loss: 0.0102 lr: 0.02\n","iteration: 175410 loss: 0.0100 lr: 0.02\n","iteration: 175420 loss: 0.0127 lr: 0.02\n","iteration: 175430 loss: 0.0109 lr: 0.02\n","iteration: 175440 loss: 0.0091 lr: 0.02\n","iteration: 175450 loss: 0.0078 lr: 0.02\n","iteration: 175460 loss: 0.0091 lr: 0.02\n","iteration: 175470 loss: 0.0073 lr: 0.02\n","iteration: 175480 loss: 0.0102 lr: 0.02\n","iteration: 175490 loss: 0.0104 lr: 0.02\n","iteration: 175500 loss: 0.0146 lr: 0.02\n","iteration: 175510 loss: 0.0127 lr: 0.02\n","iteration: 175520 loss: 0.0092 lr: 0.02\n","iteration: 175530 loss: 0.0093 lr: 0.02\n","iteration: 175540 loss: 0.0105 lr: 0.02\n","iteration: 175550 loss: 0.0080 lr: 0.02\n","iteration: 175560 loss: 0.0108 lr: 0.02\n","iteration: 175570 loss: 0.0111 lr: 0.02\n","iteration: 175580 loss: 0.0103 lr: 0.02\n","iteration: 175590 loss: 0.0092 lr: 0.02\n","iteration: 175600 loss: 0.0096 lr: 0.02\n","iteration: 175610 loss: 0.0091 lr: 0.02\n","iteration: 175620 loss: 0.0086 lr: 0.02\n","iteration: 175630 loss: 0.0108 lr: 0.02\n","iteration: 175640 loss: 0.0098 lr: 0.02\n","iteration: 175650 loss: 0.0166 lr: 0.02\n","iteration: 175660 loss: 0.0091 lr: 0.02\n","iteration: 175670 loss: 0.0088 lr: 0.02\n","iteration: 175680 loss: 0.0086 lr: 0.02\n","iteration: 175690 loss: 0.0107 lr: 0.02\n","iteration: 175700 loss: 0.0114 lr: 0.02\n","iteration: 175710 loss: 0.0146 lr: 0.02\n","iteration: 175720 loss: 0.0109 lr: 0.02\n","iteration: 175730 loss: 0.0088 lr: 0.02\n","iteration: 175740 loss: 0.0064 lr: 0.02\n","iteration: 175750 loss: 0.0116 lr: 0.02\n","iteration: 175760 loss: 0.0095 lr: 0.02\n","iteration: 175770 loss: 0.0092 lr: 0.02\n","iteration: 175780 loss: 0.0084 lr: 0.02\n","iteration: 175790 loss: 0.0082 lr: 0.02\n","iteration: 175800 loss: 0.0087 lr: 0.02\n","iteration: 175810 loss: 0.0089 lr: 0.02\n","iteration: 175820 loss: 0.0091 lr: 0.02\n","iteration: 175830 loss: 0.0077 lr: 0.02\n","iteration: 175840 loss: 0.0094 lr: 0.02\n","iteration: 175850 loss: 0.0091 lr: 0.02\n","iteration: 175860 loss: 0.0110 lr: 0.02\n","iteration: 175870 loss: 0.0086 lr: 0.02\n","iteration: 175880 loss: 0.0073 lr: 0.02\n","iteration: 175890 loss: 0.0110 lr: 0.02\n","iteration: 175900 loss: 0.0084 lr: 0.02\n","iteration: 175910 loss: 0.0080 lr: 0.02\n","iteration: 175920 loss: 0.0094 lr: 0.02\n","iteration: 175930 loss: 0.0081 lr: 0.02\n","iteration: 175940 loss: 0.0114 lr: 0.02\n","iteration: 175950 loss: 0.0071 lr: 0.02\n","iteration: 175960 loss: 0.0092 lr: 0.02\n","iteration: 175970 loss: 0.0140 lr: 0.02\n","iteration: 175980 loss: 0.0100 lr: 0.02\n","iteration: 175990 loss: 0.0113 lr: 0.02\n","iteration: 176000 loss: 0.0105 lr: 0.02\n","iteration: 176010 loss: 0.0098 lr: 0.02\n","iteration: 176020 loss: 0.0105 lr: 0.02\n","iteration: 176030 loss: 0.0122 lr: 0.02\n","iteration: 176040 loss: 0.0156 lr: 0.02\n","iteration: 176050 loss: 0.0090 lr: 0.02\n","iteration: 176060 loss: 0.0138 lr: 0.02\n","iteration: 176070 loss: 0.0125 lr: 0.02\n","iteration: 176080 loss: 0.0124 lr: 0.02\n","iteration: 176090 loss: 0.0122 lr: 0.02\n","iteration: 176100 loss: 0.0104 lr: 0.02\n","iteration: 176110 loss: 0.0127 lr: 0.02\n","iteration: 176120 loss: 0.0090 lr: 0.02\n","iteration: 176130 loss: 0.0116 lr: 0.02\n","iteration: 176140 loss: 0.0096 lr: 0.02\n","iteration: 176150 loss: 0.0073 lr: 0.02\n","iteration: 176160 loss: 0.0112 lr: 0.02\n","iteration: 176170 loss: 0.0083 lr: 0.02\n","iteration: 176180 loss: 0.0083 lr: 0.02\n","iteration: 176190 loss: 0.0087 lr: 0.02\n","iteration: 176200 loss: 0.0093 lr: 0.02\n","iteration: 176210 loss: 0.0112 lr: 0.02\n","iteration: 176220 loss: 0.0114 lr: 0.02\n","iteration: 176230 loss: 0.0228 lr: 0.02\n","iteration: 176240 loss: 0.0105 lr: 0.02\n","iteration: 176250 loss: 0.0097 lr: 0.02\n","iteration: 176260 loss: 0.0137 lr: 0.02\n","iteration: 176270 loss: 0.0120 lr: 0.02\n","iteration: 176280 loss: 0.0139 lr: 0.02\n","iteration: 176290 loss: 0.0099 lr: 0.02\n","iteration: 176300 loss: 0.0079 lr: 0.02\n","iteration: 176310 loss: 0.0095 lr: 0.02\n","iteration: 176320 loss: 0.0102 lr: 0.02\n","iteration: 176330 loss: 0.0082 lr: 0.02\n","iteration: 176340 loss: 0.0149 lr: 0.02\n","iteration: 176350 loss: 0.0111 lr: 0.02\n","iteration: 176360 loss: 0.0108 lr: 0.02\n","iteration: 176370 loss: 0.0090 lr: 0.02\n","iteration: 176380 loss: 0.0096 lr: 0.02\n","iteration: 176390 loss: 0.0110 lr: 0.02\n","iteration: 176400 loss: 0.0124 lr: 0.02\n","iteration: 176410 loss: 0.0072 lr: 0.02\n","iteration: 176420 loss: 0.0095 lr: 0.02\n","iteration: 176430 loss: 0.0103 lr: 0.02\n","iteration: 176440 loss: 0.0135 lr: 0.02\n","iteration: 176450 loss: 0.0097 lr: 0.02\n","iteration: 176460 loss: 0.0099 lr: 0.02\n","iteration: 176470 loss: 0.0120 lr: 0.02\n","iteration: 176480 loss: 0.0075 lr: 0.02\n","iteration: 176490 loss: 0.0106 lr: 0.02\n","iteration: 176500 loss: 0.0119 lr: 0.02\n","iteration: 176510 loss: 0.0129 lr: 0.02\n","iteration: 176520 loss: 0.0103 lr: 0.02\n","iteration: 176530 loss: 0.0090 lr: 0.02\n","iteration: 176540 loss: 0.0159 lr: 0.02\n","iteration: 176550 loss: 0.0087 lr: 0.02\n","iteration: 176560 loss: 0.0092 lr: 0.02\n","iteration: 176570 loss: 0.0098 lr: 0.02\n","iteration: 176580 loss: 0.0097 lr: 0.02\n","iteration: 176590 loss: 0.0080 lr: 0.02\n","iteration: 176600 loss: 0.0096 lr: 0.02\n","iteration: 176610 loss: 0.0086 lr: 0.02\n","iteration: 176620 loss: 0.0090 lr: 0.02\n","iteration: 176630 loss: 0.0094 lr: 0.02\n","iteration: 176640 loss: 0.0096 lr: 0.02\n","iteration: 176650 loss: 0.0096 lr: 0.02\n","iteration: 176660 loss: 0.0168 lr: 0.02\n","iteration: 176670 loss: 0.0119 lr: 0.02\n","iteration: 176680 loss: 0.0089 lr: 0.02\n","iteration: 176690 loss: 0.0141 lr: 0.02\n","iteration: 176700 loss: 0.0093 lr: 0.02\n","iteration: 176710 loss: 0.0121 lr: 0.02\n","iteration: 176720 loss: 0.0080 lr: 0.02\n","iteration: 176730 loss: 0.0082 lr: 0.02\n","iteration: 176740 loss: 0.0114 lr: 0.02\n","iteration: 176750 loss: 0.0091 lr: 0.02\n","iteration: 176760 loss: 0.0093 lr: 0.02\n","iteration: 176770 loss: 0.0098 lr: 0.02\n","iteration: 176780 loss: 0.0090 lr: 0.02\n","iteration: 176790 loss: 0.0092 lr: 0.02\n","iteration: 176800 loss: 0.0088 lr: 0.02\n","iteration: 176810 loss: 0.0081 lr: 0.02\n","iteration: 176820 loss: 0.0114 lr: 0.02\n","iteration: 176830 loss: 0.0131 lr: 0.02\n","iteration: 176840 loss: 0.0121 lr: 0.02\n","iteration: 176850 loss: 0.0104 lr: 0.02\n","iteration: 176860 loss: 0.0109 lr: 0.02\n","iteration: 176870 loss: 0.0107 lr: 0.02\n","iteration: 176880 loss: 0.0087 lr: 0.02\n","iteration: 176890 loss: 0.0068 lr: 0.02\n","iteration: 176900 loss: 0.0116 lr: 0.02\n","iteration: 176910 loss: 0.0105 lr: 0.02\n","iteration: 176920 loss: 0.0126 lr: 0.02\n","iteration: 176930 loss: 0.0122 lr: 0.02\n","iteration: 176940 loss: 0.0093 lr: 0.02\n","iteration: 176950 loss: 0.0105 lr: 0.02\n","iteration: 176960 loss: 0.0085 lr: 0.02\n","iteration: 176970 loss: 0.0095 lr: 0.02\n","iteration: 176980 loss: 0.0121 lr: 0.02\n","iteration: 176990 loss: 0.0087 lr: 0.02\n","iteration: 177000 loss: 0.0093 lr: 0.02\n","iteration: 177010 loss: 0.0092 lr: 0.02\n","iteration: 177020 loss: 0.0094 lr: 0.02\n","iteration: 177030 loss: 0.0104 lr: 0.02\n","iteration: 177040 loss: 0.0116 lr: 0.02\n","iteration: 177050 loss: 0.0101 lr: 0.02\n","iteration: 177060 loss: 0.0110 lr: 0.02\n","iteration: 177070 loss: 0.0108 lr: 0.02\n","iteration: 177080 loss: 0.0111 lr: 0.02\n","iteration: 177090 loss: 0.0101 lr: 0.02\n","iteration: 177100 loss: 0.0110 lr: 0.02\n","iteration: 177110 loss: 0.0079 lr: 0.02\n","iteration: 177120 loss: 0.0105 lr: 0.02\n","iteration: 177130 loss: 0.0087 lr: 0.02\n","iteration: 177140 loss: 0.0084 lr: 0.02\n","iteration: 177150 loss: 0.0107 lr: 0.02\n","iteration: 177160 loss: 0.0068 lr: 0.02\n","iteration: 177170 loss: 0.0078 lr: 0.02\n","iteration: 177180 loss: 0.0083 lr: 0.02\n","iteration: 177190 loss: 0.0089 lr: 0.02\n","iteration: 177200 loss: 0.0097 lr: 0.02\n","iteration: 177210 loss: 0.0122 lr: 0.02\n","iteration: 177220 loss: 0.0121 lr: 0.02\n","iteration: 177230 loss: 0.0101 lr: 0.02\n","iteration: 177240 loss: 0.0124 lr: 0.02\n","iteration: 177250 loss: 0.0087 lr: 0.02\n","iteration: 177260 loss: 0.0135 lr: 0.02\n","iteration: 177270 loss: 0.0111 lr: 0.02\n","iteration: 177280 loss: 0.0109 lr: 0.02\n","iteration: 177290 loss: 0.0081 lr: 0.02\n","iteration: 177300 loss: 0.0108 lr: 0.02\n","iteration: 177310 loss: 0.0116 lr: 0.02\n","iteration: 177320 loss: 0.0094 lr: 0.02\n","iteration: 177330 loss: 0.0103 lr: 0.02\n","iteration: 177340 loss: 0.0096 lr: 0.02\n","iteration: 177350 loss: 0.0104 lr: 0.02\n","iteration: 177360 loss: 0.0107 lr: 0.02\n","iteration: 177370 loss: 0.0103 lr: 0.02\n","iteration: 177380 loss: 0.0128 lr: 0.02\n","iteration: 177390 loss: 0.0105 lr: 0.02\n","iteration: 177400 loss: 0.0075 lr: 0.02\n","iteration: 177410 loss: 0.0066 lr: 0.02\n","iteration: 177420 loss: 0.0079 lr: 0.02\n","iteration: 177430 loss: 0.0095 lr: 0.02\n","iteration: 177440 loss: 0.0090 lr: 0.02\n","iteration: 177450 loss: 0.0091 lr: 0.02\n","iteration: 177460 loss: 0.0097 lr: 0.02\n","iteration: 177470 loss: 0.0098 lr: 0.02\n","iteration: 177480 loss: 0.0097 lr: 0.02\n","iteration: 177490 loss: 0.0117 lr: 0.02\n","iteration: 177500 loss: 0.0055 lr: 0.02\n","iteration: 177510 loss: 0.0089 lr: 0.02\n","iteration: 177520 loss: 0.0118 lr: 0.02\n","iteration: 177530 loss: 0.0077 lr: 0.02\n","iteration: 177540 loss: 0.0126 lr: 0.02\n","iteration: 177550 loss: 0.0197 lr: 0.02\n","iteration: 177560 loss: 0.0121 lr: 0.02\n","iteration: 177570 loss: 0.0105 lr: 0.02\n","iteration: 177580 loss: 0.0131 lr: 0.02\n","iteration: 177590 loss: 0.0086 lr: 0.02\n","iteration: 177600 loss: 0.0091 lr: 0.02\n","iteration: 177610 loss: 0.0089 lr: 0.02\n","iteration: 177620 loss: 0.0083 lr: 0.02\n","iteration: 177630 loss: 0.0116 lr: 0.02\n","iteration: 177640 loss: 0.0080 lr: 0.02\n","iteration: 177650 loss: 0.0105 lr: 0.02\n","iteration: 177660 loss: 0.0122 lr: 0.02\n","iteration: 177670 loss: 0.0112 lr: 0.02\n","iteration: 177680 loss: 0.0130 lr: 0.02\n","iteration: 177690 loss: 0.0109 lr: 0.02\n","iteration: 177700 loss: 0.0098 lr: 0.02\n","iteration: 177710 loss: 0.0122 lr: 0.02\n","iteration: 177720 loss: 0.0113 lr: 0.02\n","iteration: 177730 loss: 0.0094 lr: 0.02\n","iteration: 177740 loss: 0.0101 lr: 0.02\n","iteration: 177750 loss: 0.0086 lr: 0.02\n","iteration: 177760 loss: 0.0081 lr: 0.02\n","iteration: 177770 loss: 0.0095 lr: 0.02\n","iteration: 177780 loss: 0.0110 lr: 0.02\n","iteration: 177790 loss: 0.0088 lr: 0.02\n","iteration: 177800 loss: 0.0135 lr: 0.02\n","iteration: 177810 loss: 0.0119 lr: 0.02\n","iteration: 177820 loss: 0.0099 lr: 0.02\n","iteration: 177830 loss: 0.0111 lr: 0.02\n","iteration: 177840 loss: 0.0098 lr: 0.02\n","iteration: 177850 loss: 0.0111 lr: 0.02\n","iteration: 177860 loss: 0.0077 lr: 0.02\n","iteration: 177870 loss: 0.0113 lr: 0.02\n","iteration: 177880 loss: 0.0094 lr: 0.02\n","iteration: 177890 loss: 0.0085 lr: 0.02\n","iteration: 177900 loss: 0.0082 lr: 0.02\n","iteration: 177910 loss: 0.0110 lr: 0.02\n","iteration: 177920 loss: 0.0117 lr: 0.02\n","iteration: 177930 loss: 0.0081 lr: 0.02\n","iteration: 177940 loss: 0.0097 lr: 0.02\n","iteration: 177950 loss: 0.0095 lr: 0.02\n","iteration: 177960 loss: 0.0101 lr: 0.02\n","iteration: 177970 loss: 0.0111 lr: 0.02\n","iteration: 177980 loss: 0.0101 lr: 0.02\n","iteration: 177990 loss: 0.0108 lr: 0.02\n","iteration: 178000 loss: 0.0093 lr: 0.02\n","iteration: 178010 loss: 0.0122 lr: 0.02\n","iteration: 178020 loss: 0.0104 lr: 0.02\n","iteration: 178030 loss: 0.0119 lr: 0.02\n","iteration: 178040 loss: 0.0090 lr: 0.02\n","iteration: 178050 loss: 0.0060 lr: 0.02\n","iteration: 178060 loss: 0.0093 lr: 0.02\n","iteration: 178070 loss: 0.0118 lr: 0.02\n","iteration: 178080 loss: 0.0094 lr: 0.02\n","iteration: 178090 loss: 0.0115 lr: 0.02\n","iteration: 178100 loss: 0.0078 lr: 0.02\n","iteration: 178110 loss: 0.0093 lr: 0.02\n","iteration: 178120 loss: 0.0070 lr: 0.02\n","iteration: 178130 loss: 0.0086 lr: 0.02\n","iteration: 178140 loss: 0.0099 lr: 0.02\n","iteration: 178150 loss: 0.0112 lr: 0.02\n","iteration: 178160 loss: 0.0121 lr: 0.02\n","iteration: 178170 loss: 0.0100 lr: 0.02\n","iteration: 178180 loss: 0.0094 lr: 0.02\n","iteration: 178190 loss: 0.0100 lr: 0.02\n","iteration: 178200 loss: 0.0101 lr: 0.02\n","iteration: 178210 loss: 0.0083 lr: 0.02\n","iteration: 178220 loss: 0.0125 lr: 0.02\n","iteration: 178230 loss: 0.0085 lr: 0.02\n","iteration: 178240 loss: 0.0080 lr: 0.02\n","iteration: 178250 loss: 0.0071 lr: 0.02\n","iteration: 178260 loss: 0.0082 lr: 0.02\n","iteration: 178270 loss: 0.0106 lr: 0.02\n","iteration: 178280 loss: 0.0085 lr: 0.02\n","iteration: 178290 loss: 0.0083 lr: 0.02\n","iteration: 178300 loss: 0.0077 lr: 0.02\n","iteration: 178310 loss: 0.0078 lr: 0.02\n","iteration: 178320 loss: 0.0068 lr: 0.02\n","iteration: 178330 loss: 0.0114 lr: 0.02\n","iteration: 178340 loss: 0.0089 lr: 0.02\n","iteration: 178350 loss: 0.0104 lr: 0.02\n","iteration: 178360 loss: 0.0112 lr: 0.02\n","iteration: 178370 loss: 0.0131 lr: 0.02\n","iteration: 178380 loss: 0.0089 lr: 0.02\n","iteration: 178390 loss: 0.0101 lr: 0.02\n","iteration: 178400 loss: 0.0079 lr: 0.02\n","iteration: 178410 loss: 0.0097 lr: 0.02\n","iteration: 178420 loss: 0.0084 lr: 0.02\n","iteration: 178430 loss: 0.0096 lr: 0.02\n","iteration: 178440 loss: 0.0099 lr: 0.02\n","iteration: 178450 loss: 0.0114 lr: 0.02\n","iteration: 178460 loss: 0.0093 lr: 0.02\n","iteration: 178470 loss: 0.0065 lr: 0.02\n","iteration: 178480 loss: 0.0117 lr: 0.02\n","iteration: 178490 loss: 0.0087 lr: 0.02\n","iteration: 178500 loss: 0.0069 lr: 0.02\n","iteration: 178510 loss: 0.0103 lr: 0.02\n","iteration: 178520 loss: 0.0097 lr: 0.02\n","iteration: 178530 loss: 0.0101 lr: 0.02\n","iteration: 178540 loss: 0.0106 lr: 0.02\n","iteration: 178550 loss: 0.0069 lr: 0.02\n","iteration: 178560 loss: 0.0096 lr: 0.02\n","iteration: 178570 loss: 0.0063 lr: 0.02\n","iteration: 178580 loss: 0.0063 lr: 0.02\n","iteration: 178590 loss: 0.0077 lr: 0.02\n","iteration: 178600 loss: 0.0119 lr: 0.02\n","iteration: 178610 loss: 0.0064 lr: 0.02\n","iteration: 178620 loss: 0.0144 lr: 0.02\n","iteration: 178630 loss: 0.0126 lr: 0.02\n","iteration: 178640 loss: 0.0113 lr: 0.02\n","iteration: 178650 loss: 0.0094 lr: 0.02\n","iteration: 178660 loss: 0.0091 lr: 0.02\n","iteration: 178670 loss: 0.0086 lr: 0.02\n","iteration: 178680 loss: 0.0123 lr: 0.02\n","iteration: 178690 loss: 0.0107 lr: 0.02\n","iteration: 178700 loss: 0.0099 lr: 0.02\n","iteration: 178710 loss: 0.0098 lr: 0.02\n","iteration: 178720 loss: 0.0100 lr: 0.02\n","iteration: 178730 loss: 0.0103 lr: 0.02\n","iteration: 178740 loss: 0.0124 lr: 0.02\n","iteration: 178750 loss: 0.0122 lr: 0.02\n","iteration: 178760 loss: 0.0107 lr: 0.02\n","iteration: 178770 loss: 0.0096 lr: 0.02\n","iteration: 178780 loss: 0.0093 lr: 0.02\n","iteration: 178790 loss: 0.0091 lr: 0.02\n","iteration: 178800 loss: 0.0089 lr: 0.02\n","iteration: 178810 loss: 0.0090 lr: 0.02\n","iteration: 178820 loss: 0.0102 lr: 0.02\n","iteration: 178830 loss: 0.0080 lr: 0.02\n","iteration: 178840 loss: 0.0091 lr: 0.02\n","iteration: 178850 loss: 0.0109 lr: 0.02\n","iteration: 178860 loss: 0.0123 lr: 0.02\n","iteration: 178870 loss: 0.0084 lr: 0.02\n","iteration: 178880 loss: 0.0087 lr: 0.02\n","iteration: 178890 loss: 0.0127 lr: 0.02\n","iteration: 178900 loss: 0.0119 lr: 0.02\n","iteration: 178910 loss: 0.0094 lr: 0.02\n","iteration: 178920 loss: 0.0097 lr: 0.02\n","iteration: 178930 loss: 0.0083 lr: 0.02\n","iteration: 178940 loss: 0.0081 lr: 0.02\n","iteration: 178950 loss: 0.0119 lr: 0.02\n","iteration: 178960 loss: 0.0095 lr: 0.02\n","iteration: 178970 loss: 0.0113 lr: 0.02\n","iteration: 178980 loss: 0.0093 lr: 0.02\n","iteration: 178990 loss: 0.0105 lr: 0.02\n","iteration: 179000 loss: 0.0090 lr: 0.02\n","iteration: 179010 loss: 0.0082 lr: 0.02\n","iteration: 179020 loss: 0.0096 lr: 0.02\n","iteration: 179030 loss: 0.0075 lr: 0.02\n","iteration: 179040 loss: 0.0078 lr: 0.02\n","iteration: 179050 loss: 0.0081 lr: 0.02\n","iteration: 179060 loss: 0.0073 lr: 0.02\n","iteration: 179070 loss: 0.0094 lr: 0.02\n","iteration: 179080 loss: 0.0087 lr: 0.02\n","iteration: 179090 loss: 0.0134 lr: 0.02\n","iteration: 179100 loss: 0.0096 lr: 0.02\n","iteration: 179110 loss: 0.0092 lr: 0.02\n","iteration: 179120 loss: 0.0106 lr: 0.02\n","iteration: 179130 loss: 0.0098 lr: 0.02\n","iteration: 179140 loss: 0.0093 lr: 0.02\n","iteration: 179150 loss: 0.0089 lr: 0.02\n","iteration: 179160 loss: 0.0111 lr: 0.02\n","iteration: 179170 loss: 0.0093 lr: 0.02\n","iteration: 179180 loss: 0.0125 lr: 0.02\n","iteration: 179190 loss: 0.0087 lr: 0.02\n","iteration: 179200 loss: 0.0089 lr: 0.02\n","iteration: 179210 loss: 0.0102 lr: 0.02\n","iteration: 179220 loss: 0.0099 lr: 0.02\n","iteration: 179230 loss: 0.0139 lr: 0.02\n","iteration: 179240 loss: 0.0094 lr: 0.02\n","iteration: 179250 loss: 0.0089 lr: 0.02\n","iteration: 179260 loss: 0.0074 lr: 0.02\n","iteration: 179270 loss: 0.0091 lr: 0.02\n","iteration: 179280 loss: 0.0114 lr: 0.02\n","iteration: 179290 loss: 0.0091 lr: 0.02\n","iteration: 179300 loss: 0.0089 lr: 0.02\n","iteration: 179310 loss: 0.0107 lr: 0.02\n","iteration: 179320 loss: 0.0077 lr: 0.02\n","iteration: 179330 loss: 0.0097 lr: 0.02\n","iteration: 179340 loss: 0.0088 lr: 0.02\n","iteration: 179350 loss: 0.0088 lr: 0.02\n","iteration: 179360 loss: 0.0082 lr: 0.02\n","iteration: 179370 loss: 0.0085 lr: 0.02\n","iteration: 179380 loss: 0.0102 lr: 0.02\n","iteration: 179390 loss: 0.0093 lr: 0.02\n","iteration: 179400 loss: 0.0077 lr: 0.02\n","iteration: 179410 loss: 0.0081 lr: 0.02\n","iteration: 179420 loss: 0.0093 lr: 0.02\n","iteration: 179430 loss: 0.0085 lr: 0.02\n","iteration: 179440 loss: 0.0087 lr: 0.02\n","iteration: 179450 loss: 0.0102 lr: 0.02\n","iteration: 179460 loss: 0.0085 lr: 0.02\n","iteration: 179470 loss: 0.0100 lr: 0.02\n","iteration: 179480 loss: 0.0119 lr: 0.02\n","iteration: 179490 loss: 0.0100 lr: 0.02\n","iteration: 179500 loss: 0.0069 lr: 0.02\n","iteration: 179510 loss: 0.0081 lr: 0.02\n","iteration: 179520 loss: 0.0070 lr: 0.02\n","iteration: 179530 loss: 0.0148 lr: 0.02\n","iteration: 179540 loss: 0.0062 lr: 0.02\n","iteration: 179550 loss: 0.0117 lr: 0.02\n","iteration: 179560 loss: 0.0081 lr: 0.02\n","iteration: 179570 loss: 0.0082 lr: 0.02\n","iteration: 179580 loss: 0.0094 lr: 0.02\n","iteration: 179590 loss: 0.0084 lr: 0.02\n","iteration: 179600 loss: 0.0115 lr: 0.02\n","iteration: 179610 loss: 0.0105 lr: 0.02\n","iteration: 179620 loss: 0.0091 lr: 0.02\n","iteration: 179630 loss: 0.0111 lr: 0.02\n","iteration: 179640 loss: 0.0117 lr: 0.02\n","iteration: 179650 loss: 0.0111 lr: 0.02\n","iteration: 179660 loss: 0.0104 lr: 0.02\n","iteration: 179670 loss: 0.0113 lr: 0.02\n","iteration: 179680 loss: 0.0113 lr: 0.02\n","iteration: 179690 loss: 0.0079 lr: 0.02\n","iteration: 179700 loss: 0.0095 lr: 0.02\n","iteration: 179710 loss: 0.0102 lr: 0.02\n","iteration: 179720 loss: 0.0115 lr: 0.02\n","iteration: 179730 loss: 0.0100 lr: 0.02\n","iteration: 179740 loss: 0.0080 lr: 0.02\n","iteration: 179750 loss: 0.0128 lr: 0.02\n","iteration: 179760 loss: 0.0114 lr: 0.02\n","iteration: 179770 loss: 0.0081 lr: 0.02\n","iteration: 179780 loss: 0.0133 lr: 0.02\n","iteration: 179790 loss: 0.0095 lr: 0.02\n","iteration: 179800 loss: 0.0118 lr: 0.02\n","iteration: 179810 loss: 0.0098 lr: 0.02\n","iteration: 179820 loss: 0.0144 lr: 0.02\n","iteration: 179830 loss: 0.0137 lr: 0.02\n","iteration: 179840 loss: 0.0105 lr: 0.02\n","iteration: 179850 loss: 0.0081 lr: 0.02\n","iteration: 179860 loss: 0.0060 lr: 0.02\n","iteration: 179870 loss: 0.0121 lr: 0.02\n","iteration: 179880 loss: 0.0125 lr: 0.02\n","iteration: 179890 loss: 0.0108 lr: 0.02\n","iteration: 179900 loss: 0.0076 lr: 0.02\n","iteration: 179910 loss: 0.0115 lr: 0.02\n","iteration: 179920 loss: 0.0112 lr: 0.02\n","iteration: 179930 loss: 0.0130 lr: 0.02\n","iteration: 179940 loss: 0.0077 lr: 0.02\n","iteration: 179950 loss: 0.0098 lr: 0.02\n","iteration: 179960 loss: 0.0127 lr: 0.02\n","iteration: 179970 loss: 0.0090 lr: 0.02\n","iteration: 179980 loss: 0.0105 lr: 0.02\n","iteration: 179990 loss: 0.0131 lr: 0.02\n","iteration: 180000 loss: 0.0080 lr: 0.02\n","iteration: 180010 loss: 0.0092 lr: 0.02\n","iteration: 180020 loss: 0.0093 lr: 0.02\n","iteration: 180030 loss: 0.0088 lr: 0.02\n","iteration: 180040 loss: 0.0095 lr: 0.02\n","iteration: 180050 loss: 0.0105 lr: 0.02\n","iteration: 180060 loss: 0.0089 lr: 0.02\n","iteration: 180070 loss: 0.0094 lr: 0.02\n","iteration: 180080 loss: 0.0111 lr: 0.02\n","iteration: 180090 loss: 0.0087 lr: 0.02\n","iteration: 180100 loss: 0.0106 lr: 0.02\n","iteration: 180110 loss: 0.0090 lr: 0.02\n","iteration: 180120 loss: 0.0090 lr: 0.02\n","iteration: 180130 loss: 0.0096 lr: 0.02\n","iteration: 180140 loss: 0.0086 lr: 0.02\n","iteration: 180150 loss: 0.0075 lr: 0.02\n","iteration: 180160 loss: 0.0076 lr: 0.02\n","iteration: 180170 loss: 0.0081 lr: 0.02\n","iteration: 180180 loss: 0.0100 lr: 0.02\n","iteration: 180190 loss: 0.0078 lr: 0.02\n","iteration: 180200 loss: 0.0092 lr: 0.02\n","iteration: 180210 loss: 0.0084 lr: 0.02\n","iteration: 180220 loss: 0.0094 lr: 0.02\n","iteration: 180230 loss: 0.0088 lr: 0.02\n","iteration: 180240 loss: 0.0082 lr: 0.02\n","iteration: 180250 loss: 0.0081 lr: 0.02\n","iteration: 180260 loss: 0.0083 lr: 0.02\n","iteration: 180270 loss: 0.0065 lr: 0.02\n","iteration: 180280 loss: 0.0119 lr: 0.02\n","iteration: 180290 loss: 0.0087 lr: 0.02\n","iteration: 180300 loss: 0.0081 lr: 0.02\n","iteration: 180310 loss: 0.0097 lr: 0.02\n","iteration: 180320 loss: 0.0096 lr: 0.02\n","iteration: 180330 loss: 0.0088 lr: 0.02\n","iteration: 180340 loss: 0.0099 lr: 0.02\n","iteration: 180350 loss: 0.0075 lr: 0.02\n","iteration: 180360 loss: 0.0060 lr: 0.02\n","iteration: 180370 loss: 0.0117 lr: 0.02\n","iteration: 180380 loss: 0.0102 lr: 0.02\n","iteration: 180390 loss: 0.0094 lr: 0.02\n","iteration: 180400 loss: 0.0113 lr: 0.02\n","iteration: 180410 loss: 0.0085 lr: 0.02\n","iteration: 180420 loss: 0.0090 lr: 0.02\n","iteration: 180430 loss: 0.0123 lr: 0.02\n","iteration: 180440 loss: 0.0079 lr: 0.02\n","iteration: 180450 loss: 0.0081 lr: 0.02\n","iteration: 180460 loss: 0.0115 lr: 0.02\n","iteration: 180470 loss: 0.0083 lr: 0.02\n","iteration: 180480 loss: 0.0100 lr: 0.02\n","iteration: 180490 loss: 0.0101 lr: 0.02\n","iteration: 180500 loss: 0.0105 lr: 0.02\n","iteration: 180510 loss: 0.0095 lr: 0.02\n","iteration: 180520 loss: 0.0089 lr: 0.02\n","iteration: 180530 loss: 0.0113 lr: 0.02\n","iteration: 180540 loss: 0.0115 lr: 0.02\n","iteration: 180550 loss: 0.0127 lr: 0.02\n","iteration: 180560 loss: 0.0115 lr: 0.02\n","iteration: 180570 loss: 0.0121 lr: 0.02\n","iteration: 180580 loss: 0.0097 lr: 0.02\n","iteration: 180590 loss: 0.0098 lr: 0.02\n","iteration: 180600 loss: 0.0097 lr: 0.02\n","iteration: 180610 loss: 0.0083 lr: 0.02\n","iteration: 180620 loss: 0.0085 lr: 0.02\n","iteration: 180630 loss: 0.0107 lr: 0.02\n","iteration: 180640 loss: 0.0096 lr: 0.02\n","iteration: 180650 loss: 0.0144 lr: 0.02\n","iteration: 180660 loss: 0.0096 lr: 0.02\n","iteration: 180670 loss: 0.0095 lr: 0.02\n","iteration: 180680 loss: 0.0098 lr: 0.02\n","iteration: 180690 loss: 0.0085 lr: 0.02\n","iteration: 180700 loss: 0.0089 lr: 0.02\n","iteration: 180710 loss: 0.0095 lr: 0.02\n","iteration: 180720 loss: 0.0085 lr: 0.02\n","iteration: 180730 loss: 0.0071 lr: 0.02\n","iteration: 180740 loss: 0.0109 lr: 0.02\n","iteration: 180750 loss: 0.0111 lr: 0.02\n","iteration: 180760 loss: 0.0091 lr: 0.02\n","iteration: 180770 loss: 0.0073 lr: 0.02\n","iteration: 180780 loss: 0.0114 lr: 0.02\n","iteration: 180790 loss: 0.0113 lr: 0.02\n","iteration: 180800 loss: 0.0106 lr: 0.02\n","iteration: 180810 loss: 0.0078 lr: 0.02\n","iteration: 180820 loss: 0.0145 lr: 0.02\n","iteration: 180830 loss: 0.0070 lr: 0.02\n","iteration: 180840 loss: 0.0099 lr: 0.02\n","iteration: 180850 loss: 0.0096 lr: 0.02\n","iteration: 180860 loss: 0.0073 lr: 0.02\n","iteration: 180870 loss: 0.0111 lr: 0.02\n","iteration: 180880 loss: 0.0093 lr: 0.02\n","iteration: 180890 loss: 0.0115 lr: 0.02\n","iteration: 180900 loss: 0.0087 lr: 0.02\n","iteration: 180910 loss: 0.0111 lr: 0.02\n","iteration: 180920 loss: 0.0106 lr: 0.02\n","iteration: 180930 loss: 0.0104 lr: 0.02\n","iteration: 180940 loss: 0.0075 lr: 0.02\n","iteration: 180950 loss: 0.0084 lr: 0.02\n","iteration: 180960 loss: 0.0099 lr: 0.02\n","iteration: 180970 loss: 0.0088 lr: 0.02\n","iteration: 180980 loss: 0.0090 lr: 0.02\n","iteration: 180990 loss: 0.0095 lr: 0.02\n","iteration: 181000 loss: 0.0098 lr: 0.02\n","iteration: 181010 loss: 0.0064 lr: 0.02\n","iteration: 181020 loss: 0.0062 lr: 0.02\n","iteration: 181030 loss: 0.0094 lr: 0.02\n","iteration: 181040 loss: 0.0097 lr: 0.02\n","iteration: 181050 loss: 0.0087 lr: 0.02\n","iteration: 181060 loss: 0.0082 lr: 0.02\n","iteration: 181070 loss: 0.0109 lr: 0.02\n","iteration: 181080 loss: 0.0083 lr: 0.02\n","iteration: 181090 loss: 0.0084 lr: 0.02\n","iteration: 181100 loss: 0.0080 lr: 0.02\n","iteration: 181110 loss: 0.0101 lr: 0.02\n","iteration: 181120 loss: 0.0095 lr: 0.02\n","iteration: 181130 loss: 0.0071 lr: 0.02\n","iteration: 181140 loss: 0.0102 lr: 0.02\n","iteration: 181150 loss: 0.0098 lr: 0.02\n","iteration: 181160 loss: 0.0094 lr: 0.02\n","iteration: 181170 loss: 0.0091 lr: 0.02\n","iteration: 181180 loss: 0.0064 lr: 0.02\n","iteration: 181190 loss: 0.0095 lr: 0.02\n","iteration: 181200 loss: 0.0089 lr: 0.02\n","iteration: 181210 loss: 0.0079 lr: 0.02\n","iteration: 181220 loss: 0.0094 lr: 0.02\n","iteration: 181230 loss: 0.0098 lr: 0.02\n","iteration: 181240 loss: 0.0110 lr: 0.02\n","iteration: 181250 loss: 0.0105 lr: 0.02\n","iteration: 181260 loss: 0.0100 lr: 0.02\n","iteration: 181270 loss: 0.0086 lr: 0.02\n","iteration: 181280 loss: 0.0081 lr: 0.02\n","iteration: 181290 loss: 0.0093 lr: 0.02\n","iteration: 181300 loss: 0.0097 lr: 0.02\n","iteration: 181310 loss: 0.0082 lr: 0.02\n","iteration: 181320 loss: 0.0094 lr: 0.02\n","iteration: 181330 loss: 0.0108 lr: 0.02\n","iteration: 181340 loss: 0.0091 lr: 0.02\n","iteration: 181350 loss: 0.0094 lr: 0.02\n","iteration: 181360 loss: 0.0118 lr: 0.02\n","iteration: 181370 loss: 0.0080 lr: 0.02\n","iteration: 181380 loss: 0.0126 lr: 0.02\n","iteration: 181390 loss: 0.0077 lr: 0.02\n","iteration: 181400 loss: 0.0085 lr: 0.02\n","iteration: 181410 loss: 0.0096 lr: 0.02\n","iteration: 181420 loss: 0.0084 lr: 0.02\n","iteration: 181430 loss: 0.0082 lr: 0.02\n","iteration: 181440 loss: 0.0091 lr: 0.02\n","iteration: 181450 loss: 0.0086 lr: 0.02\n","iteration: 181460 loss: 0.0083 lr: 0.02\n","iteration: 181470 loss: 0.0099 lr: 0.02\n","iteration: 181480 loss: 0.0070 lr: 0.02\n","iteration: 181490 loss: 0.0093 lr: 0.02\n","iteration: 181500 loss: 0.0081 lr: 0.02\n","iteration: 181510 loss: 0.0125 lr: 0.02\n","iteration: 181520 loss: 0.0087 lr: 0.02\n","iteration: 181530 loss: 0.0109 lr: 0.02\n","iteration: 181540 loss: 0.0090 lr: 0.02\n","iteration: 181550 loss: 0.0099 lr: 0.02\n","iteration: 181560 loss: 0.0088 lr: 0.02\n","iteration: 181570 loss: 0.0092 lr: 0.02\n","iteration: 181580 loss: 0.0060 lr: 0.02\n","iteration: 181590 loss: 0.0066 lr: 0.02\n","iteration: 181600 loss: 0.0119 lr: 0.02\n","iteration: 181610 loss: 0.0080 lr: 0.02\n","iteration: 181620 loss: 0.0147 lr: 0.02\n","iteration: 181630 loss: 0.0094 lr: 0.02\n","iteration: 181640 loss: 0.0161 lr: 0.02\n","iteration: 181650 loss: 0.0180 lr: 0.02\n","iteration: 181660 loss: 0.0151 lr: 0.02\n","iteration: 181670 loss: 0.0132 lr: 0.02\n","iteration: 181680 loss: 0.0182 lr: 0.02\n","iteration: 181690 loss: 0.0094 lr: 0.02\n","iteration: 181700 loss: 0.0100 lr: 0.02\n","iteration: 181710 loss: 0.0106 lr: 0.02\n","iteration: 181720 loss: 0.0095 lr: 0.02\n","iteration: 181730 loss: 0.0097 lr: 0.02\n","iteration: 181740 loss: 0.0119 lr: 0.02\n","iteration: 181750 loss: 0.0096 lr: 0.02\n","iteration: 181760 loss: 0.0111 lr: 0.02\n","iteration: 181770 loss: 0.0101 lr: 0.02\n","iteration: 181780 loss: 0.0107 lr: 0.02\n","iteration: 181790 loss: 0.0072 lr: 0.02\n","iteration: 181800 loss: 0.0108 lr: 0.02\n","iteration: 181810 loss: 0.0124 lr: 0.02\n","iteration: 181820 loss: 0.0106 lr: 0.02\n","iteration: 181830 loss: 0.0097 lr: 0.02\n","iteration: 181840 loss: 0.0092 lr: 0.02\n","iteration: 181850 loss: 0.0142 lr: 0.02\n","iteration: 181860 loss: 0.0167 lr: 0.02\n","iteration: 181870 loss: 0.0098 lr: 0.02\n","iteration: 181880 loss: 0.0075 lr: 0.02\n","iteration: 181890 loss: 0.0076 lr: 0.02\n","iteration: 181900 loss: 0.0108 lr: 0.02\n","iteration: 181910 loss: 0.0106 lr: 0.02\n","iteration: 181920 loss: 0.0094 lr: 0.02\n","iteration: 181930 loss: 0.0086 lr: 0.02\n","iteration: 181940 loss: 0.0113 lr: 0.02\n","iteration: 181950 loss: 0.0130 lr: 0.02\n","iteration: 181960 loss: 0.0113 lr: 0.02\n","iteration: 181970 loss: 0.0163 lr: 0.02\n","iteration: 181980 loss: 0.0086 lr: 0.02\n","iteration: 181990 loss: 0.0119 lr: 0.02\n","iteration: 182000 loss: 0.0095 lr: 0.02\n","iteration: 182010 loss: 0.0086 lr: 0.02\n","iteration: 182020 loss: 0.0094 lr: 0.02\n","iteration: 182030 loss: 0.0100 lr: 0.02\n","iteration: 182040 loss: 0.0110 lr: 0.02\n","iteration: 182050 loss: 0.0098 lr: 0.02\n","iteration: 182060 loss: 0.0113 lr: 0.02\n","iteration: 182070 loss: 0.0101 lr: 0.02\n","iteration: 182080 loss: 0.0082 lr: 0.02\n","iteration: 182090 loss: 0.0125 lr: 0.02\n","iteration: 182100 loss: 0.0074 lr: 0.02\n","iteration: 182110 loss: 0.0111 lr: 0.02\n","iteration: 182120 loss: 0.0110 lr: 0.02\n","iteration: 182130 loss: 0.0104 lr: 0.02\n","iteration: 182140 loss: 0.0111 lr: 0.02\n","iteration: 182150 loss: 0.0104 lr: 0.02\n","iteration: 182160 loss: 0.0085 lr: 0.02\n","iteration: 182170 loss: 0.0156 lr: 0.02\n","iteration: 182180 loss: 0.0062 lr: 0.02\n","iteration: 182190 loss: 0.0078 lr: 0.02\n","iteration: 182200 loss: 0.0104 lr: 0.02\n","iteration: 182210 loss: 0.0106 lr: 0.02\n","iteration: 182220 loss: 0.0075 lr: 0.02\n","iteration: 182230 loss: 0.0113 lr: 0.02\n","iteration: 182240 loss: 0.0105 lr: 0.02\n","iteration: 182250 loss: 0.0088 lr: 0.02\n","iteration: 182260 loss: 0.0078 lr: 0.02\n","iteration: 182270 loss: 0.0068 lr: 0.02\n","iteration: 182280 loss: 0.0121 lr: 0.02\n","iteration: 182290 loss: 0.0058 lr: 0.02\n","iteration: 182300 loss: 0.0100 lr: 0.02\n","iteration: 182310 loss: 0.0114 lr: 0.02\n","iteration: 182320 loss: 0.0101 lr: 0.02\n","iteration: 182330 loss: 0.0105 lr: 0.02\n","iteration: 182340 loss: 0.0095 lr: 0.02\n","iteration: 182350 loss: 0.0092 lr: 0.02\n","iteration: 182360 loss: 0.0126 lr: 0.02\n","iteration: 182370 loss: 0.0135 lr: 0.02\n","iteration: 182380 loss: 0.0114 lr: 0.02\n","iteration: 182390 loss: 0.0147 lr: 0.02\n","iteration: 182400 loss: 0.0160 lr: 0.02\n","iteration: 182410 loss: 0.0102 lr: 0.02\n","iteration: 182420 loss: 0.0146 lr: 0.02\n","iteration: 182430 loss: 0.0119 lr: 0.02\n","iteration: 182440 loss: 0.0111 lr: 0.02\n","iteration: 182450 loss: 0.0125 lr: 0.02\n","iteration: 182460 loss: 0.0122 lr: 0.02\n","iteration: 182470 loss: 0.0121 lr: 0.02\n","iteration: 182480 loss: 0.0103 lr: 0.02\n","iteration: 182490 loss: 0.0121 lr: 0.02\n","iteration: 182500 loss: 0.0107 lr: 0.02\n","iteration: 182510 loss: 0.0072 lr: 0.02\n","iteration: 182520 loss: 0.0087 lr: 0.02\n","iteration: 182530 loss: 0.0074 lr: 0.02\n","iteration: 182540 loss: 0.0111 lr: 0.02\n","iteration: 182550 loss: 0.0086 lr: 0.02\n","iteration: 182560 loss: 0.0128 lr: 0.02\n","iteration: 182570 loss: 0.0078 lr: 0.02\n","iteration: 182580 loss: 0.0101 lr: 0.02\n","iteration: 182590 loss: 0.0094 lr: 0.02\n","iteration: 182600 loss: 0.0113 lr: 0.02\n","iteration: 182610 loss: 0.0147 lr: 0.02\n","iteration: 182620 loss: 0.0092 lr: 0.02\n","iteration: 182630 loss: 0.0077 lr: 0.02\n","iteration: 182640 loss: 0.0102 lr: 0.02\n","iteration: 182650 loss: 0.0100 lr: 0.02\n","iteration: 182660 loss: 0.0083 lr: 0.02\n","iteration: 182670 loss: 0.0108 lr: 0.02\n","iteration: 182680 loss: 0.0088 lr: 0.02\n","iteration: 182690 loss: 0.0087 lr: 0.02\n","iteration: 182700 loss: 0.0089 lr: 0.02\n","iteration: 182710 loss: 0.0092 lr: 0.02\n","iteration: 182720 loss: 0.0089 lr: 0.02\n","iteration: 182730 loss: 0.0094 lr: 0.02\n","iteration: 182740 loss: 0.0102 lr: 0.02\n","iteration: 182750 loss: 0.0089 lr: 0.02\n","iteration: 182760 loss: 0.0076 lr: 0.02\n","iteration: 182770 loss: 0.0087 lr: 0.02\n","iteration: 182780 loss: 0.0064 lr: 0.02\n","iteration: 182790 loss: 0.0089 lr: 0.02\n","iteration: 182800 loss: 0.0090 lr: 0.02\n","iteration: 182810 loss: 0.0091 lr: 0.02\n","iteration: 182820 loss: 0.0119 lr: 0.02\n","iteration: 182830 loss: 0.0100 lr: 0.02\n","iteration: 182840 loss: 0.0073 lr: 0.02\n","iteration: 182850 loss: 0.0091 lr: 0.02\n","iteration: 182860 loss: 0.0104 lr: 0.02\n","iteration: 182870 loss: 0.0124 lr: 0.02\n","iteration: 182880 loss: 0.0137 lr: 0.02\n","iteration: 182890 loss: 0.0098 lr: 0.02\n","iteration: 182900 loss: 0.0111 lr: 0.02\n","iteration: 182910 loss: 0.0112 lr: 0.02\n","iteration: 182920 loss: 0.0125 lr: 0.02\n","iteration: 182930 loss: 0.0077 lr: 0.02\n","iteration: 182940 loss: 0.0101 lr: 0.02\n","iteration: 182950 loss: 0.0095 lr: 0.02\n","iteration: 182960 loss: 0.0103 lr: 0.02\n","iteration: 182970 loss: 0.0086 lr: 0.02\n","iteration: 182980 loss: 0.0076 lr: 0.02\n","iteration: 182990 loss: 0.0086 lr: 0.02\n","iteration: 183000 loss: 0.0108 lr: 0.02\n","iteration: 183010 loss: 0.0074 lr: 0.02\n","iteration: 183020 loss: 0.0101 lr: 0.02\n","iteration: 183030 loss: 0.0064 lr: 0.02\n","iteration: 183040 loss: 0.0115 lr: 0.02\n","iteration: 183050 loss: 0.0095 lr: 0.02\n","iteration: 183060 loss: 0.0095 lr: 0.02\n","iteration: 183070 loss: 0.0110 lr: 0.02\n","iteration: 183080 loss: 0.0101 lr: 0.02\n","iteration: 183090 loss: 0.0101 lr: 0.02\n","iteration: 183100 loss: 0.0081 lr: 0.02\n","iteration: 183110 loss: 0.0103 lr: 0.02\n","iteration: 183120 loss: 0.0096 lr: 0.02\n","iteration: 183130 loss: 0.0090 lr: 0.02\n","iteration: 183140 loss: 0.0106 lr: 0.02\n","iteration: 183150 loss: 0.0097 lr: 0.02\n","iteration: 183160 loss: 0.0090 lr: 0.02\n","iteration: 183170 loss: 0.0073 lr: 0.02\n","iteration: 183180 loss: 0.0093 lr: 0.02\n","iteration: 183190 loss: 0.0095 lr: 0.02\n","iteration: 183200 loss: 0.0121 lr: 0.02\n","iteration: 183210 loss: 0.0092 lr: 0.02\n","iteration: 183220 loss: 0.0122 lr: 0.02\n","iteration: 183230 loss: 0.0095 lr: 0.02\n","iteration: 183240 loss: 0.0081 lr: 0.02\n","iteration: 183250 loss: 0.0099 lr: 0.02\n","iteration: 183260 loss: 0.0090 lr: 0.02\n","iteration: 183270 loss: 0.0095 lr: 0.02\n","iteration: 183280 loss: 0.0081 lr: 0.02\n","iteration: 183290 loss: 0.0076 lr: 0.02\n","iteration: 183300 loss: 0.0113 lr: 0.02\n","iteration: 183310 loss: 0.0096 lr: 0.02\n","iteration: 183320 loss: 0.0097 lr: 0.02\n","iteration: 183330 loss: 0.0104 lr: 0.02\n","iteration: 183340 loss: 0.0103 lr: 0.02\n","iteration: 183350 loss: 0.0087 lr: 0.02\n","iteration: 183360 loss: 0.0091 lr: 0.02\n","iteration: 183370 loss: 0.0082 lr: 0.02\n","iteration: 183380 loss: 0.0097 lr: 0.02\n","iteration: 183390 loss: 0.0073 lr: 0.02\n","iteration: 183400 loss: 0.0099 lr: 0.02\n","iteration: 183410 loss: 0.0074 lr: 0.02\n","iteration: 183420 loss: 0.0098 lr: 0.02\n","iteration: 183430 loss: 0.0095 lr: 0.02\n","iteration: 183440 loss: 0.0096 lr: 0.02\n","iteration: 183450 loss: 0.0118 lr: 0.02\n","iteration: 183460 loss: 0.0079 lr: 0.02\n","iteration: 183470 loss: 0.0076 lr: 0.02\n","iteration: 183480 loss: 0.0096 lr: 0.02\n","iteration: 183490 loss: 0.0082 lr: 0.02\n","iteration: 183500 loss: 0.0077 lr: 0.02\n","iteration: 183510 loss: 0.0061 lr: 0.02\n","iteration: 183520 loss: 0.0107 lr: 0.02\n","iteration: 183530 loss: 0.0085 lr: 0.02\n","iteration: 183540 loss: 0.0092 lr: 0.02\n","iteration: 183550 loss: 0.0087 lr: 0.02\n","iteration: 183560 loss: 0.0095 lr: 0.02\n","iteration: 183570 loss: 0.0093 lr: 0.02\n","iteration: 183580 loss: 0.0100 lr: 0.02\n","iteration: 183590 loss: 0.0096 lr: 0.02\n","iteration: 183600 loss: 0.0092 lr: 0.02\n","iteration: 183610 loss: 0.0088 lr: 0.02\n","iteration: 183620 loss: 0.0089 lr: 0.02\n","iteration: 183630 loss: 0.0096 lr: 0.02\n","iteration: 183640 loss: 0.0112 lr: 0.02\n","iteration: 183650 loss: 0.0105 lr: 0.02\n","iteration: 183660 loss: 0.0122 lr: 0.02\n","iteration: 183670 loss: 0.0107 lr: 0.02\n","iteration: 183680 loss: 0.0090 lr: 0.02\n","iteration: 183690 loss: 0.0101 lr: 0.02\n","iteration: 183700 loss: 0.0111 lr: 0.02\n","iteration: 183710 loss: 0.0067 lr: 0.02\n","iteration: 183720 loss: 0.0102 lr: 0.02\n","iteration: 183730 loss: 0.0099 lr: 0.02\n","iteration: 183740 loss: 0.0089 lr: 0.02\n","iteration: 183750 loss: 0.0087 lr: 0.02\n","iteration: 183760 loss: 0.0091 lr: 0.02\n","iteration: 183770 loss: 0.0111 lr: 0.02\n","iteration: 183780 loss: 0.0089 lr: 0.02\n","iteration: 183790 loss: 0.0084 lr: 0.02\n","iteration: 183800 loss: 0.0073 lr: 0.02\n","iteration: 183810 loss: 0.0079 lr: 0.02\n","iteration: 183820 loss: 0.0075 lr: 0.02\n","iteration: 183830 loss: 0.0061 lr: 0.02\n","iteration: 183840 loss: 0.0082 lr: 0.02\n","iteration: 183850 loss: 0.0086 lr: 0.02\n","iteration: 183860 loss: 0.0081 lr: 0.02\n","iteration: 183870 loss: 0.0126 lr: 0.02\n","iteration: 183880 loss: 0.0089 lr: 0.02\n","iteration: 183890 loss: 0.0092 lr: 0.02\n","iteration: 183900 loss: 0.0110 lr: 0.02\n","iteration: 183910 loss: 0.0133 lr: 0.02\n","iteration: 183920 loss: 0.0080 lr: 0.02\n","iteration: 183930 loss: 0.0100 lr: 0.02\n","iteration: 183940 loss: 0.0074 lr: 0.02\n","iteration: 183950 loss: 0.0097 lr: 0.02\n","iteration: 183960 loss: 0.0117 lr: 0.02\n","iteration: 183970 loss: 0.0091 lr: 0.02\n","iteration: 183980 loss: 0.0074 lr: 0.02\n","iteration: 183990 loss: 0.0106 lr: 0.02\n","iteration: 184000 loss: 0.0064 lr: 0.02\n","iteration: 184010 loss: 0.0111 lr: 0.02\n","iteration: 184020 loss: 0.0110 lr: 0.02\n","iteration: 184030 loss: 0.0096 lr: 0.02\n","iteration: 184040 loss: 0.0108 lr: 0.02\n","iteration: 184050 loss: 0.0099 lr: 0.02\n","iteration: 184060 loss: 0.0093 lr: 0.02\n","iteration: 184070 loss: 0.0097 lr: 0.02\n","iteration: 184080 loss: 0.0080 lr: 0.02\n","iteration: 184090 loss: 0.0090 lr: 0.02\n","iteration: 184100 loss: 0.0097 lr: 0.02\n","iteration: 184110 loss: 0.0093 lr: 0.02\n","iteration: 184120 loss: 0.0101 lr: 0.02\n","iteration: 184130 loss: 0.0070 lr: 0.02\n","iteration: 184140 loss: 0.0083 lr: 0.02\n","iteration: 184150 loss: 0.0083 lr: 0.02\n","iteration: 184160 loss: 0.0124 lr: 0.02\n","iteration: 184170 loss: 0.0085 lr: 0.02\n","iteration: 184180 loss: 0.0085 lr: 0.02\n","iteration: 184190 loss: 0.0096 lr: 0.02\n","iteration: 184200 loss: 0.0115 lr: 0.02\n","iteration: 184210 loss: 0.0095 lr: 0.02\n","iteration: 184220 loss: 0.0080 lr: 0.02\n","iteration: 184230 loss: 0.0083 lr: 0.02\n","iteration: 184240 loss: 0.0089 lr: 0.02\n","iteration: 184250 loss: 0.0080 lr: 0.02\n","iteration: 184260 loss: 0.0082 lr: 0.02\n","iteration: 184270 loss: 0.0111 lr: 0.02\n","iteration: 184280 loss: 0.0095 lr: 0.02\n","iteration: 184290 loss: 0.0105 lr: 0.02\n","iteration: 184300 loss: 0.0126 lr: 0.02\n","iteration: 184310 loss: 0.0076 lr: 0.02\n","iteration: 184320 loss: 0.0072 lr: 0.02\n","iteration: 184330 loss: 0.0104 lr: 0.02\n","iteration: 184340 loss: 0.0077 lr: 0.02\n","iteration: 184350 loss: 0.0092 lr: 0.02\n","iteration: 184360 loss: 0.0088 lr: 0.02\n","iteration: 184370 loss: 0.0074 lr: 0.02\n","iteration: 184380 loss: 0.0095 lr: 0.02\n","iteration: 184390 loss: 0.0108 lr: 0.02\n","iteration: 184400 loss: 0.0105 lr: 0.02\n","iteration: 184410 loss: 0.0093 lr: 0.02\n","iteration: 184420 loss: 0.0086 lr: 0.02\n","iteration: 184430 loss: 0.0098 lr: 0.02\n","iteration: 184440 loss: 0.0085 lr: 0.02\n","iteration: 184450 loss: 0.0079 lr: 0.02\n","iteration: 184460 loss: 0.0097 lr: 0.02\n","iteration: 184470 loss: 0.0085 lr: 0.02\n","iteration: 184480 loss: 0.0065 lr: 0.02\n","iteration: 184490 loss: 0.0085 lr: 0.02\n","iteration: 184500 loss: 0.0096 lr: 0.02\n","iteration: 184510 loss: 0.0077 lr: 0.02\n","iteration: 184520 loss: 0.0093 lr: 0.02\n","iteration: 184530 loss: 0.0094 lr: 0.02\n","iteration: 184540 loss: 0.0113 lr: 0.02\n","iteration: 184550 loss: 0.0101 lr: 0.02\n","iteration: 184560 loss: 0.0087 lr: 0.02\n","iteration: 184570 loss: 0.0056 lr: 0.02\n","iteration: 184580 loss: 0.0105 lr: 0.02\n","iteration: 184590 loss: 0.0109 lr: 0.02\n","iteration: 184600 loss: 0.0095 lr: 0.02\n","iteration: 184610 loss: 0.0076 lr: 0.02\n","iteration: 184620 loss: 0.0079 lr: 0.02\n","iteration: 184630 loss: 0.0108 lr: 0.02\n","iteration: 184640 loss: 0.0076 lr: 0.02\n","iteration: 184650 loss: 0.0092 lr: 0.02\n","iteration: 184660 loss: 0.0082 lr: 0.02\n","iteration: 184670 loss: 0.0085 lr: 0.02\n","iteration: 184680 loss: 0.0079 lr: 0.02\n","iteration: 184690 loss: 0.0076 lr: 0.02\n","iteration: 184700 loss: 0.0124 lr: 0.02\n","iteration: 184710 loss: 0.0060 lr: 0.02\n","iteration: 184720 loss: 0.0088 lr: 0.02\n","iteration: 184730 loss: 0.0113 lr: 0.02\n","iteration: 184740 loss: 0.0087 lr: 0.02\n","iteration: 184750 loss: 0.0079 lr: 0.02\n","iteration: 184760 loss: 0.0118 lr: 0.02\n","iteration: 184770 loss: 0.0098 lr: 0.02\n","iteration: 184780 loss: 0.0091 lr: 0.02\n","iteration: 184790 loss: 0.0089 lr: 0.02\n","iteration: 184800 loss: 0.0085 lr: 0.02\n","iteration: 184810 loss: 0.0096 lr: 0.02\n","iteration: 184820 loss: 0.0103 lr: 0.02\n","iteration: 184830 loss: 0.0087 lr: 0.02\n","iteration: 184840 loss: 0.0114 lr: 0.02\n","iteration: 184850 loss: 0.0117 lr: 0.02\n","iteration: 184860 loss: 0.0101 lr: 0.02\n","iteration: 184870 loss: 0.0109 lr: 0.02\n","iteration: 184880 loss: 0.0099 lr: 0.02\n","iteration: 184890 loss: 0.0081 lr: 0.02\n","iteration: 184900 loss: 0.0080 lr: 0.02\n","iteration: 184910 loss: 0.0092 lr: 0.02\n","iteration: 184920 loss: 0.0127 lr: 0.02\n","iteration: 184930 loss: 0.0113 lr: 0.02\n","iteration: 184940 loss: 0.0077 lr: 0.02\n","iteration: 184950 loss: 0.0105 lr: 0.02\n","iteration: 184960 loss: 0.0080 lr: 0.02\n","iteration: 184970 loss: 0.0091 lr: 0.02\n","iteration: 184980 loss: 0.0090 lr: 0.02\n","iteration: 184990 loss: 0.0085 lr: 0.02\n","iteration: 185000 loss: 0.0083 lr: 0.02\n","iteration: 185010 loss: 0.0122 lr: 0.02\n","iteration: 185020 loss: 0.0084 lr: 0.02\n","iteration: 185030 loss: 0.0073 lr: 0.02\n","iteration: 185040 loss: 0.0066 lr: 0.02\n","iteration: 185050 loss: 0.0078 lr: 0.02\n","iteration: 185060 loss: 0.0081 lr: 0.02\n","iteration: 185070 loss: 0.0072 lr: 0.02\n","iteration: 185080 loss: 0.0092 lr: 0.02\n","iteration: 185090 loss: 0.0085 lr: 0.02\n","iteration: 185100 loss: 0.0084 lr: 0.02\n","iteration: 185110 loss: 0.0097 lr: 0.02\n","iteration: 185120 loss: 0.0080 lr: 0.02\n","iteration: 185130 loss: 0.0174 lr: 0.02\n","iteration: 185140 loss: 0.0097 lr: 0.02\n","iteration: 185150 loss: 0.0108 lr: 0.02\n","iteration: 185160 loss: 0.0076 lr: 0.02\n","iteration: 185170 loss: 0.0129 lr: 0.02\n","iteration: 185180 loss: 0.0108 lr: 0.02\n","iteration: 185190 loss: 0.0083 lr: 0.02\n","iteration: 185200 loss: 0.0104 lr: 0.02\n","iteration: 185210 loss: 0.0115 lr: 0.02\n","iteration: 185220 loss: 0.0090 lr: 0.02\n","iteration: 185230 loss: 0.0099 lr: 0.02\n","iteration: 185240 loss: 0.0083 lr: 0.02\n","iteration: 185250 loss: 0.0079 lr: 0.02\n","iteration: 185260 loss: 0.0066 lr: 0.02\n","iteration: 185270 loss: 0.0088 lr: 0.02\n","iteration: 185280 loss: 0.0119 lr: 0.02\n","iteration: 185290 loss: 0.0114 lr: 0.02\n","iteration: 185300 loss: 0.0109 lr: 0.02\n","iteration: 185310 loss: 0.0092 lr: 0.02\n","iteration: 185320 loss: 0.0093 lr: 0.02\n","iteration: 185330 loss: 0.0105 lr: 0.02\n","iteration: 185340 loss: 0.0109 lr: 0.02\n","iteration: 185350 loss: 0.0080 lr: 0.02\n","iteration: 185360 loss: 0.0107 lr: 0.02\n","iteration: 185370 loss: 0.0091 lr: 0.02\n","iteration: 185380 loss: 0.0108 lr: 0.02\n","iteration: 185390 loss: 0.0101 lr: 0.02\n","iteration: 185400 loss: 0.0082 lr: 0.02\n","iteration: 185410 loss: 0.0087 lr: 0.02\n","iteration: 185420 loss: 0.0118 lr: 0.02\n","iteration: 185430 loss: 0.0080 lr: 0.02\n","iteration: 185440 loss: 0.0109 lr: 0.02\n","iteration: 185450 loss: 0.0136 lr: 0.02\n","iteration: 185460 loss: 0.0129 lr: 0.02\n","iteration: 185470 loss: 0.0103 lr: 0.02\n","iteration: 185480 loss: 0.0076 lr: 0.02\n","iteration: 185490 loss: 0.0102 lr: 0.02\n","iteration: 185500 loss: 0.0098 lr: 0.02\n","iteration: 185510 loss: 0.0099 lr: 0.02\n","iteration: 185520 loss: 0.0118 lr: 0.02\n","iteration: 185530 loss: 0.0099 lr: 0.02\n","iteration: 185540 loss: 0.0088 lr: 0.02\n","iteration: 185550 loss: 0.0104 lr: 0.02\n","iteration: 185560 loss: 0.0092 lr: 0.02\n","iteration: 185570 loss: 0.0108 lr: 0.02\n","iteration: 185580 loss: 0.0099 lr: 0.02\n","iteration: 185590 loss: 0.0097 lr: 0.02\n","iteration: 185600 loss: 0.0110 lr: 0.02\n","iteration: 185610 loss: 0.0073 lr: 0.02\n","iteration: 185620 loss: 0.0067 lr: 0.02\n","iteration: 185630 loss: 0.0096 lr: 0.02\n","iteration: 185640 loss: 0.0085 lr: 0.02\n","iteration: 185650 loss: 0.0108 lr: 0.02\n","iteration: 185660 loss: 0.0078 lr: 0.02\n","iteration: 185670 loss: 0.0089 lr: 0.02\n","iteration: 185680 loss: 0.0098 lr: 0.02\n","iteration: 185690 loss: 0.0086 lr: 0.02\n","iteration: 185700 loss: 0.0123 lr: 0.02\n","iteration: 185710 loss: 0.0151 lr: 0.02\n","iteration: 185720 loss: 0.0087 lr: 0.02\n","iteration: 185730 loss: 0.0093 lr: 0.02\n","iteration: 185740 loss: 0.0098 lr: 0.02\n","iteration: 185750 loss: 0.0094 lr: 0.02\n","iteration: 185760 loss: 0.0077 lr: 0.02\n","iteration: 185770 loss: 0.0094 lr: 0.02\n","iteration: 185780 loss: 0.0088 lr: 0.02\n","iteration: 185790 loss: 0.0097 lr: 0.02\n","iteration: 185800 loss: 0.0095 lr: 0.02\n","iteration: 185810 loss: 0.0099 lr: 0.02\n","iteration: 185820 loss: 0.0097 lr: 0.02\n","iteration: 185830 loss: 0.0089 lr: 0.02\n","iteration: 185840 loss: 0.0084 lr: 0.02\n","iteration: 185850 loss: 0.0115 lr: 0.02\n","iteration: 185860 loss: 0.0128 lr: 0.02\n","iteration: 185870 loss: 0.0123 lr: 0.02\n","iteration: 185880 loss: 0.0064 lr: 0.02\n","iteration: 185890 loss: 0.0087 lr: 0.02\n","iteration: 185900 loss: 0.0081 lr: 0.02\n","iteration: 185910 loss: 0.0088 lr: 0.02\n","iteration: 185920 loss: 0.0108 lr: 0.02\n","iteration: 185930 loss: 0.0081 lr: 0.02\n","iteration: 185940 loss: 0.0083 lr: 0.02\n","iteration: 185950 loss: 0.0073 lr: 0.02\n","iteration: 185960 loss: 0.0102 lr: 0.02\n","iteration: 185970 loss: 0.0099 lr: 0.02\n","iteration: 185980 loss: 0.0133 lr: 0.02\n","iteration: 185990 loss: 0.0110 lr: 0.02\n","iteration: 186000 loss: 0.0119 lr: 0.02\n","iteration: 186010 loss: 0.0086 lr: 0.02\n","iteration: 186020 loss: 0.0089 lr: 0.02\n","iteration: 186030 loss: 0.0083 lr: 0.02\n","iteration: 186040 loss: 0.0100 lr: 0.02\n","iteration: 186050 loss: 0.0073 lr: 0.02\n","iteration: 186060 loss: 0.0085 lr: 0.02\n","iteration: 186070 loss: 0.0075 lr: 0.02\n","iteration: 186080 loss: 0.0084 lr: 0.02\n","iteration: 186090 loss: 0.0080 lr: 0.02\n","iteration: 186100 loss: 0.0070 lr: 0.02\n","iteration: 186110 loss: 0.0067 lr: 0.02\n","iteration: 186120 loss: 0.0067 lr: 0.02\n","iteration: 186130 loss: 0.0061 lr: 0.02\n","iteration: 186140 loss: 0.0073 lr: 0.02\n","iteration: 186150 loss: 0.0109 lr: 0.02\n","iteration: 186160 loss: 0.0086 lr: 0.02\n","iteration: 186170 loss: 0.0063 lr: 0.02\n","iteration: 186180 loss: 0.0085 lr: 0.02\n","iteration: 186190 loss: 0.0064 lr: 0.02\n","iteration: 186200 loss: 0.0088 lr: 0.02\n","iteration: 186210 loss: 0.0091 lr: 0.02\n","iteration: 186220 loss: 0.0133 lr: 0.02\n","iteration: 186230 loss: 0.0091 lr: 0.02\n","iteration: 186240 loss: 0.0082 lr: 0.02\n","iteration: 186250 loss: 0.0093 lr: 0.02\n","iteration: 186260 loss: 0.0105 lr: 0.02\n","iteration: 186270 loss: 0.0130 lr: 0.02\n","iteration: 186280 loss: 0.0109 lr: 0.02\n","iteration: 186290 loss: 0.0082 lr: 0.02\n","iteration: 186300 loss: 0.0067 lr: 0.02\n","iteration: 186310 loss: 0.0078 lr: 0.02\n","iteration: 186320 loss: 0.0090 lr: 0.02\n","iteration: 186330 loss: 0.0093 lr: 0.02\n","iteration: 186340 loss: 0.0093 lr: 0.02\n","iteration: 186350 loss: 0.0132 lr: 0.02\n","iteration: 186360 loss: 0.0122 lr: 0.02\n","iteration: 186370 loss: 0.0068 lr: 0.02\n","iteration: 186380 loss: 0.0092 lr: 0.02\n","iteration: 186390 loss: 0.0097 lr: 0.02\n","iteration: 186400 loss: 0.0103 lr: 0.02\n","iteration: 186410 loss: 0.0129 lr: 0.02\n","iteration: 186420 loss: 0.0060 lr: 0.02\n","iteration: 186430 loss: 0.0099 lr: 0.02\n","iteration: 186440 loss: 0.0114 lr: 0.02\n","iteration: 186450 loss: 0.0090 lr: 0.02\n","iteration: 186460 loss: 0.0074 lr: 0.02\n","iteration: 186470 loss: 0.0095 lr: 0.02\n","iteration: 186480 loss: 0.0107 lr: 0.02\n","iteration: 186490 loss: 0.0092 lr: 0.02\n","iteration: 186500 loss: 0.0082 lr: 0.02\n","iteration: 186510 loss: 0.0111 lr: 0.02\n","iteration: 186520 loss: 0.0103 lr: 0.02\n","iteration: 186530 loss: 0.0088 lr: 0.02\n","iteration: 186540 loss: 0.0120 lr: 0.02\n","iteration: 186550 loss: 0.0127 lr: 0.02\n","iteration: 186560 loss: 0.0133 lr: 0.02\n","iteration: 186570 loss: 0.0158 lr: 0.02\n","iteration: 186580 loss: 0.0103 lr: 0.02\n","iteration: 186590 loss: 0.0180 lr: 0.02\n","iteration: 186600 loss: 0.0069 lr: 0.02\n","iteration: 186610 loss: 0.0106 lr: 0.02\n","iteration: 186620 loss: 0.0108 lr: 0.02\n","iteration: 186630 loss: 0.0081 lr: 0.02\n","iteration: 186640 loss: 0.0109 lr: 0.02\n","iteration: 186650 loss: 0.0121 lr: 0.02\n","iteration: 186660 loss: 0.0115 lr: 0.02\n","iteration: 186670 loss: 0.0096 lr: 0.02\n","iteration: 186680 loss: 0.0119 lr: 0.02\n","iteration: 186690 loss: 0.0075 lr: 0.02\n","iteration: 186700 loss: 0.0074 lr: 0.02\n","iteration: 186710 loss: 0.0104 lr: 0.02\n","iteration: 186720 loss: 0.0089 lr: 0.02\n","iteration: 186730 loss: 0.0078 lr: 0.02\n","iteration: 186740 loss: 0.0098 lr: 0.02\n","iteration: 186750 loss: 0.0094 lr: 0.02\n","iteration: 186760 loss: 0.0112 lr: 0.02\n","iteration: 186770 loss: 0.0076 lr: 0.02\n","iteration: 186780 loss: 0.0090 lr: 0.02\n","iteration: 186790 loss: 0.0094 lr: 0.02\n","iteration: 186800 loss: 0.0093 lr: 0.02\n","iteration: 186810 loss: 0.0099 lr: 0.02\n","iteration: 186820 loss: 0.0092 lr: 0.02\n","iteration: 186830 loss: 0.0076 lr: 0.02\n","iteration: 186840 loss: 0.0092 lr: 0.02\n","iteration: 186850 loss: 0.0063 lr: 0.02\n","iteration: 186860 loss: 0.0106 lr: 0.02\n","iteration: 186870 loss: 0.0081 lr: 0.02\n","iteration: 186880 loss: 0.0125 lr: 0.02\n","iteration: 186890 loss: 0.0117 lr: 0.02\n","iteration: 186900 loss: 0.0110 lr: 0.02\n","iteration: 186910 loss: 0.0105 lr: 0.02\n","iteration: 186920 loss: 0.0105 lr: 0.02\n","iteration: 186930 loss: 0.0068 lr: 0.02\n","iteration: 186940 loss: 0.0100 lr: 0.02\n","iteration: 186950 loss: 0.0090 lr: 0.02\n","iteration: 186960 loss: 0.0096 lr: 0.02\n","iteration: 186970 loss: 0.0106 lr: 0.02\n","iteration: 186980 loss: 0.0098 lr: 0.02\n","iteration: 186990 loss: 0.0123 lr: 0.02\n","iteration: 187000 loss: 0.0091 lr: 0.02\n","iteration: 187010 loss: 0.0084 lr: 0.02\n","iteration: 187020 loss: 0.0100 lr: 0.02\n","iteration: 187030 loss: 0.0081 lr: 0.02\n","iteration: 187040 loss: 0.0093 lr: 0.02\n","iteration: 187050 loss: 0.0116 lr: 0.02\n","iteration: 187060 loss: 0.0082 lr: 0.02\n","iteration: 187070 loss: 0.0124 lr: 0.02\n","iteration: 187080 loss: 0.0097 lr: 0.02\n","iteration: 187090 loss: 0.0107 lr: 0.02\n","iteration: 187100 loss: 0.0089 lr: 0.02\n","iteration: 187110 loss: 0.0084 lr: 0.02\n","iteration: 187120 loss: 0.0085 lr: 0.02\n","iteration: 187130 loss: 0.0076 lr: 0.02\n","iteration: 187140 loss: 0.0102 lr: 0.02\n","iteration: 187150 loss: 0.0111 lr: 0.02\n","iteration: 187160 loss: 0.0092 lr: 0.02\n","iteration: 187170 loss: 0.0126 lr: 0.02\n","iteration: 187180 loss: 0.0108 lr: 0.02\n","iteration: 187190 loss: 0.0086 lr: 0.02\n","iteration: 187200 loss: 0.0069 lr: 0.02\n","iteration: 187210 loss: 0.0090 lr: 0.02\n","iteration: 187220 loss: 0.0063 lr: 0.02\n","iteration: 187230 loss: 0.0072 lr: 0.02\n","iteration: 187240 loss: 0.0063 lr: 0.02\n","iteration: 187250 loss: 0.0099 lr: 0.02\n","iteration: 187260 loss: 0.0073 lr: 0.02\n","iteration: 187270 loss: 0.0080 lr: 0.02\n","iteration: 187280 loss: 0.0083 lr: 0.02\n","iteration: 187290 loss: 0.0139 lr: 0.02\n","iteration: 187300 loss: 0.0150 lr: 0.02\n","iteration: 187310 loss: 0.0094 lr: 0.02\n","iteration: 187320 loss: 0.0096 lr: 0.02\n","iteration: 187330 loss: 0.0093 lr: 0.02\n","iteration: 187340 loss: 0.0090 lr: 0.02\n","iteration: 187350 loss: 0.0096 lr: 0.02\n","iteration: 187360 loss: 0.0077 lr: 0.02\n","iteration: 187370 loss: 0.0082 lr: 0.02\n","iteration: 187380 loss: 0.0116 lr: 0.02\n","iteration: 187390 loss: 0.0083 lr: 0.02\n","iteration: 187400 loss: 0.0082 lr: 0.02\n","iteration: 187410 loss: 0.0086 lr: 0.02\n","iteration: 187420 loss: 0.0093 lr: 0.02\n","iteration: 187430 loss: 0.0088 lr: 0.02\n","iteration: 187440 loss: 0.0101 lr: 0.02\n","iteration: 187450 loss: 0.0116 lr: 0.02\n","iteration: 187460 loss: 0.0079 lr: 0.02\n","iteration: 187470 loss: 0.0080 lr: 0.02\n","iteration: 187480 loss: 0.0075 lr: 0.02\n","iteration: 187490 loss: 0.0127 lr: 0.02\n","iteration: 187500 loss: 0.0068 lr: 0.02\n","iteration: 187510 loss: 0.0124 lr: 0.02\n","iteration: 187520 loss: 0.0123 lr: 0.02\n","iteration: 187530 loss: 0.0089 lr: 0.02\n","iteration: 187540 loss: 0.0118 lr: 0.02\n","iteration: 187550 loss: 0.0091 lr: 0.02\n","iteration: 187560 loss: 0.0097 lr: 0.02\n","iteration: 187570 loss: 0.0087 lr: 0.02\n","iteration: 187580 loss: 0.0092 lr: 0.02\n","iteration: 187590 loss: 0.0089 lr: 0.02\n","iteration: 187600 loss: 0.0125 lr: 0.02\n","iteration: 187610 loss: 0.0105 lr: 0.02\n","iteration: 187620 loss: 0.0083 lr: 0.02\n","iteration: 187630 loss: 0.0101 lr: 0.02\n","iteration: 187640 loss: 0.0089 lr: 0.02\n","iteration: 187650 loss: 0.0098 lr: 0.02\n","iteration: 187660 loss: 0.0111 lr: 0.02\n","iteration: 187670 loss: 0.0127 lr: 0.02\n","iteration: 187680 loss: 0.0107 lr: 0.02\n","iteration: 187690 loss: 0.0094 lr: 0.02\n","iteration: 187700 loss: 0.0083 lr: 0.02\n","iteration: 187710 loss: 0.0105 lr: 0.02\n","iteration: 187720 loss: 0.0075 lr: 0.02\n","iteration: 187730 loss: 0.0131 lr: 0.02\n","iteration: 187740 loss: 0.0132 lr: 0.02\n","iteration: 187750 loss: 0.0110 lr: 0.02\n","iteration: 187760 loss: 0.0106 lr: 0.02\n","iteration: 187770 loss: 0.0117 lr: 0.02\n","iteration: 187780 loss: 0.0079 lr: 0.02\n","iteration: 187790 loss: 0.0085 lr: 0.02\n","iteration: 187800 loss: 0.0098 lr: 0.02\n","iteration: 187810 loss: 0.0082 lr: 0.02\n","iteration: 187820 loss: 0.0110 lr: 0.02\n","iteration: 187830 loss: 0.0095 lr: 0.02\n","iteration: 187840 loss: 0.0099 lr: 0.02\n","iteration: 187850 loss: 0.0097 lr: 0.02\n","iteration: 187860 loss: 0.0079 lr: 0.02\n","iteration: 187870 loss: 0.0092 lr: 0.02\n","iteration: 187880 loss: 0.0078 lr: 0.02\n","iteration: 187890 loss: 0.0073 lr: 0.02\n","iteration: 187900 loss: 0.0073 lr: 0.02\n","iteration: 187910 loss: 0.0095 lr: 0.02\n","iteration: 187920 loss: 0.0082 lr: 0.02\n","iteration: 187930 loss: 0.0090 lr: 0.02\n","iteration: 187940 loss: 0.0090 lr: 0.02\n","iteration: 187950 loss: 0.0086 lr: 0.02\n","iteration: 187960 loss: 0.0081 lr: 0.02\n","iteration: 187970 loss: 0.0111 lr: 0.02\n","iteration: 187980 loss: 0.0118 lr: 0.02\n","iteration: 187990 loss: 0.0101 lr: 0.02\n","iteration: 188000 loss: 0.0157 lr: 0.02\n","iteration: 188010 loss: 0.0089 lr: 0.02\n","iteration: 188020 loss: 0.0091 lr: 0.02\n","iteration: 188030 loss: 0.0080 lr: 0.02\n","iteration: 188040 loss: 0.0098 lr: 0.02\n","iteration: 188050 loss: 0.0096 lr: 0.02\n","iteration: 188060 loss: 0.0084 lr: 0.02\n","iteration: 188070 loss: 0.0065 lr: 0.02\n","iteration: 188080 loss: 0.0127 lr: 0.02\n","iteration: 188090 loss: 0.0081 lr: 0.02\n","iteration: 188100 loss: 0.0089 lr: 0.02\n","iteration: 188110 loss: 0.0107 lr: 0.02\n","iteration: 188120 loss: 0.0131 lr: 0.02\n","iteration: 188130 loss: 0.0079 lr: 0.02\n","iteration: 188140 loss: 0.0086 lr: 0.02\n","iteration: 188150 loss: 0.0100 lr: 0.02\n","iteration: 188160 loss: 0.0086 lr: 0.02\n","iteration: 188170 loss: 0.0087 lr: 0.02\n","iteration: 188180 loss: 0.0115 lr: 0.02\n","iteration: 188190 loss: 0.0115 lr: 0.02\n","iteration: 188200 loss: 0.0106 lr: 0.02\n","iteration: 188210 loss: 0.0083 lr: 0.02\n","iteration: 188220 loss: 0.0100 lr: 0.02\n","iteration: 188230 loss: 0.0110 lr: 0.02\n","iteration: 188240 loss: 0.0149 lr: 0.02\n","iteration: 188250 loss: 0.0092 lr: 0.02\n","iteration: 188260 loss: 0.0094 lr: 0.02\n","iteration: 188270 loss: 0.0126 lr: 0.02\n","iteration: 188280 loss: 0.0094 lr: 0.02\n","iteration: 188290 loss: 0.0098 lr: 0.02\n","iteration: 188300 loss: 0.0080 lr: 0.02\n","iteration: 188310 loss: 0.0105 lr: 0.02\n","iteration: 188320 loss: 0.0072 lr: 0.02\n","iteration: 188330 loss: 0.0145 lr: 0.02\n","iteration: 188340 loss: 0.0103 lr: 0.02\n","iteration: 188350 loss: 0.0078 lr: 0.02\n","iteration: 188360 loss: 0.0108 lr: 0.02\n","iteration: 188370 loss: 0.0121 lr: 0.02\n","iteration: 188380 loss: 0.0113 lr: 0.02\n","iteration: 188390 loss: 0.0116 lr: 0.02\n","iteration: 188400 loss: 0.0079 lr: 0.02\n","iteration: 188410 loss: 0.0100 lr: 0.02\n","iteration: 188420 loss: 0.0090 lr: 0.02\n","iteration: 188430 loss: 0.0082 lr: 0.02\n","iteration: 188440 loss: 0.0098 lr: 0.02\n","iteration: 188450 loss: 0.0083 lr: 0.02\n","iteration: 188460 loss: 0.0095 lr: 0.02\n","iteration: 188470 loss: 0.0088 lr: 0.02\n","iteration: 188480 loss: 0.0134 lr: 0.02\n","iteration: 188490 loss: 0.0168 lr: 0.02\n","iteration: 188500 loss: 0.0102 lr: 0.02\n","iteration: 188510 loss: 0.0097 lr: 0.02\n","iteration: 188520 loss: 0.0135 lr: 0.02\n","iteration: 188530 loss: 0.0084 lr: 0.02\n","iteration: 188540 loss: 0.0062 lr: 0.02\n","iteration: 188550 loss: 0.0095 lr: 0.02\n","iteration: 188560 loss: 0.0085 lr: 0.02\n","iteration: 188570 loss: 0.0110 lr: 0.02\n","iteration: 188580 loss: 0.0088 lr: 0.02\n","iteration: 188590 loss: 0.0077 lr: 0.02\n","iteration: 188600 loss: 0.0102 lr: 0.02\n","iteration: 188610 loss: 0.0122 lr: 0.02\n","iteration: 188620 loss: 0.0110 lr: 0.02\n","iteration: 188630 loss: 0.0087 lr: 0.02\n","iteration: 188640 loss: 0.0075 lr: 0.02\n","iteration: 188650 loss: 0.0091 lr: 0.02\n","iteration: 188660 loss: 0.0090 lr: 0.02\n","iteration: 188670 loss: 0.0066 lr: 0.02\n","iteration: 188680 loss: 0.0069 lr: 0.02\n","iteration: 188690 loss: 0.0097 lr: 0.02\n","iteration: 188700 loss: 0.0098 lr: 0.02\n","iteration: 188710 loss: 0.0084 lr: 0.02\n","iteration: 188720 loss: 0.0080 lr: 0.02\n","iteration: 188730 loss: 0.0066 lr: 0.02\n","iteration: 188740 loss: 0.0080 lr: 0.02\n","iteration: 188750 loss: 0.0084 lr: 0.02\n","iteration: 188760 loss: 0.0143 lr: 0.02\n","iteration: 188770 loss: 0.0105 lr: 0.02\n","iteration: 188780 loss: 0.0070 lr: 0.02\n","iteration: 188790 loss: 0.0106 lr: 0.02\n","iteration: 188800 loss: 0.0080 lr: 0.02\n","iteration: 188810 loss: 0.0136 lr: 0.02\n","iteration: 188820 loss: 0.0075 lr: 0.02\n","iteration: 188830 loss: 0.0133 lr: 0.02\n","iteration: 188840 loss: 0.0071 lr: 0.02\n","iteration: 188850 loss: 0.0091 lr: 0.02\n","iteration: 188860 loss: 0.0076 lr: 0.02\n","iteration: 188870 loss: 0.0101 lr: 0.02\n","iteration: 188880 loss: 0.0091 lr: 0.02\n","iteration: 188890 loss: 0.0095 lr: 0.02\n","iteration: 188900 loss: 0.0097 lr: 0.02\n","iteration: 188910 loss: 0.0085 lr: 0.02\n","iteration: 188920 loss: 0.0078 lr: 0.02\n","iteration: 188930 loss: 0.0145 lr: 0.02\n","iteration: 188940 loss: 0.0100 lr: 0.02\n","iteration: 188950 loss: 0.0092 lr: 0.02\n","iteration: 188960 loss: 0.0098 lr: 0.02\n","iteration: 188970 loss: 0.0085 lr: 0.02\n","iteration: 188980 loss: 0.0096 lr: 0.02\n","iteration: 188990 loss: 0.0075 lr: 0.02\n","iteration: 189000 loss: 0.0079 lr: 0.02\n","iteration: 189010 loss: 0.0074 lr: 0.02\n","iteration: 189020 loss: 0.0070 lr: 0.02\n","iteration: 189030 loss: 0.0081 lr: 0.02\n","iteration: 189040 loss: 0.0082 lr: 0.02\n","iteration: 189050 loss: 0.0075 lr: 0.02\n","iteration: 189060 loss: 0.0075 lr: 0.02\n","iteration: 189070 loss: 0.0119 lr: 0.02\n","iteration: 189080 loss: 0.0081 lr: 0.02\n","iteration: 189090 loss: 0.0069 lr: 0.02\n","iteration: 189100 loss: 0.0074 lr: 0.02\n","iteration: 189110 loss: 0.0088 lr: 0.02\n","iteration: 189120 loss: 0.0079 lr: 0.02\n","iteration: 189130 loss: 0.0070 lr: 0.02\n","iteration: 189140 loss: 0.0089 lr: 0.02\n","iteration: 189150 loss: 0.0098 lr: 0.02\n","iteration: 189160 loss: 0.0104 lr: 0.02\n","iteration: 189170 loss: 0.0093 lr: 0.02\n","iteration: 189180 loss: 0.0081 lr: 0.02\n","iteration: 189190 loss: 0.0104 lr: 0.02\n","iteration: 189200 loss: 0.0115 lr: 0.02\n","iteration: 189210 loss: 0.0097 lr: 0.02\n","iteration: 189220 loss: 0.0094 lr: 0.02\n","iteration: 189230 loss: 0.0085 lr: 0.02\n","iteration: 189240 loss: 0.0088 lr: 0.02\n","iteration: 189250 loss: 0.0084 lr: 0.02\n","iteration: 189260 loss: 0.0140 lr: 0.02\n","iteration: 189270 loss: 0.0094 lr: 0.02\n","iteration: 189280 loss: 0.0113 lr: 0.02\n","iteration: 189290 loss: 0.0100 lr: 0.02\n","iteration: 189300 loss: 0.0067 lr: 0.02\n","iteration: 189310 loss: 0.0104 lr: 0.02\n","iteration: 189320 loss: 0.0085 lr: 0.02\n","iteration: 189330 loss: 0.0074 lr: 0.02\n","iteration: 189340 loss: 0.0081 lr: 0.02\n","iteration: 189350 loss: 0.0084 lr: 0.02\n","iteration: 189360 loss: 0.0075 lr: 0.02\n","iteration: 189370 loss: 0.0089 lr: 0.02\n","iteration: 189380 loss: 0.0106 lr: 0.02\n","iteration: 189390 loss: 0.0108 lr: 0.02\n","iteration: 189400 loss: 0.0095 lr: 0.02\n","iteration: 189410 loss: 0.0099 lr: 0.02\n","iteration: 189420 loss: 0.0119 lr: 0.02\n","iteration: 189430 loss: 0.0115 lr: 0.02\n","iteration: 189440 loss: 0.0090 lr: 0.02\n","iteration: 189450 loss: 0.0079 lr: 0.02\n","iteration: 189460 loss: 0.0117 lr: 0.02\n","iteration: 189470 loss: 0.0108 lr: 0.02\n","iteration: 189480 loss: 0.0075 lr: 0.02\n","iteration: 189490 loss: 0.0112 lr: 0.02\n","iteration: 189500 loss: 0.0110 lr: 0.02\n","iteration: 189510 loss: 0.0106 lr: 0.02\n","iteration: 189520 loss: 0.0111 lr: 0.02\n","iteration: 189530 loss: 0.0089 lr: 0.02\n","iteration: 189540 loss: 0.0093 lr: 0.02\n","iteration: 189550 loss: 0.0107 lr: 0.02\n","iteration: 189560 loss: 0.0117 lr: 0.02\n","iteration: 189570 loss: 0.0099 lr: 0.02\n","iteration: 189580 loss: 0.0132 lr: 0.02\n","iteration: 189590 loss: 0.0084 lr: 0.02\n","iteration: 189600 loss: 0.0113 lr: 0.02\n","iteration: 189610 loss: 0.0102 lr: 0.02\n","iteration: 189620 loss: 0.0092 lr: 0.02\n","iteration: 189630 loss: 0.0075 lr: 0.02\n","iteration: 189640 loss: 0.0095 lr: 0.02\n","iteration: 189650 loss: 0.0072 lr: 0.02\n","iteration: 189660 loss: 0.0131 lr: 0.02\n","iteration: 189670 loss: 0.0078 lr: 0.02\n","iteration: 189680 loss: 0.0083 lr: 0.02\n","iteration: 189690 loss: 0.0071 lr: 0.02\n","iteration: 189700 loss: 0.0114 lr: 0.02\n","iteration: 189710 loss: 0.0086 lr: 0.02\n","iteration: 189720 loss: 0.0120 lr: 0.02\n","iteration: 189730 loss: 0.0116 lr: 0.02\n","iteration: 189740 loss: 0.0087 lr: 0.02\n","iteration: 189750 loss: 0.0093 lr: 0.02\n","iteration: 189760 loss: 0.0092 lr: 0.02\n","iteration: 189770 loss: 0.0092 lr: 0.02\n","iteration: 189780 loss: 0.0072 lr: 0.02\n","iteration: 189790 loss: 0.0080 lr: 0.02\n","iteration: 189800 loss: 0.0097 lr: 0.02\n","iteration: 189810 loss: 0.0092 lr: 0.02\n","iteration: 189820 loss: 0.0085 lr: 0.02\n","iteration: 189830 loss: 0.0086 lr: 0.02\n","iteration: 189840 loss: 0.0061 lr: 0.02\n","iteration: 189850 loss: 0.0076 lr: 0.02\n","iteration: 189860 loss: 0.0070 lr: 0.02\n","iteration: 189870 loss: 0.0090 lr: 0.02\n","iteration: 189880 loss: 0.0098 lr: 0.02\n","iteration: 189890 loss: 0.0065 lr: 0.02\n","iteration: 189900 loss: 0.0091 lr: 0.02\n","iteration: 189910 loss: 0.0099 lr: 0.02\n","iteration: 189920 loss: 0.0121 lr: 0.02\n","iteration: 189930 loss: 0.0099 lr: 0.02\n","iteration: 189940 loss: 0.0070 lr: 0.02\n","iteration: 189950 loss: 0.0084 lr: 0.02\n","iteration: 189960 loss: 0.0064 lr: 0.02\n","iteration: 189970 loss: 0.0083 lr: 0.02\n","iteration: 189980 loss: 0.0074 lr: 0.02\n","iteration: 189990 loss: 0.0098 lr: 0.02\n","iteration: 190000 loss: 0.0097 lr: 0.02\n","iteration: 190010 loss: 0.0122 lr: 0.02\n","iteration: 190020 loss: 0.0105 lr: 0.02\n","iteration: 190030 loss: 0.0099 lr: 0.02\n","iteration: 190040 loss: 0.0107 lr: 0.02\n","iteration: 190050 loss: 0.0088 lr: 0.02\n","iteration: 190060 loss: 0.0115 lr: 0.02\n","iteration: 190070 loss: 0.0130 lr: 0.02\n","iteration: 190080 loss: 0.0090 lr: 0.02\n","iteration: 190090 loss: 0.0101 lr: 0.02\n","iteration: 190100 loss: 0.0094 lr: 0.02\n","iteration: 190110 loss: 0.0083 lr: 0.02\n","iteration: 190120 loss: 0.0099 lr: 0.02\n","iteration: 190130 loss: 0.0083 lr: 0.02\n","iteration: 190140 loss: 0.0090 lr: 0.02\n","iteration: 190150 loss: 0.0088 lr: 0.02\n","iteration: 190160 loss: 0.0135 lr: 0.02\n","iteration: 190170 loss: 0.0086 lr: 0.02\n","iteration: 190180 loss: 0.0103 lr: 0.02\n","iteration: 190190 loss: 0.0123 lr: 0.02\n","iteration: 190200 loss: 0.0087 lr: 0.02\n","iteration: 190210 loss: 0.0119 lr: 0.02\n","iteration: 190220 loss: 0.0125 lr: 0.02\n","iteration: 190230 loss: 0.0095 lr: 0.02\n","iteration: 190240 loss: 0.0130 lr: 0.02\n","iteration: 190250 loss: 0.0115 lr: 0.02\n","iteration: 190260 loss: 0.0067 lr: 0.02\n","iteration: 190270 loss: 0.0099 lr: 0.02\n","iteration: 190280 loss: 0.0106 lr: 0.02\n","iteration: 190290 loss: 0.0065 lr: 0.02\n","iteration: 190300 loss: 0.0076 lr: 0.02\n","iteration: 190310 loss: 0.0071 lr: 0.02\n","iteration: 190320 loss: 0.0097 lr: 0.02\n","iteration: 190330 loss: 0.0094 lr: 0.02\n","iteration: 190340 loss: 0.0083 lr: 0.02\n","iteration: 190350 loss: 0.0098 lr: 0.02\n","iteration: 190360 loss: 0.0124 lr: 0.02\n","iteration: 190370 loss: 0.0099 lr: 0.02\n","iteration: 190380 loss: 0.0105 lr: 0.02\n","iteration: 190390 loss: 0.0081 lr: 0.02\n","iteration: 190400 loss: 0.0086 lr: 0.02\n","iteration: 190410 loss: 0.0079 lr: 0.02\n","iteration: 190420 loss: 0.0075 lr: 0.02\n","iteration: 190430 loss: 0.0078 lr: 0.02\n","iteration: 190440 loss: 0.0099 lr: 0.02\n","iteration: 190450 loss: 0.0109 lr: 0.02\n","iteration: 190460 loss: 0.0086 lr: 0.02\n","iteration: 190470 loss: 0.0078 lr: 0.02\n","iteration: 190480 loss: 0.0087 lr: 0.02\n","iteration: 190490 loss: 0.0164 lr: 0.02\n","iteration: 190500 loss: 0.0122 lr: 0.02\n","iteration: 190510 loss: 0.0071 lr: 0.02\n","iteration: 190520 loss: 0.0100 lr: 0.02\n","iteration: 190530 loss: 0.0087 lr: 0.02\n","iteration: 190540 loss: 0.0108 lr: 0.02\n","iteration: 190550 loss: 0.0106 lr: 0.02\n","iteration: 190560 loss: 0.0099 lr: 0.02\n","iteration: 190570 loss: 0.0091 lr: 0.02\n","iteration: 190580 loss: 0.0090 lr: 0.02\n","iteration: 190590 loss: 0.0080 lr: 0.02\n","iteration: 190600 loss: 0.0095 lr: 0.02\n","iteration: 190610 loss: 0.0105 lr: 0.02\n","iteration: 190620 loss: 0.0107 lr: 0.02\n","iteration: 190630 loss: 0.0104 lr: 0.02\n","iteration: 190640 loss: 0.0087 lr: 0.02\n","iteration: 190650 loss: 0.0089 lr: 0.02\n","iteration: 190660 loss: 0.0066 lr: 0.02\n","iteration: 190670 loss: 0.0090 lr: 0.02\n","iteration: 190680 loss: 0.0081 lr: 0.02\n","iteration: 190690 loss: 0.0096 lr: 0.02\n","iteration: 190700 loss: 0.0100 lr: 0.02\n","iteration: 190710 loss: 0.0093 lr: 0.02\n","iteration: 190720 loss: 0.0102 lr: 0.02\n","iteration: 190730 loss: 0.0082 lr: 0.02\n","iteration: 190740 loss: 0.0088 lr: 0.02\n","iteration: 190750 loss: 0.0086 lr: 0.02\n","iteration: 190760 loss: 0.0104 lr: 0.02\n","iteration: 190770 loss: 0.0096 lr: 0.02\n","iteration: 190780 loss: 0.0096 lr: 0.02\n","iteration: 190790 loss: 0.0082 lr: 0.02\n","iteration: 190800 loss: 0.0096 lr: 0.02\n","iteration: 190810 loss: 0.0140 lr: 0.02\n","iteration: 190820 loss: 0.0077 lr: 0.02\n","iteration: 190830 loss: 0.0096 lr: 0.02\n","iteration: 190840 loss: 0.0085 lr: 0.02\n","iteration: 190850 loss: 0.0103 lr: 0.02\n","iteration: 190860 loss: 0.0102 lr: 0.02\n","iteration: 190870 loss: 0.0082 lr: 0.02\n","iteration: 190880 loss: 0.0088 lr: 0.02\n","iteration: 190890 loss: 0.0079 lr: 0.02\n","iteration: 190900 loss: 0.0097 lr: 0.02\n","iteration: 190910 loss: 0.0101 lr: 0.02\n","iteration: 190920 loss: 0.0066 lr: 0.02\n","iteration: 190930 loss: 0.0088 lr: 0.02\n","iteration: 190940 loss: 0.0082 lr: 0.02\n","iteration: 190950 loss: 0.0094 lr: 0.02\n","iteration: 190960 loss: 0.0076 lr: 0.02\n","iteration: 190970 loss: 0.0105 lr: 0.02\n","iteration: 190980 loss: 0.0081 lr: 0.02\n","iteration: 190990 loss: 0.0083 lr: 0.02\n","iteration: 191000 loss: 0.0074 lr: 0.02\n","iteration: 191010 loss: 0.0089 lr: 0.02\n","iteration: 191020 loss: 0.0079 lr: 0.02\n","iteration: 191030 loss: 0.0082 lr: 0.02\n","iteration: 191040 loss: 0.0078 lr: 0.02\n","iteration: 191050 loss: 0.0064 lr: 0.02\n","iteration: 191060 loss: 0.0085 lr: 0.02\n","iteration: 191070 loss: 0.0089 lr: 0.02\n","iteration: 191080 loss: 0.0081 lr: 0.02\n","iteration: 191090 loss: 0.0095 lr: 0.02\n","iteration: 191100 loss: 0.0111 lr: 0.02\n","iteration: 191110 loss: 0.0094 lr: 0.02\n","iteration: 191120 loss: 0.0079 lr: 0.02\n","iteration: 191130 loss: 0.0062 lr: 0.02\n","iteration: 191140 loss: 0.0063 lr: 0.02\n","iteration: 191150 loss: 0.0112 lr: 0.02\n","iteration: 191160 loss: 0.0101 lr: 0.02\n","iteration: 191170 loss: 0.0077 lr: 0.02\n","iteration: 191180 loss: 0.0082 lr: 0.02\n","iteration: 191190 loss: 0.0075 lr: 0.02\n","iteration: 191200 loss: 0.0083 lr: 0.02\n","iteration: 191210 loss: 0.0085 lr: 0.02\n","iteration: 191220 loss: 0.0109 lr: 0.02\n","iteration: 191230 loss: 0.0082 lr: 0.02\n","iteration: 191240 loss: 0.0063 lr: 0.02\n","iteration: 191250 loss: 0.0101 lr: 0.02\n","iteration: 191260 loss: 0.0109 lr: 0.02\n","iteration: 191270 loss: 0.0108 lr: 0.02\n","iteration: 191280 loss: 0.0066 lr: 0.02\n","iteration: 191290 loss: 0.0092 lr: 0.02\n","iteration: 191300 loss: 0.0090 lr: 0.02\n","iteration: 191310 loss: 0.0079 lr: 0.02\n","iteration: 191320 loss: 0.0063 lr: 0.02\n","iteration: 191330 loss: 0.0100 lr: 0.02\n","iteration: 191340 loss: 0.0091 lr: 0.02\n","iteration: 191350 loss: 0.0108 lr: 0.02\n","iteration: 191360 loss: 0.0078 lr: 0.02\n","iteration: 191370 loss: 0.0106 lr: 0.02\n","iteration: 191380 loss: 0.0090 lr: 0.02\n","iteration: 191390 loss: 0.0118 lr: 0.02\n","iteration: 191400 loss: 0.0082 lr: 0.02\n","iteration: 191410 loss: 0.0111 lr: 0.02\n","iteration: 191420 loss: 0.0077 lr: 0.02\n","iteration: 191430 loss: 0.0077 lr: 0.02\n","iteration: 191440 loss: 0.0095 lr: 0.02\n","iteration: 191450 loss: 0.0080 lr: 0.02\n","iteration: 191460 loss: 0.0062 lr: 0.02\n","iteration: 191470 loss: 0.0069 lr: 0.02\n","iteration: 191480 loss: 0.0084 lr: 0.02\n","iteration: 191490 loss: 0.0075 lr: 0.02\n","iteration: 191500 loss: 0.0087 lr: 0.02\n","iteration: 191510 loss: 0.0071 lr: 0.02\n","iteration: 191520 loss: 0.0091 lr: 0.02\n","iteration: 191530 loss: 0.0086 lr: 0.02\n","iteration: 191540 loss: 0.0089 lr: 0.02\n","iteration: 191550 loss: 0.0087 lr: 0.02\n","iteration: 191560 loss: 0.0096 lr: 0.02\n","iteration: 191570 loss: 0.0091 lr: 0.02\n","iteration: 191580 loss: 0.0075 lr: 0.02\n","iteration: 191590 loss: 0.0094 lr: 0.02\n","iteration: 191600 loss: 0.0074 lr: 0.02\n","iteration: 191610 loss: 0.0094 lr: 0.02\n","iteration: 191620 loss: 0.0091 lr: 0.02\n","iteration: 191630 loss: 0.0120 lr: 0.02\n","iteration: 191640 loss: 0.0079 lr: 0.02\n","iteration: 191650 loss: 0.0081 lr: 0.02\n","iteration: 191660 loss: 0.0129 lr: 0.02\n","iteration: 191670 loss: 0.0079 lr: 0.02\n","iteration: 191680 loss: 0.0101 lr: 0.02\n","iteration: 191690 loss: 0.0066 lr: 0.02\n","iteration: 191700 loss: 0.0096 lr: 0.02\n","iteration: 191710 loss: 0.0130 lr: 0.02\n","iteration: 191720 loss: 0.0097 lr: 0.02\n","iteration: 191730 loss: 0.0145 lr: 0.02\n","iteration: 191740 loss: 0.0087 lr: 0.02\n","iteration: 191750 loss: 0.0085 lr: 0.02\n","iteration: 191760 loss: 0.0076 lr: 0.02\n","iteration: 191770 loss: 0.0095 lr: 0.02\n","iteration: 191780 loss: 0.0054 lr: 0.02\n","iteration: 191790 loss: 0.0071 lr: 0.02\n","iteration: 191800 loss: 0.0096 lr: 0.02\n","iteration: 191810 loss: 0.0095 lr: 0.02\n","iteration: 191820 loss: 0.0061 lr: 0.02\n","iteration: 191830 loss: 0.0086 lr: 0.02\n","iteration: 191840 loss: 0.0098 lr: 0.02\n","iteration: 191850 loss: 0.0115 lr: 0.02\n","iteration: 191860 loss: 0.0095 lr: 0.02\n","iteration: 191870 loss: 0.0084 lr: 0.02\n","iteration: 191880 loss: 0.0111 lr: 0.02\n","iteration: 191890 loss: 0.0103 lr: 0.02\n","iteration: 191900 loss: 0.0089 lr: 0.02\n","iteration: 191910 loss: 0.0063 lr: 0.02\n","iteration: 191920 loss: 0.0081 lr: 0.02\n","iteration: 191930 loss: 0.0094 lr: 0.02\n","iteration: 191940 loss: 0.0140 lr: 0.02\n","iteration: 191950 loss: 0.0087 lr: 0.02\n","iteration: 191960 loss: 0.0082 lr: 0.02\n","iteration: 191970 loss: 0.0095 lr: 0.02\n","iteration: 191980 loss: 0.0082 lr: 0.02\n","iteration: 191990 loss: 0.0086 lr: 0.02\n","iteration: 192000 loss: 0.0079 lr: 0.02\n","iteration: 192010 loss: 0.0077 lr: 0.02\n","iteration: 192020 loss: 0.0082 lr: 0.02\n","iteration: 192030 loss: 0.0091 lr: 0.02\n","iteration: 192040 loss: 0.0084 lr: 0.02\n","iteration: 192050 loss: 0.0079 lr: 0.02\n","iteration: 192060 loss: 0.0090 lr: 0.02\n","iteration: 192070 loss: 0.0088 lr: 0.02\n","iteration: 192080 loss: 0.0139 lr: 0.02\n","iteration: 192090 loss: 0.0070 lr: 0.02\n","iteration: 192100 loss: 0.0102 lr: 0.02\n","iteration: 192110 loss: 0.0080 lr: 0.02\n","iteration: 192120 loss: 0.0080 lr: 0.02\n","iteration: 192130 loss: 0.0112 lr: 0.02\n","iteration: 192140 loss: 0.0085 lr: 0.02\n","iteration: 192150 loss: 0.0105 lr: 0.02\n","iteration: 192160 loss: 0.0077 lr: 0.02\n","iteration: 192170 loss: 0.0089 lr: 0.02\n","iteration: 192180 loss: 0.0082 lr: 0.02\n","iteration: 192190 loss: 0.0098 lr: 0.02\n","iteration: 192200 loss: 0.0100 lr: 0.02\n","iteration: 192210 loss: 0.0080 lr: 0.02\n","iteration: 192220 loss: 0.0077 lr: 0.02\n","iteration: 192230 loss: 0.0097 lr: 0.02\n","iteration: 192240 loss: 0.0075 lr: 0.02\n","iteration: 192250 loss: 0.0087 lr: 0.02\n","iteration: 192260 loss: 0.0125 lr: 0.02\n","iteration: 192270 loss: 0.0217 lr: 0.02\n","iteration: 192280 loss: 0.0099 lr: 0.02\n","iteration: 192290 loss: 0.0113 lr: 0.02\n","iteration: 192300 loss: 0.0116 lr: 0.02\n","iteration: 192310 loss: 0.0108 lr: 0.02\n","iteration: 192320 loss: 0.0117 lr: 0.02\n","iteration: 192330 loss: 0.0086 lr: 0.02\n","iteration: 192340 loss: 0.0101 lr: 0.02\n","iteration: 192350 loss: 0.0090 lr: 0.02\n","iteration: 192360 loss: 0.0101 lr: 0.02\n","iteration: 192370 loss: 0.0088 lr: 0.02\n","iteration: 192380 loss: 0.0103 lr: 0.02\n","iteration: 192390 loss: 0.0094 lr: 0.02\n","iteration: 192400 loss: 0.0098 lr: 0.02\n","iteration: 192410 loss: 0.0096 lr: 0.02\n","iteration: 192420 loss: 0.0092 lr: 0.02\n","iteration: 192430 loss: 0.0114 lr: 0.02\n","iteration: 192440 loss: 0.0093 lr: 0.02\n","iteration: 192450 loss: 0.0078 lr: 0.02\n","iteration: 192460 loss: 0.0108 lr: 0.02\n","iteration: 192470 loss: 0.0103 lr: 0.02\n","iteration: 192480 loss: 0.0093 lr: 0.02\n","iteration: 192490 loss: 0.0081 lr: 0.02\n","iteration: 192500 loss: 0.0085 lr: 0.02\n","iteration: 192510 loss: 0.0088 lr: 0.02\n","iteration: 192520 loss: 0.0097 lr: 0.02\n","iteration: 192530 loss: 0.0108 lr: 0.02\n","iteration: 192540 loss: 0.0097 lr: 0.02\n","iteration: 192550 loss: 0.0069 lr: 0.02\n","iteration: 192560 loss: 0.0103 lr: 0.02\n","iteration: 192570 loss: 0.0091 lr: 0.02\n","iteration: 192580 loss: 0.0068 lr: 0.02\n","iteration: 192590 loss: 0.0106 lr: 0.02\n","iteration: 192600 loss: 0.0098 lr: 0.02\n","iteration: 192610 loss: 0.0086 lr: 0.02\n","iteration: 192620 loss: 0.0105 lr: 0.02\n","iteration: 192630 loss: 0.0105 lr: 0.02\n","iteration: 192640 loss: 0.0107 lr: 0.02\n","iteration: 192650 loss: 0.0087 lr: 0.02\n","iteration: 192660 loss: 0.0078 lr: 0.02\n","iteration: 192670 loss: 0.0093 lr: 0.02\n","iteration: 192680 loss: 0.0122 lr: 0.02\n","iteration: 192690 loss: 0.0081 lr: 0.02\n","iteration: 192700 loss: 0.0094 lr: 0.02\n","iteration: 192710 loss: 0.0115 lr: 0.02\n","iteration: 192720 loss: 0.0069 lr: 0.02\n","iteration: 192730 loss: 0.0090 lr: 0.02\n","iteration: 192740 loss: 0.0087 lr: 0.02\n","iteration: 192750 loss: 0.0079 lr: 0.02\n","iteration: 192760 loss: 0.0106 lr: 0.02\n","iteration: 192770 loss: 0.0075 lr: 0.02\n","iteration: 192780 loss: 0.0099 lr: 0.02\n","iteration: 192790 loss: 0.0077 lr: 0.02\n","iteration: 192800 loss: 0.0079 lr: 0.02\n","iteration: 192810 loss: 0.0085 lr: 0.02\n","iteration: 192820 loss: 0.0106 lr: 0.02\n","iteration: 192830 loss: 0.0068 lr: 0.02\n","iteration: 192840 loss: 0.0089 lr: 0.02\n","iteration: 192850 loss: 0.0091 lr: 0.02\n","iteration: 192860 loss: 0.0071 lr: 0.02\n","iteration: 192870 loss: 0.0102 lr: 0.02\n","iteration: 192880 loss: 0.0091 lr: 0.02\n","iteration: 192890 loss: 0.0077 lr: 0.02\n","iteration: 192900 loss: 0.0098 lr: 0.02\n","iteration: 192910 loss: 0.0087 lr: 0.02\n","iteration: 192920 loss: 0.0066 lr: 0.02\n","iteration: 192930 loss: 0.0068 lr: 0.02\n","iteration: 192940 loss: 0.0110 lr: 0.02\n","iteration: 192950 loss: 0.0102 lr: 0.02\n","iteration: 192960 loss: 0.0114 lr: 0.02\n","iteration: 192970 loss: 0.0073 lr: 0.02\n","iteration: 192980 loss: 0.0095 lr: 0.02\n","iteration: 192990 loss: 0.0100 lr: 0.02\n","iteration: 193000 loss: 0.0090 lr: 0.02\n","iteration: 193010 loss: 0.0168 lr: 0.02\n","iteration: 193020 loss: 0.0097 lr: 0.02\n","iteration: 193030 loss: 0.0077 lr: 0.02\n","iteration: 193040 loss: 0.0095 lr: 0.02\n","iteration: 193050 loss: 0.0117 lr: 0.02\n","iteration: 193060 loss: 0.0085 lr: 0.02\n","iteration: 193070 loss: 0.0099 lr: 0.02\n","iteration: 193080 loss: 0.0105 lr: 0.02\n","iteration: 193090 loss: 0.0081 lr: 0.02\n","iteration: 193100 loss: 0.0103 lr: 0.02\n","iteration: 193110 loss: 0.0079 lr: 0.02\n","iteration: 193120 loss: 0.0070 lr: 0.02\n","iteration: 193130 loss: 0.0091 lr: 0.02\n","iteration: 193140 loss: 0.0084 lr: 0.02\n","iteration: 193150 loss: 0.0096 lr: 0.02\n","iteration: 193160 loss: 0.0105 lr: 0.02\n","iteration: 193170 loss: 0.0117 lr: 0.02\n","iteration: 193180 loss: 0.0070 lr: 0.02\n","iteration: 193190 loss: 0.0068 lr: 0.02\n","iteration: 193200 loss: 0.0108 lr: 0.02\n","iteration: 193210 loss: 0.0111 lr: 0.02\n","iteration: 193220 loss: 0.0099 lr: 0.02\n","iteration: 193230 loss: 0.0108 lr: 0.02\n","iteration: 193240 loss: 0.0113 lr: 0.02\n","iteration: 193250 loss: 0.0088 lr: 0.02\n","iteration: 193260 loss: 0.0117 lr: 0.02\n","iteration: 193270 loss: 0.0065 lr: 0.02\n","iteration: 193280 loss: 0.0123 lr: 0.02\n","iteration: 193290 loss: 0.0078 lr: 0.02\n","iteration: 193300 loss: 0.0071 lr: 0.02\n","iteration: 193310 loss: 0.0083 lr: 0.02\n","iteration: 193320 loss: 0.0076 lr: 0.02\n","iteration: 193330 loss: 0.0111 lr: 0.02\n","iteration: 193340 loss: 0.0104 lr: 0.02\n","iteration: 193350 loss: 0.0108 lr: 0.02\n","iteration: 193360 loss: 0.0105 lr: 0.02\n","iteration: 193370 loss: 0.0088 lr: 0.02\n","iteration: 193380 loss: 0.0098 lr: 0.02\n","iteration: 193390 loss: 0.0093 lr: 0.02\n","iteration: 193400 loss: 0.0083 lr: 0.02\n","iteration: 193410 loss: 0.0089 lr: 0.02\n","iteration: 193420 loss: 0.0062 lr: 0.02\n","iteration: 193430 loss: 0.0098 lr: 0.02\n","iteration: 193440 loss: 0.0104 lr: 0.02\n","iteration: 193450 loss: 0.0142 lr: 0.02\n","iteration: 193460 loss: 0.0076 lr: 0.02\n","iteration: 193470 loss: 0.0077 lr: 0.02\n","iteration: 193480 loss: 0.0115 lr: 0.02\n","iteration: 193490 loss: 0.0101 lr: 0.02\n","iteration: 193500 loss: 0.0096 lr: 0.02\n","iteration: 193510 loss: 0.0062 lr: 0.02\n","iteration: 193520 loss: 0.0103 lr: 0.02\n","iteration: 193530 loss: 0.0116 lr: 0.02\n","iteration: 193540 loss: 0.0100 lr: 0.02\n","iteration: 193550 loss: 0.0086 lr: 0.02\n","iteration: 193560 loss: 0.0080 lr: 0.02\n","iteration: 193570 loss: 0.0096 lr: 0.02\n","iteration: 193580 loss: 0.0089 lr: 0.02\n","iteration: 193590 loss: 0.0102 lr: 0.02\n","iteration: 193600 loss: 0.0083 lr: 0.02\n","iteration: 193610 loss: 0.0100 lr: 0.02\n","iteration: 193620 loss: 0.0081 lr: 0.02\n","iteration: 193630 loss: 0.0101 lr: 0.02\n","iteration: 193640 loss: 0.0096 lr: 0.02\n","iteration: 193650 loss: 0.0107 lr: 0.02\n","iteration: 193660 loss: 0.0085 lr: 0.02\n","iteration: 193670 loss: 0.0087 lr: 0.02\n","iteration: 193680 loss: 0.0070 lr: 0.02\n","iteration: 193690 loss: 0.0086 lr: 0.02\n","iteration: 193700 loss: 0.0115 lr: 0.02\n","iteration: 193710 loss: 0.0109 lr: 0.02\n","iteration: 193720 loss: 0.0103 lr: 0.02\n","iteration: 193730 loss: 0.0099 lr: 0.02\n","iteration: 193740 loss: 0.0067 lr: 0.02\n","iteration: 193750 loss: 0.0090 lr: 0.02\n","iteration: 193760 loss: 0.0097 lr: 0.02\n","iteration: 193770 loss: 0.0099 lr: 0.02\n","iteration: 193780 loss: 0.0069 lr: 0.02\n","iteration: 193790 loss: 0.0081 lr: 0.02\n","iteration: 193800 loss: 0.0068 lr: 0.02\n","iteration: 193810 loss: 0.0087 lr: 0.02\n","iteration: 193820 loss: 0.0097 lr: 0.02\n","iteration: 193830 loss: 0.0088 lr: 0.02\n","iteration: 193840 loss: 0.0084 lr: 0.02\n","iteration: 193850 loss: 0.0080 lr: 0.02\n","iteration: 193860 loss: 0.0093 lr: 0.02\n","iteration: 193870 loss: 0.0096 lr: 0.02\n","iteration: 193880 loss: 0.0084 lr: 0.02\n","iteration: 193890 loss: 0.0099 lr: 0.02\n","iteration: 193900 loss: 0.0089 lr: 0.02\n","iteration: 193910 loss: 0.0099 lr: 0.02\n","iteration: 193920 loss: 0.0097 lr: 0.02\n","iteration: 193930 loss: 0.0062 lr: 0.02\n","iteration: 193940 loss: 0.0082 lr: 0.02\n","iteration: 193950 loss: 0.0097 lr: 0.02\n","iteration: 193960 loss: 0.0112 lr: 0.02\n","iteration: 193970 loss: 0.0117 lr: 0.02\n","iteration: 193980 loss: 0.0122 lr: 0.02\n","iteration: 193990 loss: 0.0119 lr: 0.02\n","iteration: 194000 loss: 0.0109 lr: 0.02\n","iteration: 194010 loss: 0.0100 lr: 0.02\n","iteration: 194020 loss: 0.0140 lr: 0.02\n","iteration: 194030 loss: 0.0111 lr: 0.02\n","iteration: 194040 loss: 0.0077 lr: 0.02\n","iteration: 194050 loss: 0.0093 lr: 0.02\n","iteration: 194060 loss: 0.0092 lr: 0.02\n","iteration: 194070 loss: 0.0089 lr: 0.02\n","iteration: 194080 loss: 0.0100 lr: 0.02\n","iteration: 194090 loss: 0.0073 lr: 0.02\n","iteration: 194100 loss: 0.0092 lr: 0.02\n","iteration: 194110 loss: 0.0104 lr: 0.02\n","iteration: 194120 loss: 0.0075 lr: 0.02\n","iteration: 194130 loss: 0.0090 lr: 0.02\n","iteration: 194140 loss: 0.0079 lr: 0.02\n","iteration: 194150 loss: 0.0094 lr: 0.02\n","iteration: 194160 loss: 0.0103 lr: 0.02\n","iteration: 194170 loss: 0.0077 lr: 0.02\n","iteration: 194180 loss: 0.0122 lr: 0.02\n","iteration: 194190 loss: 0.0091 lr: 0.02\n","iteration: 194200 loss: 0.0107 lr: 0.02\n","iteration: 194210 loss: 0.0120 lr: 0.02\n","iteration: 194220 loss: 0.0088 lr: 0.02\n","iteration: 194230 loss: 0.0090 lr: 0.02\n","iteration: 194240 loss: 0.0128 lr: 0.02\n","iteration: 194250 loss: 0.0075 lr: 0.02\n","iteration: 194260 loss: 0.0087 lr: 0.02\n","iteration: 194270 loss: 0.0100 lr: 0.02\n","iteration: 194280 loss: 0.0105 lr: 0.02\n","iteration: 194290 loss: 0.0088 lr: 0.02\n","iteration: 194300 loss: 0.0112 lr: 0.02\n","iteration: 194310 loss: 0.0079 lr: 0.02\n","iteration: 194320 loss: 0.0105 lr: 0.02\n","iteration: 194330 loss: 0.0107 lr: 0.02\n","iteration: 194340 loss: 0.0079 lr: 0.02\n","iteration: 194350 loss: 0.0094 lr: 0.02\n","iteration: 194360 loss: 0.0076 lr: 0.02\n","iteration: 194370 loss: 0.0084 lr: 0.02\n","iteration: 194380 loss: 0.0078 lr: 0.02\n","iteration: 194390 loss: 0.0090 lr: 0.02\n","iteration: 194400 loss: 0.0115 lr: 0.02\n","iteration: 194410 loss: 0.0092 lr: 0.02\n","iteration: 194420 loss: 0.0095 lr: 0.02\n","iteration: 194430 loss: 0.0086 lr: 0.02\n","iteration: 194440 loss: 0.0092 lr: 0.02\n","iteration: 194450 loss: 0.0102 lr: 0.02\n","iteration: 194460 loss: 0.0061 lr: 0.02\n","iteration: 194470 loss: 0.0079 lr: 0.02\n","iteration: 194480 loss: 0.0069 lr: 0.02\n","iteration: 194490 loss: 0.0116 lr: 0.02\n","iteration: 194500 loss: 0.0080 lr: 0.02\n","iteration: 194510 loss: 0.0106 lr: 0.02\n","iteration: 194520 loss: 0.0067 lr: 0.02\n","iteration: 194530 loss: 0.0052 lr: 0.02\n","iteration: 194540 loss: 0.0094 lr: 0.02\n","iteration: 194550 loss: 0.0080 lr: 0.02\n","iteration: 194560 loss: 0.0120 lr: 0.02\n","iteration: 194570 loss: 0.0108 lr: 0.02\n","iteration: 194580 loss: 0.0077 lr: 0.02\n","iteration: 194590 loss: 0.0075 lr: 0.02\n","iteration: 194600 loss: 0.0080 lr: 0.02\n","iteration: 194610 loss: 0.0062 lr: 0.02\n","iteration: 194620 loss: 0.0072 lr: 0.02\n","iteration: 194630 loss: 0.0104 lr: 0.02\n","iteration: 194640 loss: 0.0064 lr: 0.02\n","iteration: 194650 loss: 0.0099 lr: 0.02\n","iteration: 194660 loss: 0.0098 lr: 0.02\n","iteration: 194670 loss: 0.0144 lr: 0.02\n","iteration: 194680 loss: 0.0085 lr: 0.02\n","iteration: 194690 loss: 0.0086 lr: 0.02\n","iteration: 194700 loss: 0.0098 lr: 0.02\n","iteration: 194710 loss: 0.0072 lr: 0.02\n","iteration: 194720 loss: 0.0093 lr: 0.02\n","iteration: 194730 loss: 0.0082 lr: 0.02\n","iteration: 194740 loss: 0.0086 lr: 0.02\n","iteration: 194750 loss: 0.0080 lr: 0.02\n","iteration: 194760 loss: 0.0101 lr: 0.02\n","iteration: 194770 loss: 0.0088 lr: 0.02\n","iteration: 194780 loss: 0.0095 lr: 0.02\n","iteration: 194790 loss: 0.0100 lr: 0.02\n","iteration: 194800 loss: 0.0115 lr: 0.02\n","iteration: 194810 loss: 0.0077 lr: 0.02\n","iteration: 194820 loss: 0.0093 lr: 0.02\n","iteration: 194830 loss: 0.0122 lr: 0.02\n","iteration: 194840 loss: 0.0089 lr: 0.02\n","iteration: 194850 loss: 0.0075 lr: 0.02\n","iteration: 194860 loss: 0.0078 lr: 0.02\n","iteration: 194870 loss: 0.0080 lr: 0.02\n","iteration: 194880 loss: 0.0095 lr: 0.02\n","iteration: 194890 loss: 0.0106 lr: 0.02\n","iteration: 194900 loss: 0.0084 lr: 0.02\n","iteration: 194910 loss: 0.0081 lr: 0.02\n","iteration: 194920 loss: 0.0100 lr: 0.02\n","iteration: 194930 loss: 0.0094 lr: 0.02\n","iteration: 194940 loss: 0.0110 lr: 0.02\n","iteration: 194950 loss: 0.0131 lr: 0.02\n","iteration: 194960 loss: 0.0106 lr: 0.02\n","iteration: 194970 loss: 0.0094 lr: 0.02\n","iteration: 194980 loss: 0.0091 lr: 0.02\n","iteration: 194990 loss: 0.0092 lr: 0.02\n","iteration: 195000 loss: 0.0112 lr: 0.02\n","iteration: 195010 loss: 0.0074 lr: 0.02\n","iteration: 195020 loss: 0.0098 lr: 0.02\n","iteration: 195030 loss: 0.0079 lr: 0.02\n","iteration: 195040 loss: 0.0099 lr: 0.02\n","iteration: 195050 loss: 0.0074 lr: 0.02\n","iteration: 195060 loss: 0.0087 lr: 0.02\n","iteration: 195070 loss: 0.0084 lr: 0.02\n","iteration: 195080 loss: 0.0103 lr: 0.02\n","iteration: 195090 loss: 0.0096 lr: 0.02\n","iteration: 195100 loss: 0.0097 lr: 0.02\n","iteration: 195110 loss: 0.0076 lr: 0.02\n","iteration: 195120 loss: 0.0107 lr: 0.02\n","iteration: 195130 loss: 0.0097 lr: 0.02\n","iteration: 195140 loss: 0.0080 lr: 0.02\n","iteration: 195150 loss: 0.0083 lr: 0.02\n","iteration: 195160 loss: 0.0075 lr: 0.02\n","iteration: 195170 loss: 0.0090 lr: 0.02\n","iteration: 195180 loss: 0.0081 lr: 0.02\n","iteration: 195190 loss: 0.0097 lr: 0.02\n","iteration: 195200 loss: 0.0115 lr: 0.02\n","iteration: 195210 loss: 0.0073 lr: 0.02\n","iteration: 195220 loss: 0.0084 lr: 0.02\n","iteration: 195230 loss: 0.0101 lr: 0.02\n","iteration: 195240 loss: 0.0090 lr: 0.02\n","iteration: 195250 loss: 0.0093 lr: 0.02\n","iteration: 195260 loss: 0.0093 lr: 0.02\n","iteration: 195270 loss: 0.0098 lr: 0.02\n","iteration: 195280 loss: 0.0097 lr: 0.02\n","iteration: 195290 loss: 0.0075 lr: 0.02\n","iteration: 195300 loss: 0.0093 lr: 0.02\n","iteration: 195310 loss: 0.0131 lr: 0.02\n","iteration: 195320 loss: 0.0069 lr: 0.02\n","iteration: 195330 loss: 0.0084 lr: 0.02\n","iteration: 195340 loss: 0.0068 lr: 0.02\n","iteration: 195350 loss: 0.0084 lr: 0.02\n","iteration: 195360 loss: 0.0094 lr: 0.02\n","iteration: 195370 loss: 0.0094 lr: 0.02\n","iteration: 195380 loss: 0.0079 lr: 0.02\n","iteration: 195390 loss: 0.0087 lr: 0.02\n","iteration: 195400 loss: 0.0104 lr: 0.02\n","iteration: 195410 loss: 0.0090 lr: 0.02\n","iteration: 195420 loss: 0.0113 lr: 0.02\n","iteration: 195430 loss: 0.0079 lr: 0.02\n","iteration: 195440 loss: 0.0095 lr: 0.02\n","iteration: 195450 loss: 0.0112 lr: 0.02\n","iteration: 195460 loss: 0.0097 lr: 0.02\n","iteration: 195470 loss: 0.0077 lr: 0.02\n","iteration: 195480 loss: 0.0088 lr: 0.02\n","iteration: 195490 loss: 0.0083 lr: 0.02\n","iteration: 195500 loss: 0.0102 lr: 0.02\n","iteration: 195510 loss: 0.0106 lr: 0.02\n","iteration: 195520 loss: 0.0095 lr: 0.02\n","iteration: 195530 loss: 0.0104 lr: 0.02\n","iteration: 195540 loss: 0.0106 lr: 0.02\n","iteration: 195550 loss: 0.0117 lr: 0.02\n","iteration: 195560 loss: 0.0089 lr: 0.02\n","iteration: 195570 loss: 0.0078 lr: 0.02\n","iteration: 195580 loss: 0.0078 lr: 0.02\n","iteration: 195590 loss: 0.0075 lr: 0.02\n","iteration: 195600 loss: 0.0089 lr: 0.02\n","iteration: 195610 loss: 0.0078 lr: 0.02\n","iteration: 195620 loss: 0.0092 lr: 0.02\n","iteration: 195630 loss: 0.0085 lr: 0.02\n","iteration: 195640 loss: 0.0099 lr: 0.02\n","iteration: 195650 loss: 0.0090 lr: 0.02\n","iteration: 195660 loss: 0.0078 lr: 0.02\n","iteration: 195670 loss: 0.0112 lr: 0.02\n","iteration: 195680 loss: 0.0079 lr: 0.02\n","iteration: 195690 loss: 0.0076 lr: 0.02\n","iteration: 195700 loss: 0.0095 lr: 0.02\n","iteration: 195710 loss: 0.0106 lr: 0.02\n","iteration: 195720 loss: 0.0077 lr: 0.02\n","iteration: 195730 loss: 0.0094 lr: 0.02\n","iteration: 195740 loss: 0.0072 lr: 0.02\n","iteration: 195750 loss: 0.0085 lr: 0.02\n","iteration: 195760 loss: 0.0099 lr: 0.02\n","iteration: 195770 loss: 0.0090 lr: 0.02\n","iteration: 195780 loss: 0.0087 lr: 0.02\n","iteration: 195790 loss: 0.0098 lr: 0.02\n","iteration: 195800 loss: 0.0094 lr: 0.02\n","iteration: 195810 loss: 0.0070 lr: 0.02\n","iteration: 195820 loss: 0.0071 lr: 0.02\n","iteration: 195830 loss: 0.0130 lr: 0.02\n","iteration: 195840 loss: 0.0110 lr: 0.02\n","iteration: 195850 loss: 0.0113 lr: 0.02\n","iteration: 195860 loss: 0.0080 lr: 0.02\n","iteration: 195870 loss: 0.0070 lr: 0.02\n","iteration: 195880 loss: 0.0114 lr: 0.02\n","iteration: 195890 loss: 0.0092 lr: 0.02\n","iteration: 195900 loss: 0.0130 lr: 0.02\n","iteration: 195910 loss: 0.0086 lr: 0.02\n","iteration: 195920 loss: 0.0084 lr: 0.02\n","iteration: 195930 loss: 0.0083 lr: 0.02\n","iteration: 195940 loss: 0.0087 lr: 0.02\n","iteration: 195950 loss: 0.0080 lr: 0.02\n","iteration: 195960 loss: 0.0071 lr: 0.02\n","iteration: 195970 loss: 0.0082 lr: 0.02\n","iteration: 195980 loss: 0.0126 lr: 0.02\n","iteration: 195990 loss: 0.0084 lr: 0.02\n","iteration: 196000 loss: 0.0105 lr: 0.02\n","iteration: 196010 loss: 0.0090 lr: 0.02\n","iteration: 196020 loss: 0.0054 lr: 0.02\n","iteration: 196030 loss: 0.0103 lr: 0.02\n","iteration: 196040 loss: 0.0126 lr: 0.02\n","iteration: 196050 loss: 0.0092 lr: 0.02\n","iteration: 196060 loss: 0.0103 lr: 0.02\n","iteration: 196070 loss: 0.0095 lr: 0.02\n","iteration: 196080 loss: 0.0087 lr: 0.02\n","iteration: 196090 loss: 0.0086 lr: 0.02\n","iteration: 196100 loss: 0.0102 lr: 0.02\n","iteration: 196110 loss: 0.0076 lr: 0.02\n","iteration: 196120 loss: 0.0084 lr: 0.02\n","iteration: 196130 loss: 0.0101 lr: 0.02\n","iteration: 196140 loss: 0.0104 lr: 0.02\n","iteration: 196150 loss: 0.0103 lr: 0.02\n","iteration: 196160 loss: 0.0088 lr: 0.02\n","iteration: 196170 loss: 0.0089 lr: 0.02\n","iteration: 196180 loss: 0.0099 lr: 0.02\n","iteration: 196190 loss: 0.0120 lr: 0.02\n","iteration: 196200 loss: 0.0099 lr: 0.02\n","iteration: 196210 loss: 0.0083 lr: 0.02\n","iteration: 196220 loss: 0.0078 lr: 0.02\n","iteration: 196230 loss: 0.0088 lr: 0.02\n","iteration: 196240 loss: 0.0111 lr: 0.02\n","iteration: 196250 loss: 0.0079 lr: 0.02\n","iteration: 196260 loss: 0.0075 lr: 0.02\n","iteration: 196270 loss: 0.0085 lr: 0.02\n","iteration: 196280 loss: 0.0116 lr: 0.02\n","iteration: 196290 loss: 0.0086 lr: 0.02\n","iteration: 196300 loss: 0.0099 lr: 0.02\n","iteration: 196310 loss: 0.0084 lr: 0.02\n","iteration: 196320 loss: 0.0136 lr: 0.02\n","iteration: 196330 loss: 0.0093 lr: 0.02\n","iteration: 196340 loss: 0.0070 lr: 0.02\n","iteration: 196350 loss: 0.0107 lr: 0.02\n","iteration: 196360 loss: 0.0078 lr: 0.02\n","iteration: 196370 loss: 0.0093 lr: 0.02\n","iteration: 196380 loss: 0.0094 lr: 0.02\n","iteration: 196390 loss: 0.0106 lr: 0.02\n","iteration: 196400 loss: 0.0067 lr: 0.02\n","iteration: 196410 loss: 0.0091 lr: 0.02\n","iteration: 196420 loss: 0.0073 lr: 0.02\n","iteration: 196430 loss: 0.0094 lr: 0.02\n","iteration: 196440 loss: 0.0089 lr: 0.02\n","iteration: 196450 loss: 0.0104 lr: 0.02\n","iteration: 196460 loss: 0.0107 lr: 0.02\n","iteration: 196470 loss: 0.0098 lr: 0.02\n","iteration: 196480 loss: 0.0098 lr: 0.02\n","iteration: 196490 loss: 0.0095 lr: 0.02\n","iteration: 196500 loss: 0.0053 lr: 0.02\n","iteration: 196510 loss: 0.0080 lr: 0.02\n","iteration: 196520 loss: 0.0084 lr: 0.02\n","iteration: 196530 loss: 0.0074 lr: 0.02\n","iteration: 196540 loss: 0.0065 lr: 0.02\n","iteration: 196550 loss: 0.0101 lr: 0.02\n","iteration: 196560 loss: 0.0102 lr: 0.02\n","iteration: 196570 loss: 0.0100 lr: 0.02\n","iteration: 196580 loss: 0.0077 lr: 0.02\n","iteration: 196590 loss: 0.0085 lr: 0.02\n","iteration: 196600 loss: 0.0097 lr: 0.02\n","iteration: 196610 loss: 0.0058 lr: 0.02\n","iteration: 196620 loss: 0.0088 lr: 0.02\n","iteration: 196630 loss: 0.0077 lr: 0.02\n","iteration: 196640 loss: 0.0088 lr: 0.02\n","iteration: 196650 loss: 0.0078 lr: 0.02\n","iteration: 196660 loss: 0.0074 lr: 0.02\n","iteration: 196670 loss: 0.0075 lr: 0.02\n","iteration: 196680 loss: 0.0138 lr: 0.02\n","iteration: 196690 loss: 0.0060 lr: 0.02\n","iteration: 196700 loss: 0.0107 lr: 0.02\n","iteration: 196710 loss: 0.0107 lr: 0.02\n","iteration: 196720 loss: 0.0102 lr: 0.02\n","iteration: 196730 loss: 0.0098 lr: 0.02\n","iteration: 196740 loss: 0.0112 lr: 0.02\n","iteration: 196750 loss: 0.0152 lr: 0.02\n","iteration: 196760 loss: 0.0096 lr: 0.02\n","iteration: 196770 loss: 0.0110 lr: 0.02\n","iteration: 196780 loss: 0.0064 lr: 0.02\n","iteration: 196790 loss: 0.0104 lr: 0.02\n","iteration: 196800 loss: 0.0092 lr: 0.02\n","iteration: 196810 loss: 0.0061 lr: 0.02\n","iteration: 196820 loss: 0.0093 lr: 0.02\n","iteration: 196830 loss: 0.0102 lr: 0.02\n","iteration: 196840 loss: 0.0090 lr: 0.02\n","iteration: 196850 loss: 0.0086 lr: 0.02\n","iteration: 196860 loss: 0.0103 lr: 0.02\n","iteration: 196870 loss: 0.0094 lr: 0.02\n","iteration: 196880 loss: 0.0085 lr: 0.02\n","iteration: 196890 loss: 0.0072 lr: 0.02\n","iteration: 196900 loss: 0.0086 lr: 0.02\n","iteration: 196910 loss: 0.0103 lr: 0.02\n","iteration: 196920 loss: 0.0089 lr: 0.02\n","iteration: 196930 loss: 0.0065 lr: 0.02\n","iteration: 196940 loss: 0.0081 lr: 0.02\n","iteration: 196950 loss: 0.0106 lr: 0.02\n","iteration: 196960 loss: 0.0090 lr: 0.02\n","iteration: 196970 loss: 0.0079 lr: 0.02\n","iteration: 196980 loss: 0.0094 lr: 0.02\n","iteration: 196990 loss: 0.0078 lr: 0.02\n","iteration: 197000 loss: 0.0087 lr: 0.02\n","iteration: 197010 loss: 0.0136 lr: 0.02\n","iteration: 197020 loss: 0.0080 lr: 0.02\n","iteration: 197030 loss: 0.0084 lr: 0.02\n","iteration: 197040 loss: 0.0097 lr: 0.02\n","iteration: 197050 loss: 0.0092 lr: 0.02\n","iteration: 197060 loss: 0.0082 lr: 0.02\n","iteration: 197070 loss: 0.0128 lr: 0.02\n","iteration: 197080 loss: 0.0072 lr: 0.02\n","iteration: 197090 loss: 0.0109 lr: 0.02\n","iteration: 197100 loss: 0.0082 lr: 0.02\n","iteration: 197110 loss: 0.0119 lr: 0.02\n","iteration: 197120 loss: 0.0120 lr: 0.02\n","iteration: 197130 loss: 0.0128 lr: 0.02\n","iteration: 197140 loss: 0.0121 lr: 0.02\n","iteration: 197150 loss: 0.0093 lr: 0.02\n","iteration: 197160 loss: 0.0085 lr: 0.02\n","iteration: 197170 loss: 0.0102 lr: 0.02\n","iteration: 197180 loss: 0.0106 lr: 0.02\n","iteration: 197190 loss: 0.0101 lr: 0.02\n","iteration: 197200 loss: 0.0084 lr: 0.02\n","iteration: 197210 loss: 0.0091 lr: 0.02\n","iteration: 197220 loss: 0.0092 lr: 0.02\n","iteration: 197230 loss: 0.0091 lr: 0.02\n","iteration: 197240 loss: 0.0078 lr: 0.02\n","iteration: 197250 loss: 0.0081 lr: 0.02\n","iteration: 197260 loss: 0.0092 lr: 0.02\n","iteration: 197270 loss: 0.0095 lr: 0.02\n","iteration: 197280 loss: 0.0073 lr: 0.02\n","iteration: 197290 loss: 0.0093 lr: 0.02\n","iteration: 197300 loss: 0.0086 lr: 0.02\n","iteration: 197310 loss: 0.0092 lr: 0.02\n","iteration: 197320 loss: 0.0079 lr: 0.02\n","iteration: 197330 loss: 0.0083 lr: 0.02\n","iteration: 197340 loss: 0.0078 lr: 0.02\n","iteration: 197350 loss: 0.0064 lr: 0.02\n","iteration: 197360 loss: 0.0097 lr: 0.02\n","iteration: 197370 loss: 0.0103 lr: 0.02\n","iteration: 197380 loss: 0.0075 lr: 0.02\n","iteration: 197390 loss: 0.0124 lr: 0.02\n","iteration: 197400 loss: 0.0112 lr: 0.02\n","iteration: 197410 loss: 0.0097 lr: 0.02\n","iteration: 197420 loss: 0.0117 lr: 0.02\n","iteration: 197430 loss: 0.0067 lr: 0.02\n","iteration: 197440 loss: 0.0093 lr: 0.02\n","iteration: 197450 loss: 0.0106 lr: 0.02\n","iteration: 197460 loss: 0.0085 lr: 0.02\n","iteration: 197470 loss: 0.0085 lr: 0.02\n","iteration: 197480 loss: 0.0078 lr: 0.02\n","iteration: 197490 loss: 0.0126 lr: 0.02\n","iteration: 197500 loss: 0.0091 lr: 0.02\n","iteration: 197510 loss: 0.0123 lr: 0.02\n","iteration: 197520 loss: 0.0088 lr: 0.02\n","iteration: 197530 loss: 0.0070 lr: 0.02\n","iteration: 197540 loss: 0.0086 lr: 0.02\n","iteration: 197550 loss: 0.0093 lr: 0.02\n","iteration: 197560 loss: 0.0108 lr: 0.02\n","iteration: 197570 loss: 0.0106 lr: 0.02\n","iteration: 197580 loss: 0.0115 lr: 0.02\n","iteration: 197590 loss: 0.0124 lr: 0.02\n","iteration: 197600 loss: 0.0070 lr: 0.02\n","iteration: 197610 loss: 0.0074 lr: 0.02\n","iteration: 197620 loss: 0.0100 lr: 0.02\n","iteration: 197630 loss: 0.0071 lr: 0.02\n","iteration: 197640 loss: 0.0101 lr: 0.02\n","iteration: 197650 loss: 0.0089 lr: 0.02\n","iteration: 197660 loss: 0.0089 lr: 0.02\n","iteration: 197670 loss: 0.0083 lr: 0.02\n","iteration: 197680 loss: 0.0105 lr: 0.02\n","iteration: 197690 loss: 0.0094 lr: 0.02\n","iteration: 197700 loss: 0.0125 lr: 0.02\n","iteration: 197710 loss: 0.0077 lr: 0.02\n","iteration: 197720 loss: 0.0101 lr: 0.02\n","iteration: 197730 loss: 0.0122 lr: 0.02\n","iteration: 197740 loss: 0.0067 lr: 0.02\n","iteration: 197750 loss: 0.0079 lr: 0.02\n","iteration: 197760 loss: 0.0092 lr: 0.02\n","iteration: 197770 loss: 0.0104 lr: 0.02\n","iteration: 197780 loss: 0.0089 lr: 0.02\n","iteration: 197790 loss: 0.0078 lr: 0.02\n","iteration: 197800 loss: 0.0097 lr: 0.02\n","iteration: 197810 loss: 0.0094 lr: 0.02\n","iteration: 197820 loss: 0.0076 lr: 0.02\n","iteration: 197830 loss: 0.0078 lr: 0.02\n","iteration: 197840 loss: 0.0077 lr: 0.02\n","iteration: 197850 loss: 0.0068 lr: 0.02\n","iteration: 197860 loss: 0.0081 lr: 0.02\n","iteration: 197870 loss: 0.0074 lr: 0.02\n","iteration: 197880 loss: 0.0087 lr: 0.02\n","iteration: 197890 loss: 0.0110 lr: 0.02\n","iteration: 197900 loss: 0.0072 lr: 0.02\n","iteration: 197910 loss: 0.0094 lr: 0.02\n","iteration: 197920 loss: 0.0056 lr: 0.02\n","iteration: 197930 loss: 0.0062 lr: 0.02\n","iteration: 197940 loss: 0.0092 lr: 0.02\n","iteration: 197950 loss: 0.0075 lr: 0.02\n","iteration: 197960 loss: 0.0136 lr: 0.02\n","iteration: 197970 loss: 0.0081 lr: 0.02\n","iteration: 197980 loss: 0.0080 lr: 0.02\n","iteration: 197990 loss: 0.0154 lr: 0.02\n","iteration: 198000 loss: 0.0104 lr: 0.02\n","iteration: 198010 loss: 0.0093 lr: 0.02\n","iteration: 198020 loss: 0.0074 lr: 0.02\n","iteration: 198030 loss: 0.0107 lr: 0.02\n","iteration: 198040 loss: 0.0101 lr: 0.02\n","iteration: 198050 loss: 0.0109 lr: 0.02\n","iteration: 198060 loss: 0.0099 lr: 0.02\n","iteration: 198070 loss: 0.0101 lr: 0.02\n","iteration: 198080 loss: 0.0076 lr: 0.02\n","iteration: 198090 loss: 0.0104 lr: 0.02\n","iteration: 198100 loss: 0.0106 lr: 0.02\n","iteration: 198110 loss: 0.0097 lr: 0.02\n","iteration: 198120 loss: 0.0070 lr: 0.02\n","iteration: 198130 loss: 0.0091 lr: 0.02\n","iteration: 198140 loss: 0.0080 lr: 0.02\n","iteration: 198150 loss: 0.0100 lr: 0.02\n","iteration: 198160 loss: 0.0088 lr: 0.02\n","iteration: 198170 loss: 0.0100 lr: 0.02\n","iteration: 198180 loss: 0.0097 lr: 0.02\n","iteration: 198190 loss: 0.0071 lr: 0.02\n","iteration: 198200 loss: 0.0089 lr: 0.02\n","iteration: 198210 loss: 0.0060 lr: 0.02\n","iteration: 198220 loss: 0.0097 lr: 0.02\n","iteration: 198230 loss: 0.0072 lr: 0.02\n","iteration: 198240 loss: 0.0081 lr: 0.02\n","iteration: 198250 loss: 0.0086 lr: 0.02\n","iteration: 198260 loss: 0.0074 lr: 0.02\n","iteration: 198270 loss: 0.0084 lr: 0.02\n","iteration: 198280 loss: 0.0081 lr: 0.02\n","iteration: 198290 loss: 0.0096 lr: 0.02\n","iteration: 198300 loss: 0.0077 lr: 0.02\n","iteration: 198310 loss: 0.0088 lr: 0.02\n","iteration: 198320 loss: 0.0092 lr: 0.02\n","iteration: 198330 loss: 0.0097 lr: 0.02\n","iteration: 198340 loss: 0.0065 lr: 0.02\n","iteration: 198350 loss: 0.0077 lr: 0.02\n","iteration: 198360 loss: 0.0087 lr: 0.02\n","iteration: 198370 loss: 0.0095 lr: 0.02\n","iteration: 198380 loss: 0.0089 lr: 0.02\n","iteration: 198390 loss: 0.0108 lr: 0.02\n","iteration: 198400 loss: 0.0091 lr: 0.02\n","iteration: 198410 loss: 0.0079 lr: 0.02\n","iteration: 198420 loss: 0.0140 lr: 0.02\n","iteration: 198430 loss: 0.0126 lr: 0.02\n","iteration: 198440 loss: 0.0072 lr: 0.02\n","iteration: 198450 loss: 0.0092 lr: 0.02\n","iteration: 198460 loss: 0.0108 lr: 0.02\n","iteration: 198470 loss: 0.0084 lr: 0.02\n","iteration: 198480 loss: 0.0085 lr: 0.02\n","iteration: 198490 loss: 0.0098 lr: 0.02\n","iteration: 198500 loss: 0.0110 lr: 0.02\n","iteration: 198510 loss: 0.0109 lr: 0.02\n","iteration: 198520 loss: 0.0162 lr: 0.02\n","iteration: 198530 loss: 0.0104 lr: 0.02\n","iteration: 198540 loss: 0.0092 lr: 0.02\n","iteration: 198550 loss: 0.0122 lr: 0.02\n","iteration: 198560 loss: 0.0112 lr: 0.02\n","iteration: 198570 loss: 0.0113 lr: 0.02\n","iteration: 198580 loss: 0.0092 lr: 0.02\n","iteration: 198590 loss: 0.0068 lr: 0.02\n","iteration: 198600 loss: 0.0091 lr: 0.02\n","iteration: 198610 loss: 0.0101 lr: 0.02\n","iteration: 198620 loss: 0.0097 lr: 0.02\n","iteration: 198630 loss: 0.0113 lr: 0.02\n","iteration: 198640 loss: 0.0079 lr: 0.02\n","iteration: 198650 loss: 0.0093 lr: 0.02\n","iteration: 198660 loss: 0.0077 lr: 0.02\n","iteration: 198670 loss: 0.0089 lr: 0.02\n","iteration: 198680 loss: 0.0073 lr: 0.02\n","iteration: 198690 loss: 0.0102 lr: 0.02\n","iteration: 198700 loss: 0.0092 lr: 0.02\n","iteration: 198710 loss: 0.0118 lr: 0.02\n","iteration: 198720 loss: 0.0092 lr: 0.02\n","iteration: 198730 loss: 0.0093 lr: 0.02\n","iteration: 198740 loss: 0.0090 lr: 0.02\n","iteration: 198750 loss: 0.0054 lr: 0.02\n","iteration: 198760 loss: 0.0108 lr: 0.02\n","iteration: 198770 loss: 0.0118 lr: 0.02\n","iteration: 198780 loss: 0.0074 lr: 0.02\n","iteration: 198790 loss: 0.0080 lr: 0.02\n","iteration: 198800 loss: 0.0076 lr: 0.02\n","iteration: 198810 loss: 0.0091 lr: 0.02\n","iteration: 198820 loss: 0.0088 lr: 0.02\n","iteration: 198830 loss: 0.0100 lr: 0.02\n","iteration: 198840 loss: 0.0088 lr: 0.02\n","iteration: 198850 loss: 0.0077 lr: 0.02\n","iteration: 198860 loss: 0.0117 lr: 0.02\n","iteration: 198870 loss: 0.0101 lr: 0.02\n","iteration: 198880 loss: 0.0095 lr: 0.02\n","iteration: 198890 loss: 0.0082 lr: 0.02\n","iteration: 198900 loss: 0.0070 lr: 0.02\n","iteration: 198910 loss: 0.0068 lr: 0.02\n","iteration: 198920 loss: 0.0080 lr: 0.02\n","iteration: 198930 loss: 0.0090 lr: 0.02\n","iteration: 198940 loss: 0.0119 lr: 0.02\n","iteration: 198950 loss: 0.0082 lr: 0.02\n","iteration: 198960 loss: 0.0084 lr: 0.02\n","iteration: 198970 loss: 0.0076 lr: 0.02\n","iteration: 198980 loss: 0.0063 lr: 0.02\n","iteration: 198990 loss: 0.0076 lr: 0.02\n","iteration: 199000 loss: 0.0104 lr: 0.02\n","iteration: 199010 loss: 0.0070 lr: 0.02\n","iteration: 199020 loss: 0.0097 lr: 0.02\n","iteration: 199030 loss: 0.0095 lr: 0.02\n","iteration: 199040 loss: 0.0069 lr: 0.02\n","iteration: 199050 loss: 0.0085 lr: 0.02\n","iteration: 199060 loss: 0.0099 lr: 0.02\n","iteration: 199070 loss: 0.0105 lr: 0.02\n","iteration: 199080 loss: 0.0063 lr: 0.02\n","iteration: 199090 loss: 0.0095 lr: 0.02\n","iteration: 199100 loss: 0.0053 lr: 0.02\n","iteration: 199110 loss: 0.0085 lr: 0.02\n","iteration: 199120 loss: 0.0121 lr: 0.02\n","iteration: 199130 loss: 0.0073 lr: 0.02\n","iteration: 199140 loss: 0.0084 lr: 0.02\n","iteration: 199150 loss: 0.0089 lr: 0.02\n","iteration: 199160 loss: 0.0091 lr: 0.02\n","iteration: 199170 loss: 0.0083 lr: 0.02\n","iteration: 199180 loss: 0.0098 lr: 0.02\n","iteration: 199190 loss: 0.0083 lr: 0.02\n","iteration: 199200 loss: 0.0117 lr: 0.02\n","iteration: 199210 loss: 0.0097 lr: 0.02\n","iteration: 199220 loss: 0.0093 lr: 0.02\n","iteration: 199230 loss: 0.0062 lr: 0.02\n","iteration: 199240 loss: 0.0131 lr: 0.02\n","iteration: 199250 loss: 0.0101 lr: 0.02\n","iteration: 199260 loss: 0.0087 lr: 0.02\n","iteration: 199270 loss: 0.0179 lr: 0.02\n","iteration: 199280 loss: 0.0089 lr: 0.02\n","iteration: 199290 loss: 0.0076 lr: 0.02\n","iteration: 199300 loss: 0.0092 lr: 0.02\n","iteration: 199310 loss: 0.0099 lr: 0.02\n","iteration: 199320 loss: 0.0101 lr: 0.02\n","iteration: 199330 loss: 0.0085 lr: 0.02\n","iteration: 199340 loss: 0.0081 lr: 0.02\n","iteration: 199350 loss: 0.0107 lr: 0.02\n","iteration: 199360 loss: 0.0100 lr: 0.02\n","iteration: 199370 loss: 0.0110 lr: 0.02\n","iteration: 199380 loss: 0.0144 lr: 0.02\n","iteration: 199390 loss: 0.0117 lr: 0.02\n","iteration: 199400 loss: 0.0083 lr: 0.02\n","iteration: 199410 loss: 0.0088 lr: 0.02\n","iteration: 199420 loss: 0.0096 lr: 0.02\n","iteration: 199430 loss: 0.0077 lr: 0.02\n","iteration: 199440 loss: 0.0077 lr: 0.02\n","iteration: 199450 loss: 0.0073 lr: 0.02\n","iteration: 199460 loss: 0.0087 lr: 0.02\n","iteration: 199470 loss: 0.0126 lr: 0.02\n","iteration: 199480 loss: 0.0128 lr: 0.02\n","iteration: 199490 loss: 0.0093 lr: 0.02\n","iteration: 199500 loss: 0.0091 lr: 0.02\n","iteration: 199510 loss: 0.0080 lr: 0.02\n","iteration: 199520 loss: 0.0075 lr: 0.02\n","iteration: 199530 loss: 0.0087 lr: 0.02\n","iteration: 199540 loss: 0.0107 lr: 0.02\n","iteration: 199550 loss: 0.0105 lr: 0.02\n","iteration: 199560 loss: 0.0113 lr: 0.02\n","iteration: 199570 loss: 0.0077 lr: 0.02\n","iteration: 199580 loss: 0.0073 lr: 0.02\n","iteration: 199590 loss: 0.0074 lr: 0.02\n","iteration: 199600 loss: 0.0072 lr: 0.02\n","iteration: 199610 loss: 0.0117 lr: 0.02\n","iteration: 199620 loss: 0.0077 lr: 0.02\n","iteration: 199630 loss: 0.0087 lr: 0.02\n","iteration: 199640 loss: 0.0113 lr: 0.02\n","iteration: 199650 loss: 0.0077 lr: 0.02\n","iteration: 199660 loss: 0.0128 lr: 0.02\n","iteration: 199670 loss: 0.0139 lr: 0.02\n","iteration: 199680 loss: 0.0125 lr: 0.02\n","iteration: 199690 loss: 0.0119 lr: 0.02\n","iteration: 199700 loss: 0.0091 lr: 0.02\n","iteration: 199710 loss: 0.0118 lr: 0.02\n","iteration: 199720 loss: 0.0107 lr: 0.02\n","iteration: 199730 loss: 0.0098 lr: 0.02\n","iteration: 199740 loss: 0.0087 lr: 0.02\n","iteration: 199750 loss: 0.0064 lr: 0.02\n","iteration: 199760 loss: 0.0123 lr: 0.02\n","iteration: 199770 loss: 0.0119 lr: 0.02\n","iteration: 199780 loss: 0.0082 lr: 0.02\n","iteration: 199790 loss: 0.0119 lr: 0.02\n","iteration: 199800 loss: 0.0098 lr: 0.02\n","iteration: 199810 loss: 0.0109 lr: 0.02\n","iteration: 199820 loss: 0.0111 lr: 0.02\n","iteration: 199830 loss: 0.0105 lr: 0.02\n","iteration: 199840 loss: 0.0097 lr: 0.02\n","iteration: 199850 loss: 0.0079 lr: 0.02\n","iteration: 199860 loss: 0.0083 lr: 0.02\n","iteration: 199870 loss: 0.0102 lr: 0.02\n","iteration: 199880 loss: 0.0110 lr: 0.02\n","iteration: 199890 loss: 0.0109 lr: 0.02\n","iteration: 199900 loss: 0.0113 lr: 0.02\n","iteration: 199910 loss: 0.0090 lr: 0.02\n","iteration: 199920 loss: 0.0082 lr: 0.02\n","iteration: 199930 loss: 0.0121 lr: 0.02\n","iteration: 199940 loss: 0.0081 lr: 0.02\n","iteration: 199950 loss: 0.0101 lr: 0.02\n","iteration: 199960 loss: 0.0118 lr: 0.02\n","iteration: 199970 loss: 0.0084 lr: 0.02\n","iteration: 199980 loss: 0.0092 lr: 0.02\n","iteration: 199990 loss: 0.0106 lr: 0.02\n","iteration: 200000 loss: 0.0089 lr: 0.02\n","iteration: 200010 loss: 0.0096 lr: 0.02\n","iteration: 200020 loss: 0.0083 lr: 0.02\n","iteration: 200030 loss: 0.0080 lr: 0.02\n","iteration: 200040 loss: 0.0104 lr: 0.02\n","iteration: 200050 loss: 0.0065 lr: 0.02\n","iteration: 200060 loss: 0.0090 lr: 0.02\n","iteration: 200070 loss: 0.0095 lr: 0.02\n","iteration: 200080 loss: 0.0079 lr: 0.02\n","iteration: 200090 loss: 0.0094 lr: 0.02\n","iteration: 200100 loss: 0.0113 lr: 0.02\n","iteration: 200110 loss: 0.0088 lr: 0.02\n","iteration: 200120 loss: 0.0122 lr: 0.02\n","iteration: 200130 loss: 0.0097 lr: 0.02\n","iteration: 200140 loss: 0.0062 lr: 0.02\n","iteration: 200150 loss: 0.0119 lr: 0.02\n","iteration: 200160 loss: 0.0085 lr: 0.02\n","iteration: 200170 loss: 0.0115 lr: 0.02\n","iteration: 200180 loss: 0.0083 lr: 0.02\n","iteration: 200190 loss: 0.0100 lr: 0.02\n","iteration: 200200 loss: 0.0123 lr: 0.02\n","iteration: 200210 loss: 0.0101 lr: 0.02\n","iteration: 200220 loss: 0.0104 lr: 0.02\n","iteration: 200230 loss: 0.0097 lr: 0.02\n","iteration: 200240 loss: 0.0102 lr: 0.02\n","iteration: 200250 loss: 0.0088 lr: 0.02\n","iteration: 200260 loss: 0.0093 lr: 0.02\n","iteration: 200270 loss: 0.0094 lr: 0.02\n","iteration: 200280 loss: 0.0084 lr: 0.02\n","iteration: 200290 loss: 0.0089 lr: 0.02\n","iteration: 200300 loss: 0.0090 lr: 0.02\n","iteration: 200310 loss: 0.0107 lr: 0.02\n","iteration: 200320 loss: 0.0087 lr: 0.02\n","iteration: 200330 loss: 0.0105 lr: 0.02\n","iteration: 200340 loss: 0.0101 lr: 0.02\n","iteration: 200350 loss: 0.0057 lr: 0.02\n","iteration: 200360 loss: 0.0081 lr: 0.02\n","iteration: 200370 loss: 0.0108 lr: 0.02\n","iteration: 200380 loss: 0.0098 lr: 0.02\n","iteration: 200390 loss: 0.0082 lr: 0.02\n","iteration: 200400 loss: 0.0097 lr: 0.02\n","iteration: 200410 loss: 0.0103 lr: 0.02\n","iteration: 200420 loss: 0.0095 lr: 0.02\n","iteration: 200430 loss: 0.0088 lr: 0.02\n","iteration: 200440 loss: 0.0114 lr: 0.02\n","iteration: 200450 loss: 0.0109 lr: 0.02\n","iteration: 200460 loss: 0.0110 lr: 0.02\n","iteration: 200470 loss: 0.0099 lr: 0.02\n","iteration: 200480 loss: 0.0085 lr: 0.02\n","iteration: 200490 loss: 0.0083 lr: 0.02\n","iteration: 200500 loss: 0.0093 lr: 0.02\n","iteration: 200510 loss: 0.0084 lr: 0.02\n","iteration: 200520 loss: 0.0105 lr: 0.02\n","iteration: 200530 loss: 0.0121 lr: 0.02\n","iteration: 200540 loss: 0.0093 lr: 0.02\n","iteration: 200550 loss: 0.0080 lr: 0.02\n","iteration: 200560 loss: 0.0110 lr: 0.02\n","iteration: 200570 loss: 0.0141 lr: 0.02\n","iteration: 200580 loss: 0.0093 lr: 0.02\n","iteration: 200590 loss: 0.0091 lr: 0.02\n","iteration: 200600 loss: 0.0081 lr: 0.02\n","iteration: 200610 loss: 0.0090 lr: 0.02\n","iteration: 200620 loss: 0.0157 lr: 0.02\n","iteration: 200630 loss: 0.0073 lr: 0.02\n","iteration: 200640 loss: 0.0108 lr: 0.02\n","iteration: 200650 loss: 0.0053 lr: 0.02\n","iteration: 200660 loss: 0.0107 lr: 0.02\n","iteration: 200670 loss: 0.0072 lr: 0.02\n","iteration: 200680 loss: 0.0093 lr: 0.02\n","iteration: 200690 loss: 0.0112 lr: 0.02\n","iteration: 200700 loss: 0.0085 lr: 0.02\n","iteration: 200710 loss: 0.0106 lr: 0.02\n","iteration: 200720 loss: 0.0104 lr: 0.02\n","iteration: 200730 loss: 0.0106 lr: 0.02\n","iteration: 200740 loss: 0.0098 lr: 0.02\n","iteration: 200750 loss: 0.0082 lr: 0.02\n","iteration: 200760 loss: 0.0073 lr: 0.02\n","iteration: 200770 loss: 0.0053 lr: 0.02\n","iteration: 200780 loss: 0.0103 lr: 0.02\n","iteration: 200790 loss: 0.0088 lr: 0.02\n","iteration: 200800 loss: 0.0072 lr: 0.02\n","iteration: 200810 loss: 0.0101 lr: 0.02\n","iteration: 200820 loss: 0.0085 lr: 0.02\n","iteration: 200830 loss: 0.0063 lr: 0.02\n","iteration: 200840 loss: 0.0109 lr: 0.02\n","iteration: 200850 loss: 0.0110 lr: 0.02\n","iteration: 200860 loss: 0.0067 lr: 0.02\n","iteration: 200870 loss: 0.0083 lr: 0.02\n","iteration: 200880 loss: 0.0108 lr: 0.02\n","iteration: 200890 loss: 0.0116 lr: 0.02\n","iteration: 200900 loss: 0.0059 lr: 0.02\n","iteration: 200910 loss: 0.0077 lr: 0.02\n","iteration: 200920 loss: 0.0089 lr: 0.02\n","iteration: 200930 loss: 0.0090 lr: 0.02\n","iteration: 200940 loss: 0.0125 lr: 0.02\n","iteration: 200950 loss: 0.0102 lr: 0.02\n","iteration: 200960 loss: 0.0080 lr: 0.02\n","iteration: 200970 loss: 0.0093 lr: 0.02\n","iteration: 200980 loss: 0.0094 lr: 0.02\n","iteration: 200990 loss: 0.0105 lr: 0.02\n","iteration: 201000 loss: 0.0101 lr: 0.02\n","iteration: 201010 loss: 0.0087 lr: 0.02\n","iteration: 201020 loss: 0.0129 lr: 0.02\n","iteration: 201030 loss: 0.0086 lr: 0.02\n","iteration: 201040 loss: 0.0119 lr: 0.02\n","iteration: 201050 loss: 0.0079 lr: 0.02\n","iteration: 201060 loss: 0.0089 lr: 0.02\n","iteration: 201070 loss: 0.0098 lr: 0.02\n","iteration: 201080 loss: 0.0118 lr: 0.02\n","iteration: 201090 loss: 0.0087 lr: 0.02\n","iteration: 201100 loss: 0.0096 lr: 0.02\n","iteration: 201110 loss: 0.0100 lr: 0.02\n","iteration: 201120 loss: 0.0074 lr: 0.02\n","iteration: 201130 loss: 0.0137 lr: 0.02\n","iteration: 201140 loss: 0.0145 lr: 0.02\n","iteration: 201150 loss: 0.0125 lr: 0.02\n","iteration: 201160 loss: 0.0110 lr: 0.02\n","iteration: 201170 loss: 0.0102 lr: 0.02\n","iteration: 201180 loss: 0.0091 lr: 0.02\n","iteration: 201190 loss: 0.0100 lr: 0.02\n","iteration: 201200 loss: 0.0069 lr: 0.02\n","iteration: 201210 loss: 0.0148 lr: 0.02\n","iteration: 201220 loss: 0.0079 lr: 0.02\n","iteration: 201230 loss: 0.0054 lr: 0.02\n","iteration: 201240 loss: 0.0090 lr: 0.02\n","iteration: 201250 loss: 0.0116 lr: 0.02\n","iteration: 201260 loss: 0.0128 lr: 0.02\n","iteration: 201270 loss: 0.0073 lr: 0.02\n","iteration: 201280 loss: 0.0083 lr: 0.02\n","iteration: 201290 loss: 0.0089 lr: 0.02\n","iteration: 201300 loss: 0.0063 lr: 0.02\n","iteration: 201310 loss: 0.0072 lr: 0.02\n","iteration: 201320 loss: 0.0102 lr: 0.02\n","iteration: 201330 loss: 0.0078 lr: 0.02\n","iteration: 201340 loss: 0.0077 lr: 0.02\n","iteration: 201350 loss: 0.0114 lr: 0.02\n","iteration: 201360 loss: 0.0077 lr: 0.02\n","iteration: 201370 loss: 0.0123 lr: 0.02\n","iteration: 201380 loss: 0.0079 lr: 0.02\n","iteration: 201390 loss: 0.0099 lr: 0.02\n","iteration: 201400 loss: 0.0096 lr: 0.02\n","iteration: 201410 loss: 0.0102 lr: 0.02\n","iteration: 201420 loss: 0.0085 lr: 0.02\n","iteration: 201430 loss: 0.0083 lr: 0.02\n","iteration: 201440 loss: 0.0090 lr: 0.02\n","iteration: 201450 loss: 0.0099 lr: 0.02\n","iteration: 201460 loss: 0.0102 lr: 0.02\n","iteration: 201470 loss: 0.0087 lr: 0.02\n","iteration: 201480 loss: 0.0092 lr: 0.02\n","iteration: 201490 loss: 0.0098 lr: 0.02\n","iteration: 201500 loss: 0.0086 lr: 0.02\n","iteration: 201510 loss: 0.0073 lr: 0.02\n","iteration: 201520 loss: 0.0084 lr: 0.02\n","iteration: 201530 loss: 0.0065 lr: 0.02\n","iteration: 201540 loss: 0.0089 lr: 0.02\n","iteration: 201550 loss: 0.0126 lr: 0.02\n","iteration: 201560 loss: 0.0091 lr: 0.02\n","iteration: 201570 loss: 0.0075 lr: 0.02\n","iteration: 201580 loss: 0.0111 lr: 0.02\n","iteration: 201590 loss: 0.0112 lr: 0.02\n","iteration: 201600 loss: 0.0105 lr: 0.02\n","iteration: 201610 loss: 0.0115 lr: 0.02\n","iteration: 201620 loss: 0.0103 lr: 0.02\n","iteration: 201630 loss: 0.0070 lr: 0.02\n","iteration: 201640 loss: 0.0107 lr: 0.02\n","iteration: 201650 loss: 0.0081 lr: 0.02\n","iteration: 201660 loss: 0.0109 lr: 0.02\n","iteration: 201670 loss: 0.0094 lr: 0.02\n","iteration: 201680 loss: 0.0079 lr: 0.02\n","iteration: 201690 loss: 0.0068 lr: 0.02\n","iteration: 201700 loss: 0.0093 lr: 0.02\n","iteration: 201710 loss: 0.0069 lr: 0.02\n","iteration: 201720 loss: 0.0163 lr: 0.02\n","iteration: 201730 loss: 0.0100 lr: 0.02\n","iteration: 201740 loss: 0.0101 lr: 0.02\n","iteration: 201750 loss: 0.0097 lr: 0.02\n","iteration: 201760 loss: 0.0082 lr: 0.02\n","iteration: 201770 loss: 0.0074 lr: 0.02\n","iteration: 201780 loss: 0.0107 lr: 0.02\n","iteration: 201790 loss: 0.0087 lr: 0.02\n","iteration: 201800 loss: 0.0077 lr: 0.02\n","iteration: 201810 loss: 0.0083 lr: 0.02\n","iteration: 201820 loss: 0.0082 lr: 0.02\n","iteration: 201830 loss: 0.0107 lr: 0.02\n","iteration: 201840 loss: 0.0092 lr: 0.02\n","iteration: 201850 loss: 0.0119 lr: 0.02\n","iteration: 201860 loss: 0.0094 lr: 0.02\n","iteration: 201870 loss: 0.0100 lr: 0.02\n","iteration: 201880 loss: 0.0127 lr: 0.02\n","iteration: 201890 loss: 0.0130 lr: 0.02\n","iteration: 201900 loss: 0.0082 lr: 0.02\n","iteration: 201910 loss: 0.0093 lr: 0.02\n","iteration: 201920 loss: 0.0076 lr: 0.02\n","iteration: 201930 loss: 0.0072 lr: 0.02\n","iteration: 201940 loss: 0.0077 lr: 0.02\n","iteration: 201950 loss: 0.0095 lr: 0.02\n","iteration: 201960 loss: 0.0080 lr: 0.02\n","iteration: 201970 loss: 0.0097 lr: 0.02\n","iteration: 201980 loss: 0.0085 lr: 0.02\n","iteration: 201990 loss: 0.0094 lr: 0.02\n","iteration: 202000 loss: 0.0092 lr: 0.02\n","iteration: 202010 loss: 0.0070 lr: 0.02\n","iteration: 202020 loss: 0.0093 lr: 0.02\n","iteration: 202030 loss: 0.0103 lr: 0.02\n","iteration: 202040 loss: 0.0077 lr: 0.02\n","iteration: 202050 loss: 0.0124 lr: 0.02\n","iteration: 202060 loss: 0.0060 lr: 0.02\n","iteration: 202070 loss: 0.0094 lr: 0.02\n","iteration: 202080 loss: 0.0102 lr: 0.02\n","iteration: 202090 loss: 0.0109 lr: 0.02\n","iteration: 202100 loss: 0.0088 lr: 0.02\n","iteration: 202110 loss: 0.0101 lr: 0.02\n","iteration: 202120 loss: 0.0083 lr: 0.02\n","iteration: 202130 loss: 0.0096 lr: 0.02\n","iteration: 202140 loss: 0.0117 lr: 0.02\n","iteration: 202150 loss: 0.0096 lr: 0.02\n","iteration: 202160 loss: 0.0103 lr: 0.02\n","iteration: 202170 loss: 0.0080 lr: 0.02\n","iteration: 202180 loss: 0.0084 lr: 0.02\n","iteration: 202190 loss: 0.0103 lr: 0.02\n","iteration: 202200 loss: 0.0087 lr: 0.02\n","iteration: 202210 loss: 0.0080 lr: 0.02\n","iteration: 202220 loss: 0.0080 lr: 0.02\n","iteration: 202230 loss: 0.0083 lr: 0.02\n","iteration: 202240 loss: 0.0063 lr: 0.02\n","iteration: 202250 loss: 0.0078 lr: 0.02\n","iteration: 202260 loss: 0.0097 lr: 0.02\n","iteration: 202270 loss: 0.0106 lr: 0.02\n","iteration: 202280 loss: 0.0141 lr: 0.02\n","iteration: 202290 loss: 0.0094 lr: 0.02\n","iteration: 202300 loss: 0.0088 lr: 0.02\n","iteration: 202310 loss: 0.0088 lr: 0.02\n","iteration: 202320 loss: 0.0088 lr: 0.02\n","iteration: 202330 loss: 0.0072 lr: 0.02\n","iteration: 202340 loss: 0.0081 lr: 0.02\n","iteration: 202350 loss: 0.0077 lr: 0.02\n","iteration: 202360 loss: 0.0077 lr: 0.02\n","iteration: 202370 loss: 0.0100 lr: 0.02\n","iteration: 202380 loss: 0.0077 lr: 0.02\n","iteration: 202390 loss: 0.0114 lr: 0.02\n","iteration: 202400 loss: 0.0096 lr: 0.02\n","iteration: 202410 loss: 0.0112 lr: 0.02\n","iteration: 202420 loss: 0.0104 lr: 0.02\n","iteration: 202430 loss: 0.0091 lr: 0.02\n","iteration: 202440 loss: 0.0107 lr: 0.02\n","iteration: 202450 loss: 0.0102 lr: 0.02\n","iteration: 202460 loss: 0.0111 lr: 0.02\n","iteration: 202470 loss: 0.0108 lr: 0.02\n","iteration: 202480 loss: 0.0062 lr: 0.02\n","iteration: 202490 loss: 0.0092 lr: 0.02\n","iteration: 202500 loss: 0.0078 lr: 0.02\n","iteration: 202510 loss: 0.0084 lr: 0.02\n","iteration: 202520 loss: 0.0099 lr: 0.02\n","iteration: 202530 loss: 0.0086 lr: 0.02\n","iteration: 202540 loss: 0.0086 lr: 0.02\n","iteration: 202550 loss: 0.0088 lr: 0.02\n","iteration: 202560 loss: 0.0103 lr: 0.02\n","iteration: 202570 loss: 0.0089 lr: 0.02\n","iteration: 202580 loss: 0.0075 lr: 0.02\n","iteration: 202590 loss: 0.0094 lr: 0.02\n","iteration: 202600 loss: 0.0148 lr: 0.02\n","iteration: 202610 loss: 0.0099 lr: 0.02\n","iteration: 202620 loss: 0.0116 lr: 0.02\n","iteration: 202630 loss: 0.0086 lr: 0.02\n","iteration: 202640 loss: 0.0104 lr: 0.02\n","iteration: 202650 loss: 0.0090 lr: 0.02\n","iteration: 202660 loss: 0.0098 lr: 0.02\n","iteration: 202670 loss: 0.0078 lr: 0.02\n","iteration: 202680 loss: 0.0101 lr: 0.02\n","iteration: 202690 loss: 0.0083 lr: 0.02\n","iteration: 202700 loss: 0.0089 lr: 0.02\n","iteration: 202710 loss: 0.0077 lr: 0.02\n","iteration: 202720 loss: 0.0076 lr: 0.02\n","iteration: 202730 loss: 0.0097 lr: 0.02\n","iteration: 202740 loss: 0.0105 lr: 0.02\n","iteration: 202750 loss: 0.0077 lr: 0.02\n","iteration: 202760 loss: 0.0070 lr: 0.02\n","iteration: 202770 loss: 0.0066 lr: 0.02\n","iteration: 202780 loss: 0.0130 lr: 0.02\n","iteration: 202790 loss: 0.0094 lr: 0.02\n","iteration: 202800 loss: 0.0082 lr: 0.02\n","iteration: 202810 loss: 0.0112 lr: 0.02\n","iteration: 202820 loss: 0.0084 lr: 0.02\n","iteration: 202830 loss: 0.0103 lr: 0.02\n","iteration: 202840 loss: 0.0094 lr: 0.02\n","iteration: 202850 loss: 0.0091 lr: 0.02\n","iteration: 202860 loss: 0.0089 lr: 0.02\n","iteration: 202870 loss: 0.0083 lr: 0.02\n","iteration: 202880 loss: 0.0128 lr: 0.02\n","iteration: 202890 loss: 0.0071 lr: 0.02\n","iteration: 202900 loss: 0.0073 lr: 0.02\n","iteration: 202910 loss: 0.0099 lr: 0.02\n","iteration: 202920 loss: 0.0085 lr: 0.02\n","iteration: 202930 loss: 0.0079 lr: 0.02\n","iteration: 202940 loss: 0.0135 lr: 0.02\n","iteration: 202950 loss: 0.0098 lr: 0.02\n","iteration: 202960 loss: 0.0088 lr: 0.02\n","iteration: 202970 loss: 0.0121 lr: 0.02\n","iteration: 202980 loss: 0.0084 lr: 0.02\n","iteration: 202990 loss: 0.0112 lr: 0.02\n","iteration: 203000 loss: 0.0098 lr: 0.02\n","iteration: 203010 loss: 0.0094 lr: 0.02\n","iteration: 203020 loss: 0.0074 lr: 0.02\n","iteration: 203030 loss: 0.0102 lr: 0.02\n","iteration: 203040 loss: 0.0120 lr: 0.02\n","iteration: 203050 loss: 0.0104 lr: 0.02\n","iteration: 203060 loss: 0.0088 lr: 0.02\n","iteration: 203070 loss: 0.0074 lr: 0.02\n","iteration: 203080 loss: 0.0094 lr: 0.02\n","iteration: 203090 loss: 0.0093 lr: 0.02\n","iteration: 203100 loss: 0.0087 lr: 0.02\n","iteration: 203110 loss: 0.0081 lr: 0.02\n","iteration: 203120 loss: 0.0087 lr: 0.02\n","iteration: 203130 loss: 0.0093 lr: 0.02\n","iteration: 203140 loss: 0.0100 lr: 0.02\n","iteration: 203150 loss: 0.0076 lr: 0.02\n","iteration: 203160 loss: 0.0074 lr: 0.02\n","iteration: 203170 loss: 0.0088 lr: 0.02\n","iteration: 203180 loss: 0.0092 lr: 0.02\n","iteration: 203190 loss: 0.0089 lr: 0.02\n","iteration: 203200 loss: 0.0089 lr: 0.02\n","iteration: 203210 loss: 0.0086 lr: 0.02\n","iteration: 203220 loss: 0.0078 lr: 0.02\n","iteration: 203230 loss: 0.0096 lr: 0.02\n","iteration: 203240 loss: 0.0087 lr: 0.02\n","iteration: 203250 loss: 0.0102 lr: 0.02\n","iteration: 203260 loss: 0.0085 lr: 0.02\n","iteration: 203270 loss: 0.0109 lr: 0.02\n","iteration: 203280 loss: 0.0141 lr: 0.02\n","iteration: 203290 loss: 0.0111 lr: 0.02\n","iteration: 203300 loss: 0.0073 lr: 0.02\n","iteration: 203310 loss: 0.0084 lr: 0.02\n","iteration: 203320 loss: 0.0138 lr: 0.02\n","iteration: 203330 loss: 0.0099 lr: 0.02\n","iteration: 203340 loss: 0.0070 lr: 0.02\n","iteration: 203350 loss: 0.0105 lr: 0.02\n","iteration: 203360 loss: 0.0091 lr: 0.02\n","iteration: 203370 loss: 0.0076 lr: 0.02\n","iteration: 203380 loss: 0.0101 lr: 0.02\n","iteration: 203390 loss: 0.0089 lr: 0.02\n","iteration: 203400 loss: 0.0077 lr: 0.02\n","iteration: 203410 loss: 0.0085 lr: 0.02\n","iteration: 203420 loss: 0.0088 lr: 0.02\n","iteration: 203430 loss: 0.0094 lr: 0.02\n","iteration: 203440 loss: 0.0093 lr: 0.02\n","iteration: 203450 loss: 0.0089 lr: 0.02\n","iteration: 203460 loss: 0.0087 lr: 0.02\n","iteration: 203470 loss: 0.0066 lr: 0.02\n","iteration: 203480 loss: 0.0074 lr: 0.02\n","iteration: 203490 loss: 0.0094 lr: 0.02\n","iteration: 203500 loss: 0.0115 lr: 0.02\n","iteration: 203510 loss: 0.0099 lr: 0.02\n","iteration: 203520 loss: 0.0109 lr: 0.02\n","iteration: 203530 loss: 0.0088 lr: 0.02\n","iteration: 203540 loss: 0.0095 lr: 0.02\n","iteration: 203550 loss: 0.0097 lr: 0.02\n","iteration: 203560 loss: 0.0077 lr: 0.02\n","iteration: 203570 loss: 0.0113 lr: 0.02\n","iteration: 203580 loss: 0.0120 lr: 0.02\n","iteration: 203590 loss: 0.0064 lr: 0.02\n","iteration: 203600 loss: 0.0074 lr: 0.02\n","iteration: 203610 loss: 0.0079 lr: 0.02\n","iteration: 203620 loss: 0.0078 lr: 0.02\n","iteration: 203630 loss: 0.0068 lr: 0.02\n","iteration: 203640 loss: 0.0099 lr: 0.02\n","iteration: 203650 loss: 0.0092 lr: 0.02\n","iteration: 203660 loss: 0.0119 lr: 0.02\n","iteration: 203670 loss: 0.0087 lr: 0.02\n","iteration: 203680 loss: 0.0067 lr: 0.02\n","iteration: 203690 loss: 0.0090 lr: 0.02\n","iteration: 203700 loss: 0.0083 lr: 0.02\n","iteration: 203710 loss: 0.0067 lr: 0.02\n","iteration: 203720 loss: 0.0078 lr: 0.02\n","iteration: 203730 loss: 0.0060 lr: 0.02\n","iteration: 203740 loss: 0.0124 lr: 0.02\n","iteration: 203750 loss: 0.0097 lr: 0.02\n","iteration: 203760 loss: 0.0070 lr: 0.02\n","iteration: 203770 loss: 0.0064 lr: 0.02\n","iteration: 203780 loss: 0.0091 lr: 0.02\n","iteration: 203790 loss: 0.0085 lr: 0.02\n","iteration: 203800 loss: 0.0103 lr: 0.02\n","iteration: 203810 loss: 0.0082 lr: 0.02\n","iteration: 203820 loss: 0.0079 lr: 0.02\n","iteration: 203830 loss: 0.0068 lr: 0.02\n","iteration: 203840 loss: 0.0080 lr: 0.02\n","iteration: 203850 loss: 0.0085 lr: 0.02\n","iteration: 203860 loss: 0.0104 lr: 0.02\n","iteration: 203870 loss: 0.0076 lr: 0.02\n","iteration: 203880 loss: 0.0108 lr: 0.02\n","iteration: 203890 loss: 0.0066 lr: 0.02\n","iteration: 203900 loss: 0.0081 lr: 0.02\n","iteration: 203910 loss: 0.0090 lr: 0.02\n","iteration: 203920 loss: 0.0089 lr: 0.02\n","iteration: 203930 loss: 0.0077 lr: 0.02\n","iteration: 203940 loss: 0.0085 lr: 0.02\n","iteration: 203950 loss: 0.0102 lr: 0.02\n","iteration: 203960 loss: 0.0089 lr: 0.02\n","iteration: 203970 loss: 0.0087 lr: 0.02\n","iteration: 203980 loss: 0.0159 lr: 0.02\n","iteration: 203990 loss: 0.0092 lr: 0.02\n","iteration: 204000 loss: 0.0063 lr: 0.02\n","iteration: 204010 loss: 0.0084 lr: 0.02\n","iteration: 204020 loss: 0.0066 lr: 0.02\n","iteration: 204030 loss: 0.0094 lr: 0.02\n","iteration: 204040 loss: 0.0102 lr: 0.02\n","iteration: 204050 loss: 0.0086 lr: 0.02\n","iteration: 204060 loss: 0.0078 lr: 0.02\n","iteration: 204070 loss: 0.0106 lr: 0.02\n","iteration: 204080 loss: 0.0081 lr: 0.02\n","iteration: 204090 loss: 0.0094 lr: 0.02\n","iteration: 204100 loss: 0.0089 lr: 0.02\n","iteration: 204110 loss: 0.0080 lr: 0.02\n","iteration: 204120 loss: 0.0105 lr: 0.02\n","iteration: 204130 loss: 0.0056 lr: 0.02\n","iteration: 204140 loss: 0.0076 lr: 0.02\n","iteration: 204150 loss: 0.0110 lr: 0.02\n","iteration: 204160 loss: 0.0101 lr: 0.02\n","iteration: 204170 loss: 0.0096 lr: 0.02\n","iteration: 204180 loss: 0.0096 lr: 0.02\n","iteration: 204190 loss: 0.0061 lr: 0.02\n","iteration: 204200 loss: 0.0094 lr: 0.02\n","iteration: 204210 loss: 0.0117 lr: 0.02\n","iteration: 204220 loss: 0.0100 lr: 0.02\n","iteration: 204230 loss: 0.0084 lr: 0.02\n","iteration: 204240 loss: 0.0099 lr: 0.02\n","iteration: 204250 loss: 0.0115 lr: 0.02\n","iteration: 204260 loss: 0.0076 lr: 0.02\n","iteration: 204270 loss: 0.0139 lr: 0.02\n","iteration: 204280 loss: 0.0081 lr: 0.02\n","iteration: 204290 loss: 0.0055 lr: 0.02\n","iteration: 204300 loss: 0.0077 lr: 0.02\n","iteration: 204310 loss: 0.0077 lr: 0.02\n","iteration: 204320 loss: 0.0087 lr: 0.02\n","iteration: 204330 loss: 0.0100 lr: 0.02\n","iteration: 204340 loss: 0.0092 lr: 0.02\n","iteration: 204350 loss: 0.0107 lr: 0.02\n","iteration: 204360 loss: 0.0087 lr: 0.02\n","iteration: 204370 loss: 0.0092 lr: 0.02\n","iteration: 204380 loss: 0.0068 lr: 0.02\n","iteration: 204390 loss: 0.0083 lr: 0.02\n","iteration: 204400 loss: 0.0091 lr: 0.02\n","iteration: 204410 loss: 0.0089 lr: 0.02\n","iteration: 204420 loss: 0.0081 lr: 0.02\n","iteration: 204430 loss: 0.0080 lr: 0.02\n","iteration: 204440 loss: 0.0064 lr: 0.02\n","iteration: 204450 loss: 0.0077 lr: 0.02\n","iteration: 204460 loss: 0.0079 lr: 0.02\n","iteration: 204470 loss: 0.0103 lr: 0.02\n","iteration: 204480 loss: 0.0084 lr: 0.02\n","iteration: 204490 loss: 0.0080 lr: 0.02\n","iteration: 204500 loss: 0.0140 lr: 0.02\n","iteration: 204510 loss: 0.0087 lr: 0.02\n","iteration: 204520 loss: 0.0096 lr: 0.02\n","iteration: 204530 loss: 0.0067 lr: 0.02\n","iteration: 204540 loss: 0.0081 lr: 0.02\n","iteration: 204550 loss: 0.0085 lr: 0.02\n","iteration: 204560 loss: 0.0147 lr: 0.02\n","iteration: 204570 loss: 0.0085 lr: 0.02\n","iteration: 204580 loss: 0.0102 lr: 0.02\n","iteration: 204590 loss: 0.0125 lr: 0.02\n","iteration: 204600 loss: 0.0066 lr: 0.02\n","iteration: 204610 loss: 0.0148 lr: 0.02\n","iteration: 204620 loss: 0.0091 lr: 0.02\n","iteration: 204630 loss: 0.0082 lr: 0.02\n","iteration: 204640 loss: 0.0079 lr: 0.02\n","iteration: 204650 loss: 0.0090 lr: 0.02\n","iteration: 204660 loss: 0.0112 lr: 0.02\n","iteration: 204670 loss: 0.0108 lr: 0.02\n","iteration: 204680 loss: 0.0117 lr: 0.02\n","iteration: 204690 loss: 0.0077 lr: 0.02\n","iteration: 204700 loss: 0.0085 lr: 0.02\n","iteration: 204710 loss: 0.0078 lr: 0.02\n","iteration: 204720 loss: 0.0093 lr: 0.02\n","iteration: 204730 loss: 0.0085 lr: 0.02\n","iteration: 204740 loss: 0.0108 lr: 0.02\n","iteration: 204750 loss: 0.0081 lr: 0.02\n","iteration: 204760 loss: 0.0082 lr: 0.02\n","iteration: 204770 loss: 0.0093 lr: 0.02\n","iteration: 204780 loss: 0.0083 lr: 0.02\n","iteration: 204790 loss: 0.0145 lr: 0.02\n","iteration: 204800 loss: 0.0080 lr: 0.02\n","iteration: 204810 loss: 0.0077 lr: 0.02\n","iteration: 204820 loss: 0.0073 lr: 0.02\n","iteration: 204830 loss: 0.0085 lr: 0.02\n","iteration: 204840 loss: 0.0083 lr: 0.02\n","iteration: 204850 loss: 0.0155 lr: 0.02\n","iteration: 204860 loss: 0.0114 lr: 0.02\n","iteration: 204870 loss: 0.0101 lr: 0.02\n","iteration: 204880 loss: 0.0115 lr: 0.02\n","iteration: 204890 loss: 0.0109 lr: 0.02\n","iteration: 204900 loss: 0.0069 lr: 0.02\n","iteration: 204910 loss: 0.0097 lr: 0.02\n","iteration: 204920 loss: 0.0076 lr: 0.02\n","iteration: 204930 loss: 0.0064 lr: 0.02\n","iteration: 204940 loss: 0.0131 lr: 0.02\n","iteration: 204950 loss: 0.0098 lr: 0.02\n","iteration: 204960 loss: 0.0094 lr: 0.02\n","iteration: 204970 loss: 0.0094 lr: 0.02\n","iteration: 204980 loss: 0.0104 lr: 0.02\n","iteration: 204990 loss: 0.0085 lr: 0.02\n","iteration: 205000 loss: 0.0097 lr: 0.02\n","iteration: 205010 loss: 0.0061 lr: 0.02\n","iteration: 205020 loss: 0.0096 lr: 0.02\n","iteration: 205030 loss: 0.0097 lr: 0.02\n","iteration: 205040 loss: 0.0077 lr: 0.02\n","iteration: 205050 loss: 0.0098 lr: 0.02\n","iteration: 205060 loss: 0.0077 lr: 0.02\n","iteration: 205070 loss: 0.0086 lr: 0.02\n","iteration: 205080 loss: 0.0091 lr: 0.02\n","iteration: 205090 loss: 0.0114 lr: 0.02\n","iteration: 205100 loss: 0.0085 lr: 0.02\n","iteration: 205110 loss: 0.0090 lr: 0.02\n","iteration: 205120 loss: 0.0108 lr: 0.02\n","iteration: 205130 loss: 0.0070 lr: 0.02\n","iteration: 205140 loss: 0.0094 lr: 0.02\n","iteration: 205150 loss: 0.0087 lr: 0.02\n","iteration: 205160 loss: 0.0080 lr: 0.02\n","iteration: 205170 loss: 0.0114 lr: 0.02\n","iteration: 205180 loss: 0.0092 lr: 0.02\n","iteration: 205190 loss: 0.0078 lr: 0.02\n","iteration: 205200 loss: 0.0099 lr: 0.02\n","iteration: 205210 loss: 0.0110 lr: 0.02\n","iteration: 205220 loss: 0.0076 lr: 0.02\n","iteration: 205230 loss: 0.0117 lr: 0.02\n","iteration: 205240 loss: 0.0070 lr: 0.02\n","iteration: 205250 loss: 0.0108 lr: 0.02\n","iteration: 205260 loss: 0.0099 lr: 0.02\n","iteration: 205270 loss: 0.0101 lr: 0.02\n","iteration: 205280 loss: 0.0092 lr: 0.02\n","iteration: 205290 loss: 0.0094 lr: 0.02\n","iteration: 205300 loss: 0.0087 lr: 0.02\n","iteration: 205310 loss: 0.0090 lr: 0.02\n","iteration: 205320 loss: 0.0072 lr: 0.02\n","iteration: 205330 loss: 0.0092 lr: 0.02\n","iteration: 205340 loss: 0.0094 lr: 0.02\n","iteration: 205350 loss: 0.0091 lr: 0.02\n","iteration: 205360 loss: 0.0106 lr: 0.02\n","iteration: 205370 loss: 0.0093 lr: 0.02\n","iteration: 205380 loss: 0.0129 lr: 0.02\n","iteration: 205390 loss: 0.0132 lr: 0.02\n","iteration: 205400 loss: 0.0103 lr: 0.02\n","iteration: 205410 loss: 0.0104 lr: 0.02\n","iteration: 205420 loss: 0.0065 lr: 0.02\n","iteration: 205430 loss: 0.0111 lr: 0.02\n","iteration: 205440 loss: 0.0102 lr: 0.02\n","iteration: 205450 loss: 0.0075 lr: 0.02\n","iteration: 205460 loss: 0.0073 lr: 0.02\n","iteration: 205470 loss: 0.0069 lr: 0.02\n","iteration: 205480 loss: 0.0071 lr: 0.02\n","iteration: 205490 loss: 0.0099 lr: 0.02\n","iteration: 205500 loss: 0.0094 lr: 0.02\n","iteration: 205510 loss: 0.0083 lr: 0.02\n","iteration: 205520 loss: 0.0095 lr: 0.02\n","iteration: 205530 loss: 0.0114 lr: 0.02\n","iteration: 205540 loss: 0.0091 lr: 0.02\n","iteration: 205550 loss: 0.0100 lr: 0.02\n","iteration: 205560 loss: 0.0093 lr: 0.02\n","iteration: 205570 loss: 0.0093 lr: 0.02\n","iteration: 205580 loss: 0.0093 lr: 0.02\n","iteration: 205590 loss: 0.0101 lr: 0.02\n","iteration: 205600 loss: 0.0110 lr: 0.02\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d200f964417d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#old == deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Selecting single-animal trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             train(\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mlr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         [_, loss_val, summary] = sess.run(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#let's also change the display and save_iters just in case Colab takes away the GPU...\n","#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n","#more info and there are more things you can set: https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\n","\n","#old == deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n","\n","deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n","\n","#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\n","#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."]},{"cell_type":"markdown","metadata":{"id":"RiDwIVf5-3H_"},"source":["**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"]},{"cell_type":"markdown","metadata":{"id":"xZygsb2DoEJc"},"source":["## Start evaluating:\n","This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n","and stores the results as .csv file in a subdirectory under **evaluation-results**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":984},"executionInfo":{"elapsed":42941,"status":"error","timestamp":1681936489188,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"nv4zlbrnoEJg","outputId":"94ac1e55-9579-48a0-d15b-2f2460bccb09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running  DLC_resnet50_HorsesMay8shuffle1_57500  with # of training iterations: 57500\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"name":"stdout","output_type":"stream","text":["Running evaluation ...\n"]},{"name":"stderr","output_type":"stream","text":["8114it [1:18:43,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-57500\n","Results for 57500  training iterations: 5 1 train error: 3.39 pixels. Test error: 4.31  pixels.\n","With pcutoff of 0.1  train error: 3.36 pixels. Test error: 4.28 pixels\n","Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n","Plotting...\n"]},{"data":{"application/javascript":["/* Put everything inside the global mpl namespace */\n","/* global mpl */\n","window.mpl = {};\n","\n","mpl.get_websocket_type = function () {\n","    if (typeof WebSocket !== 'undefined') {\n","        return WebSocket;\n","    } else if (typeof MozWebSocket !== 'undefined') {\n","        return MozWebSocket;\n","    } else {\n","        alert(\n","            'Your browser does not have WebSocket support. ' +\n","                'Please try Chrome, Safari or Firefox  6. ' +\n","                'Firefox 4 and 5 are also supported but you ' +\n","                'have to enable WebSockets in about:config.'\n","        );\n","    }\n","};\n","\n","mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n","    this.id = figure_id;\n","\n","    this.ws = websocket;\n","\n","    this.supports_binary = this.ws.binaryType !== undefined;\n","\n","    if (!this.supports_binary) {\n","        var warnings = document.getElementById('mpl-warnings');\n","        if (warnings) {\n","            warnings.style.display = 'block';\n","            warnings.textContent =\n","                'This browser does not support binary websocket messages. ' +\n","                'Performance may be slow.';\n","        }\n","    }\n","\n","    this.imageObj = new Image();\n","\n","    this.context = undefined;\n","    this.message = undefined;\n","    this.canvas = undefined;\n","    this.rubberband_canvas = undefined;\n","    this.rubberband_context = undefined;\n","    this.format_dropdown = undefined;\n","\n","    this.image_mode = 'full';\n","\n","    this.root = document.createElement('div');\n","    this.root.setAttribute('style', 'display: inline-block');\n","    this._root_extra_style(this.root);\n","\n","    parent_element.appendChild(this.root);\n","\n","    this._init_header(this);\n","    this._init_canvas(this);\n","    this._init_toolbar(this);\n","\n","    var fig = this;\n","\n","    this.waiting = false;\n","\n","    this.ws.onopen = function () {\n","        fig.send_message('supports_binary', { value: fig.supports_binary });\n","        fig.send_message('send_image_mode', {});\n","        if (fig.ratio !== 1) {\n","            fig.send_message('set_device_pixel_ratio', {\n","                device_pixel_ratio: fig.ratio,\n","            });\n","        }\n","        fig.send_message('refresh', {});\n","    };\n","\n","    this.imageObj.onload = function () {\n","        if (fig.image_mode === 'full') {\n","            // Full images could contain transparency (where diff images\n","            // almost always do), so we need to clear the canvas so that\n","            // there is no ghosting.\n","            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n","        }\n","        fig.context.drawImage(fig.imageObj, 0, 0);\n","    };\n","\n","    this.imageObj.onunload = function () {\n","        fig.ws.close();\n","    };\n","\n","    this.ws.onmessage = this._make_on_message_function(this);\n","\n","    this.ondownload = ondownload;\n","};\n","\n","mpl.figure.prototype._init_header = function () {\n","    var titlebar = document.createElement('div');\n","    titlebar.classList =\n","        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n","    var titletext = document.createElement('div');\n","    titletext.classList = 'ui-dialog-title';\n","    titletext.setAttribute(\n","        'style',\n","        'width: 100%; text-align: center; padding: 3px;'\n","    );\n","    titlebar.appendChild(titletext);\n","    this.root.appendChild(titlebar);\n","    this.header = titletext;\n","};\n","\n","mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n","\n","mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n","\n","mpl.figure.prototype._init_canvas = function () {\n","    var fig = this;\n","\n","    var canvas_div = (this.canvas_div = document.createElement('div'));\n","    canvas_div.setAttribute('tabindex', '0');\n","    canvas_div.setAttribute(\n","        'style',\n","        'border: 1px solid #ddd;' +\n","            'box-sizing: content-box;' +\n","            'clear: both;' +\n","            'min-height: 1px;' +\n","            'min-width: 1px;' +\n","            'outline: 0;' +\n","            'overflow: hidden;' +\n","            'position: relative;' +\n","            'resize: both;' +\n","            'z-index: 2;'\n","    );\n","\n","    function on_keyboard_event_closure(name) {\n","        return function (event) {\n","            return fig.key_event(event, name);\n","        };\n","    }\n","\n","    canvas_div.addEventListener(\n","        'keydown',\n","        on_keyboard_event_closure('key_press')\n","    );\n","    canvas_div.addEventListener(\n","        'keyup',\n","        on_keyboard_event_closure('key_release')\n","    );\n","\n","    this._canvas_extra_style(canvas_div);\n","    this.root.appendChild(canvas_div);\n","\n","    var canvas = (this.canvas = document.createElement('canvas'));\n","    canvas.classList.add('mpl-canvas');\n","    canvas.setAttribute(\n","        'style',\n","        'box-sizing: content-box;' +\n","            'pointer-events: none;' +\n","            'position: relative;' +\n","            'z-index: 0;'\n","    );\n","\n","    this.context = canvas.getContext('2d');\n","\n","    var backingStore =\n","        this.context.backingStorePixelRatio ||\n","        this.context.webkitBackingStorePixelRatio ||\n","        this.context.mozBackingStorePixelRatio ||\n","        this.context.msBackingStorePixelRatio ||\n","        this.context.oBackingStorePixelRatio ||\n","        this.context.backingStorePixelRatio ||\n","        1;\n","\n","    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n","\n","    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n","        'canvas'\n","    ));\n","    rubberband_canvas.setAttribute(\n","        'style',\n","        'box-sizing: content-box;' +\n","            'left: 0;' +\n","            'pointer-events: none;' +\n","            'position: absolute;' +\n","            'top: 0;' +\n","            'z-index: 1;'\n","    );\n","\n","    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n","    if (this.ResizeObserver === undefined) {\n","        if (window.ResizeObserver !== undefined) {\n","            this.ResizeObserver = window.ResizeObserver;\n","        } else {\n","            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n","            this.ResizeObserver = obs.ResizeObserver;\n","        }\n","    }\n","\n","    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n","        var nentries = entries.length;\n","        for (var i = 0; i < nentries; i++) {\n","            var entry = entries[i];\n","            var width, height;\n","            if (entry.contentBoxSize) {\n","                if (entry.contentBoxSize instanceof Array) {\n","                    // Chrome 84 implements new version of spec.\n","                    width = entry.contentBoxSize[0].inlineSize;\n","                    height = entry.contentBoxSize[0].blockSize;\n","                } else {\n","                    // Firefox implements old version of spec.\n","                    width = entry.contentBoxSize.inlineSize;\n","                    height = entry.contentBoxSize.blockSize;\n","                }\n","            } else {\n","                // Chrome <84 implements even older version of spec.\n","                width = entry.contentRect.width;\n","                height = entry.contentRect.height;\n","            }\n","\n","            // Keep the size of the canvas and rubber band canvas in sync with\n","            // the canvas container.\n","            if (entry.devicePixelContentBoxSize) {\n","                // Chrome 84 implements new version of spec.\n","                canvas.setAttribute(\n","                    'width',\n","                    entry.devicePixelContentBoxSize[0].inlineSize\n","                );\n","                canvas.setAttribute(\n","                    'height',\n","                    entry.devicePixelContentBoxSize[0].blockSize\n","                );\n","            } else {\n","                canvas.setAttribute('width', width * fig.ratio);\n","                canvas.setAttribute('height', height * fig.ratio);\n","            }\n","            /* This rescales the canvas back to display pixels, so that it\n","             * appears correct on HiDPI screens. */\n","            canvas.style.width = width + 'px';\n","            canvas.style.height = height + 'px';\n","\n","            rubberband_canvas.setAttribute('width', width);\n","            rubberband_canvas.setAttribute('height', height);\n","\n","            // And update the size in Python. We ignore the initial 0/0 size\n","            // that occurs as the element is placed into the DOM, which should\n","            // otherwise not happen due to the minimum size styling.\n","            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n","                fig.request_resize(width, height);\n","            }\n","        }\n","    });\n","    this.resizeObserverInstance.observe(canvas_div);\n","\n","    function on_mouse_event_closure(name) {\n","        /* User Agent sniffing is bad, but WebKit is busted:\n","         * https://bugs.webkit.org/show_bug.cgi?id=144526\n","         * https://bugs.webkit.org/show_bug.cgi?id=181818\n","         * The worst that happens here is that they get an extra browser\n","         * selection when dragging, if this check fails to catch them.\n","         */\n","        var UA = navigator.userAgent;\n","        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n","        if(isWebKit) {\n","            return function (event) {\n","                /* This prevents the web browser from automatically changing to\n","                 * the text insertion cursor when the button is pressed. We\n","                 * want to control all of the cursor setting manually through\n","                 * the 'cursor' event from matplotlib */\n","                event.preventDefault()\n","                return fig.mouse_event(event, name);\n","            };\n","        } else {\n","            return function (event) {\n","                return fig.mouse_event(event, name);\n","            };\n","        }\n","    }\n","\n","    canvas_div.addEventListener(\n","        'mousedown',\n","        on_mouse_event_closure('button_press')\n","    );\n","    canvas_div.addEventListener(\n","        'mouseup',\n","        on_mouse_event_closure('button_release')\n","    );\n","    canvas_div.addEventListener(\n","        'dblclick',\n","        on_mouse_event_closure('dblclick')\n","    );\n","    // Throttle sequential mouse events to 1 every 20ms.\n","    canvas_div.addEventListener(\n","        'mousemove',\n","        on_mouse_event_closure('motion_notify')\n","    );\n","\n","    canvas_div.addEventListener(\n","        'mouseenter',\n","        on_mouse_event_closure('figure_enter')\n","    );\n","    canvas_div.addEventListener(\n","        'mouseleave',\n","        on_mouse_event_closure('figure_leave')\n","    );\n","\n","    canvas_div.addEventListener('wheel', function (event) {\n","        if (event.deltaY < 0) {\n","            event.step = 1;\n","        } else {\n","            event.step = -1;\n","        }\n","        on_mouse_event_closure('scroll')(event);\n","    });\n","\n","    canvas_div.appendChild(canvas);\n","    canvas_div.appendChild(rubberband_canvas);\n","\n","    this.rubberband_context = rubberband_canvas.getContext('2d');\n","    this.rubberband_context.strokeStyle = '#000000';\n","\n","    this._resize_canvas = function (width, height, forward) {\n","        if (forward) {\n","            canvas_div.style.width = width + 'px';\n","            canvas_div.style.height = height + 'px';\n","        }\n","    };\n","\n","    // Disable right mouse context menu.\n","    canvas_div.addEventListener('contextmenu', function (_e) {\n","        event.preventDefault();\n","        return false;\n","    });\n","\n","    function set_focus() {\n","        canvas.focus();\n","        canvas_div.focus();\n","    }\n","\n","    window.setTimeout(set_focus, 100);\n","};\n","\n","mpl.figure.prototype._init_toolbar = function () {\n","    var fig = this;\n","\n","    var toolbar = document.createElement('div');\n","    toolbar.classList = 'mpl-toolbar';\n","    this.root.appendChild(toolbar);\n","\n","    function on_click_closure(name) {\n","        return function (_event) {\n","            return fig.toolbar_button_onclick(name);\n","        };\n","    }\n","\n","    function on_mouseover_closure(tooltip) {\n","        return function (event) {\n","            if (!event.currentTarget.disabled) {\n","                return fig.toolbar_button_onmouseover(tooltip);\n","            }\n","        };\n","    }\n","\n","    fig.buttons = {};\n","    var buttonGroup = document.createElement('div');\n","    buttonGroup.classList = 'mpl-button-group';\n","    for (var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            /* Instead of a spacer, we start a new button group. */\n","            if (buttonGroup.hasChildNodes()) {\n","                toolbar.appendChild(buttonGroup);\n","            }\n","            buttonGroup = document.createElement('div');\n","            buttonGroup.classList = 'mpl-button-group';\n","            continue;\n","        }\n","\n","        var button = (fig.buttons[name] = document.createElement('button'));\n","        button.classList = 'mpl-widget';\n","        button.setAttribute('role', 'button');\n","        button.setAttribute('aria-disabled', 'false');\n","        button.addEventListener('click', on_click_closure(method_name));\n","        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n","\n","        var icon_img = document.createElement('img');\n","        icon_img.src = '_images/' + image + '.png';\n","        icon_img.srcset = '_images/' + image + '_large.png 2x';\n","        icon_img.alt = tooltip;\n","        button.appendChild(icon_img);\n","\n","        buttonGroup.appendChild(button);\n","    }\n","\n","    if (buttonGroup.hasChildNodes()) {\n","        toolbar.appendChild(buttonGroup);\n","    }\n","\n","    var fmt_picker = document.createElement('select');\n","    fmt_picker.classList = 'mpl-widget';\n","    toolbar.appendChild(fmt_picker);\n","    this.format_dropdown = fmt_picker;\n","\n","    for (var ind in mpl.extensions) {\n","        var fmt = mpl.extensions[ind];\n","        var option = document.createElement('option');\n","        option.selected = fmt === mpl.default_extension;\n","        option.innerHTML = fmt;\n","        fmt_picker.appendChild(option);\n","    }\n","\n","    var status_bar = document.createElement('span');\n","    status_bar.classList = 'mpl-message';\n","    toolbar.appendChild(status_bar);\n","    this.message = status_bar;\n","};\n","\n","mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n","    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n","    // which will in turn request a refresh of the image.\n","    this.send_message('resize', { width: x_pixels, height: y_pixels });\n","};\n","\n","mpl.figure.prototype.send_message = function (type, properties) {\n","    properties['type'] = type;\n","    properties['figure_id'] = this.id;\n","    this.ws.send(JSON.stringify(properties));\n","};\n","\n","mpl.figure.prototype.send_draw_message = function () {\n","    if (!this.waiting) {\n","        this.waiting = true;\n","        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n","    }\n","};\n","\n","mpl.figure.prototype.handle_save = function (fig, _msg) {\n","    var format_dropdown = fig.format_dropdown;\n","    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n","    fig.ondownload(fig, format);\n","};\n","\n","mpl.figure.prototype.handle_resize = function (fig, msg) {\n","    var size = msg['size'];\n","    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n","        fig._resize_canvas(size[0], size[1], msg['forward']);\n","        fig.send_message('refresh', {});\n","    }\n","};\n","\n","mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n","    var x0 = msg['x0'] / fig.ratio;\n","    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n","    var x1 = msg['x1'] / fig.ratio;\n","    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n","    x0 = Math.floor(x0) + 0.5;\n","    y0 = Math.floor(y0) + 0.5;\n","    x1 = Math.floor(x1) + 0.5;\n","    y1 = Math.floor(y1) + 0.5;\n","    var min_x = Math.min(x0, x1);\n","    var min_y = Math.min(y0, y1);\n","    var width = Math.abs(x1 - x0);\n","    var height = Math.abs(y1 - y0);\n","\n","    fig.rubberband_context.clearRect(\n","        0,\n","        0,\n","        fig.canvas.width / fig.ratio,\n","        fig.canvas.height / fig.ratio\n","    );\n","\n","    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n","};\n","\n","mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n","    // Updates the figure title.\n","    fig.header.textContent = msg['label'];\n","};\n","\n","mpl.figure.prototype.handle_cursor = function (fig, msg) {\n","    fig.canvas_div.style.cursor = msg['cursor'];\n","};\n","\n","mpl.figure.prototype.handle_message = function (fig, msg) {\n","    fig.message.textContent = msg['message'];\n","};\n","\n","mpl.figure.prototype.handle_draw = function (fig, _msg) {\n","    // Request the server to send over a new figure.\n","    fig.send_draw_message();\n","};\n","\n","mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n","    fig.image_mode = msg['mode'];\n","};\n","\n","mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n","    for (var key in msg) {\n","        if (!(key in fig.buttons)) {\n","            continue;\n","        }\n","        fig.buttons[key].disabled = !msg[key];\n","        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n","    }\n","};\n","\n","mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n","    if (msg['mode'] === 'PAN') {\n","        fig.buttons['Pan'].classList.add('active');\n","        fig.buttons['Zoom'].classList.remove('active');\n","    } else if (msg['mode'] === 'ZOOM') {\n","        fig.buttons['Pan'].classList.remove('active');\n","        fig.buttons['Zoom'].classList.add('active');\n","    } else {\n","        fig.buttons['Pan'].classList.remove('active');\n","        fig.buttons['Zoom'].classList.remove('active');\n","    }\n","};\n","\n","mpl.figure.prototype.updated_canvas_event = function () {\n","    // Called whenever the canvas gets updated.\n","    this.send_message('ack', {});\n","};\n","\n","// A function to construct a web socket function for onmessage handling.\n","// Called in the figure constructor.\n","mpl.figure.prototype._make_on_message_function = function (fig) {\n","    return function socket_on_message(evt) {\n","        if (evt.data instanceof Blob) {\n","            var img = evt.data;\n","            if (img.type !== 'image/png') {\n","                /* FIXME: We get \"Resource interpreted as Image but\n","                 * transferred with MIME type text/plain:\" errors on\n","                 * Chrome.  But how to set the MIME type?  It doesn't seem\n","                 * to be part of the websocket stream */\n","                img.type = 'image/png';\n","            }\n","\n","            /* Free the memory for the previous frames */\n","            if (fig.imageObj.src) {\n","                (window.URL || window.webkitURL).revokeObjectURL(\n","                    fig.imageObj.src\n","                );\n","            }\n","\n","            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n","                img\n","            );\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        } else if (\n","            typeof evt.data === 'string' &&\n","            evt.data.slice(0, 21) === 'data:image/png;base64'\n","        ) {\n","            fig.imageObj.src = evt.data;\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","\n","        var msg = JSON.parse(evt.data);\n","        var msg_type = msg['type'];\n","\n","        // Call the  \"handle_{type}\" callback, which takes\n","        // the figure and JSON message as its only arguments.\n","        try {\n","            var callback = fig['handle_' + msg_type];\n","        } catch (e) {\n","            console.log(\n","                \"No handler for the '\" + msg_type + \"' message type: \",\n","                msg\n","            );\n","            return;\n","        }\n","\n","        if (callback) {\n","            try {\n","                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n","                callback(fig, msg);\n","            } catch (e) {\n","                console.log(\n","                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n","                    e,\n","                    e.stack,\n","                    msg\n","                );\n","            }\n","        }\n","    };\n","};\n","\n","function getModifiers(event) {\n","    var mods = [];\n","    if (event.ctrlKey) {\n","        mods.push('ctrl');\n","    }\n","    if (event.altKey) {\n","        mods.push('alt');\n","    }\n","    if (event.shiftKey) {\n","        mods.push('shift');\n","    }\n","    if (event.metaKey) {\n","        mods.push('meta');\n","    }\n","    return mods;\n","}\n","\n","/*\n"," * return a copy of an object with only non-object keys\n"," * we need this to avoid circular references\n"," * https://stackoverflow.com/a/24161582/3208463\n"," */\n","function simpleKeys(original) {\n","    return Object.keys(original).reduce(function (obj, key) {\n","        if (typeof original[key] !== 'object') {\n","            obj[key] = original[key];\n","        }\n","        return obj;\n","    }, {});\n","}\n","\n","mpl.figure.prototype.mouse_event = function (event, name) {\n","    if (name === 'button_press') {\n","        this.canvas.focus();\n","        this.canvas_div.focus();\n","    }\n","\n","    // from https://stackoverflow.com/q/1114465\n","    var boundingRect = this.canvas.getBoundingClientRect();\n","    var x = (event.clientX - boundingRect.left) * this.ratio;\n","    var y = (event.clientY - boundingRect.top) * this.ratio;\n","\n","    this.send_message(name, {\n","        x: x,\n","        y: y,\n","        button: event.button,\n","        step: event.step,\n","        modifiers: getModifiers(event),\n","        guiEvent: simpleKeys(event),\n","    });\n","\n","    return false;\n","};\n","\n","mpl.figure.prototype._key_event_extra = function (_event, _name) {\n","    // Handle any extra behaviour associated with a key event\n","};\n","\n","mpl.figure.prototype.key_event = function (event, name) {\n","    // Prevent repeat events\n","    if (name === 'key_press') {\n","        if (event.key === this._key) {\n","            return;\n","        } else {\n","            this._key = event.key;\n","        }\n","    }\n","    if (name === 'key_release') {\n","        this._key = null;\n","    }\n","\n","    var value = '';\n","    if (event.ctrlKey && event.key !== 'Control') {\n","        value += 'ctrl+';\n","    }\n","    else if (event.altKey && event.key !== 'Alt') {\n","        value += 'alt+';\n","    }\n","    else if (event.shiftKey && event.key !== 'Shift') {\n","        value += 'shift+';\n","    }\n","\n","    value += 'k' + event.key;\n","\n","    this._key_event_extra(event, name);\n","\n","    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n","    return false;\n","};\n","\n","mpl.figure.prototype.toolbar_button_onclick = function (name) {\n","    if (name === 'download') {\n","        this.handle_save(this, null);\n","    } else {\n","        this.send_message('toolbar_button', { name: name });\n","    }\n","};\n","\n","mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n","    this.message.textContent = tooltip;\n","};\n","\n","///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n","// prettier-ignore\n","var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n","mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n","\n","mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n","\n","mpl.default_extension = \"png\";/* global mpl */\n","\n","var comm_websocket_adapter = function (comm) {\n","    // Create a \"websocket\"-like object which calls the given IPython comm\n","    // object with the appropriate methods. Currently this is a non binary\n","    // socket, so there is still some room for performance tuning.\n","    var ws = {};\n","\n","    ws.binaryType = comm.kernel.ws.binaryType;\n","    ws.readyState = comm.kernel.ws.readyState;\n","    function updateReadyState(_event) {\n","        if (comm.kernel.ws) {\n","            ws.readyState = comm.kernel.ws.readyState;\n","        } else {\n","            ws.readyState = 3; // Closed state.\n","        }\n","    }\n","    comm.kernel.ws.addEventListener('open', updateReadyState);\n","    comm.kernel.ws.addEventListener('close', updateReadyState);\n","    comm.kernel.ws.addEventListener('error', updateReadyState);\n","\n","    ws.close = function () {\n","        comm.close();\n","    };\n","    ws.send = function (m) {\n","        //console.log('sending', m);\n","        comm.send(m);\n","    };\n","    // Register the callback with on_msg.\n","    comm.on_msg(function (msg) {\n","        //console.log('receiving', msg['content']['data'], msg);\n","        var data = msg['content']['data'];\n","        if (data['blob'] !== undefined) {\n","            data = {\n","                data: new Blob(msg['buffers'], { type: data['blob'] }),\n","            };\n","        }\n","        // Pass the mpl event to the overridden (by mpl) onmessage function.\n","        ws.onmessage(data);\n","    });\n","    return ws;\n","};\n","\n","mpl.mpl_figure_comm = function (comm, msg) {\n","    // This is the function which gets called when the mpl process\n","    // starts-up an IPython Comm through the \"matplotlib\" channel.\n","\n","    var id = msg.content.data.id;\n","    // Get hold of the div created by the display call when the Comm\n","    // socket was opened in Python.\n","    var element = document.getElementById(id);\n","    var ws_proxy = comm_websocket_adapter(comm);\n","\n","    function ondownload(figure, _format) {\n","        window.open(figure.canvas.toDataURL());\n","    }\n","\n","    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n","\n","    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n","    // web socket which is closed, not our websocket->open comm proxy.\n","    ws_proxy.onopen();\n","\n","    fig.parent_element = element;\n","    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n","    if (!fig.cell_info) {\n","        console.error('Failed to find cell for figure', id, fig);\n","        return;\n","    }\n","    fig.cell_info[0].output_area.element.on(\n","        'cleared',\n","        { fig: fig },\n","        fig._remove_fig_handler\n","    );\n","};\n","\n","mpl.figure.prototype.handle_close = function (fig, msg) {\n","    var width = fig.canvas.width / fig.ratio;\n","    fig.cell_info[0].output_area.element.off(\n","        'cleared',\n","        fig._remove_fig_handler\n","    );\n","    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n","\n","    // Update the output cell to use the data from the current canvas.\n","    fig.push_to_output();\n","    var dataURL = fig.canvas.toDataURL();\n","    // Re-enable the keyboard manager in IPython - without this line, in FF,\n","    // the notebook keyboard shortcuts fail.\n","    IPython.keyboard_manager.enable();\n","    fig.parent_element.innerHTML =\n","        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","    fig.close_ws(fig, msg);\n","};\n","\n","mpl.figure.prototype.close_ws = function (fig, msg) {\n","    fig.send_message('closing', msg);\n","    // fig.ws.close()\n","};\n","\n","mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n","    // Turn the data on the canvas into data in the output cell.\n","    var width = this.canvas.width / this.ratio;\n","    var dataURL = this.canvas.toDataURL();\n","    this.cell_info[1]['text/html'] =\n","        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","};\n","\n","mpl.figure.prototype.updated_canvas_event = function () {\n","    // Tell IPython that the notebook contents must change.\n","    IPython.notebook.set_dirty(true);\n","    this.send_message('ack', {});\n","    var fig = this;\n","    // Wait a second, then push the new image to the DOM so\n","    // that it is saved nicely (might be nice to debounce this).\n","    setTimeout(function () {\n","        fig.push_to_output();\n","    }, 1000);\n","};\n","\n","mpl.figure.prototype._init_toolbar = function () {\n","    var fig = this;\n","\n","    var toolbar = document.createElement('div');\n","    toolbar.classList = 'btn-toolbar';\n","    this.root.appendChild(toolbar);\n","\n","    function on_click_closure(name) {\n","        return function (_event) {\n","            return fig.toolbar_button_onclick(name);\n","        };\n","    }\n","\n","    function on_mouseover_closure(tooltip) {\n","        return function (event) {\n","            if (!event.currentTarget.disabled) {\n","                return fig.toolbar_button_onmouseover(tooltip);\n","            }\n","        };\n","    }\n","\n","    fig.buttons = {};\n","    var buttonGroup = document.createElement('div');\n","    buttonGroup.classList = 'btn-group';\n","    var button;\n","    for (var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            /* Instead of a spacer, we start a new button group. */\n","            if (buttonGroup.hasChildNodes()) {\n","                toolbar.appendChild(buttonGroup);\n","            }\n","            buttonGroup = document.createElement('div');\n","            buttonGroup.classList = 'btn-group';\n","            continue;\n","        }\n","\n","        button = fig.buttons[name] = document.createElement('button');\n","        button.classList = 'btn btn-default';\n","        button.href = '#';\n","        button.title = name;\n","        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n","        button.addEventListener('click', on_click_closure(method_name));\n","        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n","        buttonGroup.appendChild(button);\n","    }\n","\n","    if (buttonGroup.hasChildNodes()) {\n","        toolbar.appendChild(buttonGroup);\n","    }\n","\n","    // Add the status bar.\n","    var status_bar = document.createElement('span');\n","    status_bar.classList = 'mpl-message pull-right';\n","    toolbar.appendChild(status_bar);\n","    this.message = status_bar;\n","\n","    // Add the close button to the window.\n","    var buttongrp = document.createElement('div');\n","    buttongrp.classList = 'btn-group inline pull-right';\n","    button = document.createElement('button');\n","    button.classList = 'btn btn-mini btn-primary';\n","    button.href = '#';\n","    button.title = 'Stop Interaction';\n","    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n","    button.addEventListener('click', function (_evt) {\n","        fig.handle_close(fig, {});\n","    });\n","    button.addEventListener(\n","        'mouseover',\n","        on_mouseover_closure('Stop Interaction')\n","    );\n","    buttongrp.appendChild(button);\n","    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n","    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n","};\n","\n","mpl.figure.prototype._remove_fig_handler = function (event) {\n","    var fig = event.data.fig;\n","    if (event.target !== this) {\n","        // Ignore bubbled events from children.\n","        return;\n","    }\n","    fig.close_ws(fig, {});\n","};\n","\n","mpl.figure.prototype._root_extra_style = function (el) {\n","    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n","};\n","\n","mpl.figure.prototype._canvas_extra_style = function (el) {\n","    // this is important to make the div 'focusable\n","    el.setAttribute('tabindex', 0);\n","    // reach out to IPython and tell the keyboard manager to turn it's self\n","    // off when our div gets focus\n","\n","    // location in version 3\n","    if (IPython.notebook.keyboard_manager) {\n","        IPython.notebook.keyboard_manager.register_events(el);\n","    } else {\n","        // location in version 2\n","        IPython.keyboard_manager.register_events(el);\n","    }\n","};\n","\n","mpl.figure.prototype._key_event_extra = function (event, _name) {\n","    // Check for shift+enter\n","    if (event.shiftKey && event.which === 13) {\n","        this.canvas_div.blur();\n","        // select the cell after this one\n","        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n","        IPython.notebook.select(index + 1);\n","    }\n","};\n","\n","mpl.figure.prototype.handle_save = function (fig, _msg) {\n","    fig.ondownload(fig, null);\n","};\n","\n","mpl.find_output_cell = function (html_output) {\n","    // Return the cell and output element which can be found *uniquely* in the notebook.\n","    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n","    // IPython event is triggered only after the cells have been serialised, which for\n","    // our purposes (turning an active figure into a static one), is too late.\n","    var cells = IPython.notebook.get_cells();\n","    var ncells = cells.length;\n","    for (var i = 0; i < ncells; i++) {\n","        var cell = cells[i];\n","        if (cell.cell_type === 'code') {\n","            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n","                var data = cell.output_area.outputs[j];\n","                if (data.data) {\n","                    // IPython >= 3 moved mimebundle to data attribute of output\n","                    data = data.data;\n","                }\n","                if (data['text/html'] === html_output) {\n","                    return [cell, data, j];\n","                }\n","            }\n","        }\n","    }\n","};\n","\n","// Register the function which deals with the matplotlib target/channel.\n","// The kernel may be null if the page has been refreshed.\n","if (IPython.notebook.kernel !== null) {\n","    IPython.notebook.kernel.comm_manager.register_target(\n","        'matplotlib',\n","        mpl.mpl_figure_comm\n","    );\n","}\n"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div id='e2aeb280-8f2f-4827-be5d-a31eb061a058'></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|| 8114/8114 [45:39<00:00,  2.96it/s]\n","/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"name":"stdout","output_type":"stream","text":["Running  DLC_resnet50_HorsesMay8shuffle1_58000  with # of training iterations: 58000\n","Running evaluation ...\n"]},{"name":"stderr","output_type":"stream","text":["8114it [47:25,  2.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-58000\n","Results for 58000  training iterations: 5 1 train error: 3.11 pixels. Test error: 3.91  pixels.\n","With pcutoff of 0.1  train error: 3.11 pixels. Test error: 3.9 pixels\n","Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n","Plotting...\n"]},{"data":{"application/javascript":["/* Put everything inside the global mpl namespace */\n","/* global mpl */\n","window.mpl = {};\n","\n","mpl.get_websocket_type = function () {\n","    if (typeof WebSocket !== 'undefined') {\n","        return WebSocket;\n","    } else if (typeof MozWebSocket !== 'undefined') {\n","        return MozWebSocket;\n","    } else {\n","        alert(\n","            'Your browser does not have WebSocket support. ' +\n","                'Please try Chrome, Safari or Firefox  6. ' +\n","                'Firefox 4 and 5 are also supported but you ' +\n","                'have to enable WebSockets in about:config.'\n","        );\n","    }\n","};\n","\n","mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n","    this.id = figure_id;\n","\n","    this.ws = websocket;\n","\n","    this.supports_binary = this.ws.binaryType !== undefined;\n","\n","    if (!this.supports_binary) {\n","        var warnings = document.getElementById('mpl-warnings');\n","        if (warnings) {\n","            warnings.style.display = 'block';\n","            warnings.textContent =\n","                'This browser does not support binary websocket messages. ' +\n","                'Performance may be slow.';\n","        }\n","    }\n","\n","    this.imageObj = new Image();\n","\n","    this.context = undefined;\n","    this.message = undefined;\n","    this.canvas = undefined;\n","    this.rubberband_canvas = undefined;\n","    this.rubberband_context = undefined;\n","    this.format_dropdown = undefined;\n","\n","    this.image_mode = 'full';\n","\n","    this.root = document.createElement('div');\n","    this.root.setAttribute('style', 'display: inline-block');\n","    this._root_extra_style(this.root);\n","\n","    parent_element.appendChild(this.root);\n","\n","    this._init_header(this);\n","    this._init_canvas(this);\n","    this._init_toolbar(this);\n","\n","    var fig = this;\n","\n","    this.waiting = false;\n","\n","    this.ws.onopen = function () {\n","        fig.send_message('supports_binary', { value: fig.supports_binary });\n","        fig.send_message('send_image_mode', {});\n","        if (fig.ratio !== 1) {\n","            fig.send_message('set_device_pixel_ratio', {\n","                device_pixel_ratio: fig.ratio,\n","            });\n","        }\n","        fig.send_message('refresh', {});\n","    };\n","\n","    this.imageObj.onload = function () {\n","        if (fig.image_mode === 'full') {\n","            // Full images could contain transparency (where diff images\n","            // almost always do), so we need to clear the canvas so that\n","            // there is no ghosting.\n","            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n","        }\n","        fig.context.drawImage(fig.imageObj, 0, 0);\n","    };\n","\n","    this.imageObj.onunload = function () {\n","        fig.ws.close();\n","    };\n","\n","    this.ws.onmessage = this._make_on_message_function(this);\n","\n","    this.ondownload = ondownload;\n","};\n","\n","mpl.figure.prototype._init_header = function () {\n","    var titlebar = document.createElement('div');\n","    titlebar.classList =\n","        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n","    var titletext = document.createElement('div');\n","    titletext.classList = 'ui-dialog-title';\n","    titletext.setAttribute(\n","        'style',\n","        'width: 100%; text-align: center; padding: 3px;'\n","    );\n","    titlebar.appendChild(titletext);\n","    this.root.appendChild(titlebar);\n","    this.header = titletext;\n","};\n","\n","mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n","\n","mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n","\n","mpl.figure.prototype._init_canvas = function () {\n","    var fig = this;\n","\n","    var canvas_div = (this.canvas_div = document.createElement('div'));\n","    canvas_div.setAttribute('tabindex', '0');\n","    canvas_div.setAttribute(\n","        'style',\n","        'border: 1px solid #ddd;' +\n","            'box-sizing: content-box;' +\n","            'clear: both;' +\n","            'min-height: 1px;' +\n","            'min-width: 1px;' +\n","            'outline: 0;' +\n","            'overflow: hidden;' +\n","            'position: relative;' +\n","            'resize: both;' +\n","            'z-index: 2;'\n","    );\n","\n","    function on_keyboard_event_closure(name) {\n","        return function (event) {\n","            return fig.key_event(event, name);\n","        };\n","    }\n","\n","    canvas_div.addEventListener(\n","        'keydown',\n","        on_keyboard_event_closure('key_press')\n","    );\n","    canvas_div.addEventListener(\n","        'keyup',\n","        on_keyboard_event_closure('key_release')\n","    );\n","\n","    this._canvas_extra_style(canvas_div);\n","    this.root.appendChild(canvas_div);\n","\n","    var canvas = (this.canvas = document.createElement('canvas'));\n","    canvas.classList.add('mpl-canvas');\n","    canvas.setAttribute(\n","        'style',\n","        'box-sizing: content-box;' +\n","            'pointer-events: none;' +\n","            'position: relative;' +\n","            'z-index: 0;'\n","    );\n","\n","    this.context = canvas.getContext('2d');\n","\n","    var backingStore =\n","        this.context.backingStorePixelRatio ||\n","        this.context.webkitBackingStorePixelRatio ||\n","        this.context.mozBackingStorePixelRatio ||\n","        this.context.msBackingStorePixelRatio ||\n","        this.context.oBackingStorePixelRatio ||\n","        this.context.backingStorePixelRatio ||\n","        1;\n","\n","    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n","\n","    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n","        'canvas'\n","    ));\n","    rubberband_canvas.setAttribute(\n","        'style',\n","        'box-sizing: content-box;' +\n","            'left: 0;' +\n","            'pointer-events: none;' +\n","            'position: absolute;' +\n","            'top: 0;' +\n","            'z-index: 1;'\n","    );\n","\n","    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n","    if (this.ResizeObserver === undefined) {\n","        if (window.ResizeObserver !== undefined) {\n","            this.ResizeObserver = window.ResizeObserver;\n","        } else {\n","            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n","            this.ResizeObserver = obs.ResizeObserver;\n","        }\n","    }\n","\n","    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n","        var nentries = entries.length;\n","        for (var i = 0; i < nentries; i++) {\n","            var entry = entries[i];\n","            var width, height;\n","            if (entry.contentBoxSize) {\n","                if (entry.contentBoxSize instanceof Array) {\n","                    // Chrome 84 implements new version of spec.\n","                    width = entry.contentBoxSize[0].inlineSize;\n","                    height = entry.contentBoxSize[0].blockSize;\n","                } else {\n","                    // Firefox implements old version of spec.\n","                    width = entry.contentBoxSize.inlineSize;\n","                    height = entry.contentBoxSize.blockSize;\n","                }\n","            } else {\n","                // Chrome <84 implements even older version of spec.\n","                width = entry.contentRect.width;\n","                height = entry.contentRect.height;\n","            }\n","\n","            // Keep the size of the canvas and rubber band canvas in sync with\n","            // the canvas container.\n","            if (entry.devicePixelContentBoxSize) {\n","                // Chrome 84 implements new version of spec.\n","                canvas.setAttribute(\n","                    'width',\n","                    entry.devicePixelContentBoxSize[0].inlineSize\n","                );\n","                canvas.setAttribute(\n","                    'height',\n","                    entry.devicePixelContentBoxSize[0].blockSize\n","                );\n","            } else {\n","                canvas.setAttribute('width', width * fig.ratio);\n","                canvas.setAttribute('height', height * fig.ratio);\n","            }\n","            /* This rescales the canvas back to display pixels, so that it\n","             * appears correct on HiDPI screens. */\n","            canvas.style.width = width + 'px';\n","            canvas.style.height = height + 'px';\n","\n","            rubberband_canvas.setAttribute('width', width);\n","            rubberband_canvas.setAttribute('height', height);\n","\n","            // And update the size in Python. We ignore the initial 0/0 size\n","            // that occurs as the element is placed into the DOM, which should\n","            // otherwise not happen due to the minimum size styling.\n","            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n","                fig.request_resize(width, height);\n","            }\n","        }\n","    });\n","    this.resizeObserverInstance.observe(canvas_div);\n","\n","    function on_mouse_event_closure(name) {\n","        /* User Agent sniffing is bad, but WebKit is busted:\n","         * https://bugs.webkit.org/show_bug.cgi?id=144526\n","         * https://bugs.webkit.org/show_bug.cgi?id=181818\n","         * The worst that happens here is that they get an extra browser\n","         * selection when dragging, if this check fails to catch them.\n","         */\n","        var UA = navigator.userAgent;\n","        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n","        if(isWebKit) {\n","            return function (event) {\n","                /* This prevents the web browser from automatically changing to\n","                 * the text insertion cursor when the button is pressed. We\n","                 * want to control all of the cursor setting manually through\n","                 * the 'cursor' event from matplotlib */\n","                event.preventDefault()\n","                return fig.mouse_event(event, name);\n","            };\n","        } else {\n","            return function (event) {\n","                return fig.mouse_event(event, name);\n","            };\n","        }\n","    }\n","\n","    canvas_div.addEventListener(\n","        'mousedown',\n","        on_mouse_event_closure('button_press')\n","    );\n","    canvas_div.addEventListener(\n","        'mouseup',\n","        on_mouse_event_closure('button_release')\n","    );\n","    canvas_div.addEventListener(\n","        'dblclick',\n","        on_mouse_event_closure('dblclick')\n","    );\n","    // Throttle sequential mouse events to 1 every 20ms.\n","    canvas_div.addEventListener(\n","        'mousemove',\n","        on_mouse_event_closure('motion_notify')\n","    );\n","\n","    canvas_div.addEventListener(\n","        'mouseenter',\n","        on_mouse_event_closure('figure_enter')\n","    );\n","    canvas_div.addEventListener(\n","        'mouseleave',\n","        on_mouse_event_closure('figure_leave')\n","    );\n","\n","    canvas_div.addEventListener('wheel', function (event) {\n","        if (event.deltaY < 0) {\n","            event.step = 1;\n","        } else {\n","            event.step = -1;\n","        }\n","        on_mouse_event_closure('scroll')(event);\n","    });\n","\n","    canvas_div.appendChild(canvas);\n","    canvas_div.appendChild(rubberband_canvas);\n","\n","    this.rubberband_context = rubberband_canvas.getContext('2d');\n","    this.rubberband_context.strokeStyle = '#000000';\n","\n","    this._resize_canvas = function (width, height, forward) {\n","        if (forward) {\n","            canvas_div.style.width = width + 'px';\n","            canvas_div.style.height = height + 'px';\n","        }\n","    };\n","\n","    // Disable right mouse context menu.\n","    canvas_div.addEventListener('contextmenu', function (_e) {\n","        event.preventDefault();\n","        return false;\n","    });\n","\n","    function set_focus() {\n","        canvas.focus();\n","        canvas_div.focus();\n","    }\n","\n","    window.setTimeout(set_focus, 100);\n","};\n","\n","mpl.figure.prototype._init_toolbar = function () {\n","    var fig = this;\n","\n","    var toolbar = document.createElement('div');\n","    toolbar.classList = 'mpl-toolbar';\n","    this.root.appendChild(toolbar);\n","\n","    function on_click_closure(name) {\n","        return function (_event) {\n","            return fig.toolbar_button_onclick(name);\n","        };\n","    }\n","\n","    function on_mouseover_closure(tooltip) {\n","        return function (event) {\n","            if (!event.currentTarget.disabled) {\n","                return fig.toolbar_button_onmouseover(tooltip);\n","            }\n","        };\n","    }\n","\n","    fig.buttons = {};\n","    var buttonGroup = document.createElement('div');\n","    buttonGroup.classList = 'mpl-button-group';\n","    for (var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            /* Instead of a spacer, we start a new button group. */\n","            if (buttonGroup.hasChildNodes()) {\n","                toolbar.appendChild(buttonGroup);\n","            }\n","            buttonGroup = document.createElement('div');\n","            buttonGroup.classList = 'mpl-button-group';\n","            continue;\n","        }\n","\n","        var button = (fig.buttons[name] = document.createElement('button'));\n","        button.classList = 'mpl-widget';\n","        button.setAttribute('role', 'button');\n","        button.setAttribute('aria-disabled', 'false');\n","        button.addEventListener('click', on_click_closure(method_name));\n","        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n","\n","        var icon_img = document.createElement('img');\n","        icon_img.src = '_images/' + image + '.png';\n","        icon_img.srcset = '_images/' + image + '_large.png 2x';\n","        icon_img.alt = tooltip;\n","        button.appendChild(icon_img);\n","\n","        buttonGroup.appendChild(button);\n","    }\n","\n","    if (buttonGroup.hasChildNodes()) {\n","        toolbar.appendChild(buttonGroup);\n","    }\n","\n","    var fmt_picker = document.createElement('select');\n","    fmt_picker.classList = 'mpl-widget';\n","    toolbar.appendChild(fmt_picker);\n","    this.format_dropdown = fmt_picker;\n","\n","    for (var ind in mpl.extensions) {\n","        var fmt = mpl.extensions[ind];\n","        var option = document.createElement('option');\n","        option.selected = fmt === mpl.default_extension;\n","        option.innerHTML = fmt;\n","        fmt_picker.appendChild(option);\n","    }\n","\n","    var status_bar = document.createElement('span');\n","    status_bar.classList = 'mpl-message';\n","    toolbar.appendChild(status_bar);\n","    this.message = status_bar;\n","};\n","\n","mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n","    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n","    // which will in turn request a refresh of the image.\n","    this.send_message('resize', { width: x_pixels, height: y_pixels });\n","};\n","\n","mpl.figure.prototype.send_message = function (type, properties) {\n","    properties['type'] = type;\n","    properties['figure_id'] = this.id;\n","    this.ws.send(JSON.stringify(properties));\n","};\n","\n","mpl.figure.prototype.send_draw_message = function () {\n","    if (!this.waiting) {\n","        this.waiting = true;\n","        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n","    }\n","};\n","\n","mpl.figure.prototype.handle_save = function (fig, _msg) {\n","    var format_dropdown = fig.format_dropdown;\n","    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n","    fig.ondownload(fig, format);\n","};\n","\n","mpl.figure.prototype.handle_resize = function (fig, msg) {\n","    var size = msg['size'];\n","    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n","        fig._resize_canvas(size[0], size[1], msg['forward']);\n","        fig.send_message('refresh', {});\n","    }\n","};\n","\n","mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n","    var x0 = msg['x0'] / fig.ratio;\n","    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n","    var x1 = msg['x1'] / fig.ratio;\n","    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n","    x0 = Math.floor(x0) + 0.5;\n","    y0 = Math.floor(y0) + 0.5;\n","    x1 = Math.floor(x1) + 0.5;\n","    y1 = Math.floor(y1) + 0.5;\n","    var min_x = Math.min(x0, x1);\n","    var min_y = Math.min(y0, y1);\n","    var width = Math.abs(x1 - x0);\n","    var height = Math.abs(y1 - y0);\n","\n","    fig.rubberband_context.clearRect(\n","        0,\n","        0,\n","        fig.canvas.width / fig.ratio,\n","        fig.canvas.height / fig.ratio\n","    );\n","\n","    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n","};\n","\n","mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n","    // Updates the figure title.\n","    fig.header.textContent = msg['label'];\n","};\n","\n","mpl.figure.prototype.handle_cursor = function (fig, msg) {\n","    fig.canvas_div.style.cursor = msg['cursor'];\n","};\n","\n","mpl.figure.prototype.handle_message = function (fig, msg) {\n","    fig.message.textContent = msg['message'];\n","};\n","\n","mpl.figure.prototype.handle_draw = function (fig, _msg) {\n","    // Request the server to send over a new figure.\n","    fig.send_draw_message();\n","};\n","\n","mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n","    fig.image_mode = msg['mode'];\n","};\n","\n","mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n","    for (var key in msg) {\n","        if (!(key in fig.buttons)) {\n","            continue;\n","        }\n","        fig.buttons[key].disabled = !msg[key];\n","        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n","    }\n","};\n","\n","mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n","    if (msg['mode'] === 'PAN') {\n","        fig.buttons['Pan'].classList.add('active');\n","        fig.buttons['Zoom'].classList.remove('active');\n","    } else if (msg['mode'] === 'ZOOM') {\n","        fig.buttons['Pan'].classList.remove('active');\n","        fig.buttons['Zoom'].classList.add('active');\n","    } else {\n","        fig.buttons['Pan'].classList.remove('active');\n","        fig.buttons['Zoom'].classList.remove('active');\n","    }\n","};\n","\n","mpl.figure.prototype.updated_canvas_event = function () {\n","    // Called whenever the canvas gets updated.\n","    this.send_message('ack', {});\n","};\n","\n","// A function to construct a web socket function for onmessage handling.\n","// Called in the figure constructor.\n","mpl.figure.prototype._make_on_message_function = function (fig) {\n","    return function socket_on_message(evt) {\n","        if (evt.data instanceof Blob) {\n","            var img = evt.data;\n","            if (img.type !== 'image/png') {\n","                /* FIXME: We get \"Resource interpreted as Image but\n","                 * transferred with MIME type text/plain:\" errors on\n","                 * Chrome.  But how to set the MIME type?  It doesn't seem\n","                 * to be part of the websocket stream */\n","                img.type = 'image/png';\n","            }\n","\n","            /* Free the memory for the previous frames */\n","            if (fig.imageObj.src) {\n","                (window.URL || window.webkitURL).revokeObjectURL(\n","                    fig.imageObj.src\n","                );\n","            }\n","\n","            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n","                img\n","            );\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        } else if (\n","            typeof evt.data === 'string' &&\n","            evt.data.slice(0, 21) === 'data:image/png;base64'\n","        ) {\n","            fig.imageObj.src = evt.data;\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","\n","        var msg = JSON.parse(evt.data);\n","        var msg_type = msg['type'];\n","\n","        // Call the  \"handle_{type}\" callback, which takes\n","        // the figure and JSON message as its only arguments.\n","        try {\n","            var callback = fig['handle_' + msg_type];\n","        } catch (e) {\n","            console.log(\n","                \"No handler for the '\" + msg_type + \"' message type: \",\n","                msg\n","            );\n","            return;\n","        }\n","\n","        if (callback) {\n","            try {\n","                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n","                callback(fig, msg);\n","            } catch (e) {\n","                console.log(\n","                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n","                    e,\n","                    e.stack,\n","                    msg\n","                );\n","            }\n","        }\n","    };\n","};\n","\n","function getModifiers(event) {\n","    var mods = [];\n","    if (event.ctrlKey) {\n","        mods.push('ctrl');\n","    }\n","    if (event.altKey) {\n","        mods.push('alt');\n","    }\n","    if (event.shiftKey) {\n","        mods.push('shift');\n","    }\n","    if (event.metaKey) {\n","        mods.push('meta');\n","    }\n","    return mods;\n","}\n","\n","/*\n"," * return a copy of an object with only non-object keys\n"," * we need this to avoid circular references\n"," * https://stackoverflow.com/a/24161582/3208463\n"," */\n","function simpleKeys(original) {\n","    return Object.keys(original).reduce(function (obj, key) {\n","        if (typeof original[key] !== 'object') {\n","            obj[key] = original[key];\n","        }\n","        return obj;\n","    }, {});\n","}\n","\n","mpl.figure.prototype.mouse_event = function (event, name) {\n","    if (name === 'button_press') {\n","        this.canvas.focus();\n","        this.canvas_div.focus();\n","    }\n","\n","    // from https://stackoverflow.com/q/1114465\n","    var boundingRect = this.canvas.getBoundingClientRect();\n","    var x = (event.clientX - boundingRect.left) * this.ratio;\n","    var y = (event.clientY - boundingRect.top) * this.ratio;\n","\n","    this.send_message(name, {\n","        x: x,\n","        y: y,\n","        button: event.button,\n","        step: event.step,\n","        modifiers: getModifiers(event),\n","        guiEvent: simpleKeys(event),\n","    });\n","\n","    return false;\n","};\n","\n","mpl.figure.prototype._key_event_extra = function (_event, _name) {\n","    // Handle any extra behaviour associated with a key event\n","};\n","\n","mpl.figure.prototype.key_event = function (event, name) {\n","    // Prevent repeat events\n","    if (name === 'key_press') {\n","        if (event.key === this._key) {\n","            return;\n","        } else {\n","            this._key = event.key;\n","        }\n","    }\n","    if (name === 'key_release') {\n","        this._key = null;\n","    }\n","\n","    var value = '';\n","    if (event.ctrlKey && event.key !== 'Control') {\n","        value += 'ctrl+';\n","    }\n","    else if (event.altKey && event.key !== 'Alt') {\n","        value += 'alt+';\n","    }\n","    else if (event.shiftKey && event.key !== 'Shift') {\n","        value += 'shift+';\n","    }\n","\n","    value += 'k' + event.key;\n","\n","    this._key_event_extra(event, name);\n","\n","    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n","    return false;\n","};\n","\n","mpl.figure.prototype.toolbar_button_onclick = function (name) {\n","    if (name === 'download') {\n","        this.handle_save(this, null);\n","    } else {\n","        this.send_message('toolbar_button', { name: name });\n","    }\n","};\n","\n","mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n","    this.message.textContent = tooltip;\n","};\n","\n","///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n","// prettier-ignore\n","var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n","mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n","\n","mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n","\n","mpl.default_extension = \"png\";/* global mpl */\n","\n","var comm_websocket_adapter = function (comm) {\n","    // Create a \"websocket\"-like object which calls the given IPython comm\n","    // object with the appropriate methods. Currently this is a non binary\n","    // socket, so there is still some room for performance tuning.\n","    var ws = {};\n","\n","    ws.binaryType = comm.kernel.ws.binaryType;\n","    ws.readyState = comm.kernel.ws.readyState;\n","    function updateReadyState(_event) {\n","        if (comm.kernel.ws) {\n","            ws.readyState = comm.kernel.ws.readyState;\n","        } else {\n","            ws.readyState = 3; // Closed state.\n","        }\n","    }\n","    comm.kernel.ws.addEventListener('open', updateReadyState);\n","    comm.kernel.ws.addEventListener('close', updateReadyState);\n","    comm.kernel.ws.addEventListener('error', updateReadyState);\n","\n","    ws.close = function () {\n","        comm.close();\n","    };\n","    ws.send = function (m) {\n","        //console.log('sending', m);\n","        comm.send(m);\n","    };\n","    // Register the callback with on_msg.\n","    comm.on_msg(function (msg) {\n","        //console.log('receiving', msg['content']['data'], msg);\n","        var data = msg['content']['data'];\n","        if (data['blob'] !== undefined) {\n","            data = {\n","                data: new Blob(msg['buffers'], { type: data['blob'] }),\n","            };\n","        }\n","        // Pass the mpl event to the overridden (by mpl) onmessage function.\n","        ws.onmessage(data);\n","    });\n","    return ws;\n","};\n","\n","mpl.mpl_figure_comm = function (comm, msg) {\n","    // This is the function which gets called when the mpl process\n","    // starts-up an IPython Comm through the \"matplotlib\" channel.\n","\n","    var id = msg.content.data.id;\n","    // Get hold of the div created by the display call when the Comm\n","    // socket was opened in Python.\n","    var element = document.getElementById(id);\n","    var ws_proxy = comm_websocket_adapter(comm);\n","\n","    function ondownload(figure, _format) {\n","        window.open(figure.canvas.toDataURL());\n","    }\n","\n","    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n","\n","    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n","    // web socket which is closed, not our websocket->open comm proxy.\n","    ws_proxy.onopen();\n","\n","    fig.parent_element = element;\n","    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n","    if (!fig.cell_info) {\n","        console.error('Failed to find cell for figure', id, fig);\n","        return;\n","    }\n","    fig.cell_info[0].output_area.element.on(\n","        'cleared',\n","        { fig: fig },\n","        fig._remove_fig_handler\n","    );\n","};\n","\n","mpl.figure.prototype.handle_close = function (fig, msg) {\n","    var width = fig.canvas.width / fig.ratio;\n","    fig.cell_info[0].output_area.element.off(\n","        'cleared',\n","        fig._remove_fig_handler\n","    );\n","    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n","\n","    // Update the output cell to use the data from the current canvas.\n","    fig.push_to_output();\n","    var dataURL = fig.canvas.toDataURL();\n","    // Re-enable the keyboard manager in IPython - without this line, in FF,\n","    // the notebook keyboard shortcuts fail.\n","    IPython.keyboard_manager.enable();\n","    fig.parent_element.innerHTML =\n","        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","    fig.close_ws(fig, msg);\n","};\n","\n","mpl.figure.prototype.close_ws = function (fig, msg) {\n","    fig.send_message('closing', msg);\n","    // fig.ws.close()\n","};\n","\n","mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n","    // Turn the data on the canvas into data in the output cell.\n","    var width = this.canvas.width / this.ratio;\n","    var dataURL = this.canvas.toDataURL();\n","    this.cell_info[1]['text/html'] =\n","        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","};\n","\n","mpl.figure.prototype.updated_canvas_event = function () {\n","    // Tell IPython that the notebook contents must change.\n","    IPython.notebook.set_dirty(true);\n","    this.send_message('ack', {});\n","    var fig = this;\n","    // Wait a second, then push the new image to the DOM so\n","    // that it is saved nicely (might be nice to debounce this).\n","    setTimeout(function () {\n","        fig.push_to_output();\n","    }, 1000);\n","};\n","\n","mpl.figure.prototype._init_toolbar = function () {\n","    var fig = this;\n","\n","    var toolbar = document.createElement('div');\n","    toolbar.classList = 'btn-toolbar';\n","    this.root.appendChild(toolbar);\n","\n","    function on_click_closure(name) {\n","        return function (_event) {\n","            return fig.toolbar_button_onclick(name);\n","        };\n","    }\n","\n","    function on_mouseover_closure(tooltip) {\n","        return function (event) {\n","            if (!event.currentTarget.disabled) {\n","                return fig.toolbar_button_onmouseover(tooltip);\n","            }\n","        };\n","    }\n","\n","    fig.buttons = {};\n","    var buttonGroup = document.createElement('div');\n","    buttonGroup.classList = 'btn-group';\n","    var button;\n","    for (var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            /* Instead of a spacer, we start a new button group. */\n","            if (buttonGroup.hasChildNodes()) {\n","                toolbar.appendChild(buttonGroup);\n","            }\n","            buttonGroup = document.createElement('div');\n","            buttonGroup.classList = 'btn-group';\n","            continue;\n","        }\n","\n","        button = fig.buttons[name] = document.createElement('button');\n","        button.classList = 'btn btn-default';\n","        button.href = '#';\n","        button.title = name;\n","        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n","        button.addEventListener('click', on_click_closure(method_name));\n","        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n","        buttonGroup.appendChild(button);\n","    }\n","\n","    if (buttonGroup.hasChildNodes()) {\n","        toolbar.appendChild(buttonGroup);\n","    }\n","\n","    // Add the status bar.\n","    var status_bar = document.createElement('span');\n","    status_bar.classList = 'mpl-message pull-right';\n","    toolbar.appendChild(status_bar);\n","    this.message = status_bar;\n","\n","    // Add the close button to the window.\n","    var buttongrp = document.createElement('div');\n","    buttongrp.classList = 'btn-group inline pull-right';\n","    button = document.createElement('button');\n","    button.classList = 'btn btn-mini btn-primary';\n","    button.href = '#';\n","    button.title = 'Stop Interaction';\n","    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n","    button.addEventListener('click', function (_evt) {\n","        fig.handle_close(fig, {});\n","    });\n","    button.addEventListener(\n","        'mouseover',\n","        on_mouseover_closure('Stop Interaction')\n","    );\n","    buttongrp.appendChild(button);\n","    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n","    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n","};\n","\n","mpl.figure.prototype._remove_fig_handler = function (event) {\n","    var fig = event.data.fig;\n","    if (event.target !== this) {\n","        // Ignore bubbled events from children.\n","        return;\n","    }\n","    fig.close_ws(fig, {});\n","};\n","\n","mpl.figure.prototype._root_extra_style = function (el) {\n","    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n","};\n","\n","mpl.figure.prototype._canvas_extra_style = function (el) {\n","    // this is important to make the div 'focusable\n","    el.setAttribute('tabindex', 0);\n","    // reach out to IPython and tell the keyboard manager to turn it's self\n","    // off when our div gets focus\n","\n","    // location in version 3\n","    if (IPython.notebook.keyboard_manager) {\n","        IPython.notebook.keyboard_manager.register_events(el);\n","    } else {\n","        // location in version 2\n","        IPython.keyboard_manager.register_events(el);\n","    }\n","};\n","\n","mpl.figure.prototype._key_event_extra = function (event, _name) {\n","    // Check for shift+enter\n","    if (event.shiftKey && event.which === 13) {\n","        this.canvas_div.blur();\n","        // select the cell after this one\n","        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n","        IPython.notebook.select(index + 1);\n","    }\n","};\n","\n","mpl.figure.prototype.handle_save = function (fig, _msg) {\n","    fig.ondownload(fig, null);\n","};\n","\n","mpl.find_output_cell = function (html_output) {\n","    // Return the cell and output element which can be found *uniquely* in the notebook.\n","    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n","    // IPython event is triggered only after the cells have been serialised, which for\n","    // our purposes (turning an active figure into a static one), is too late.\n","    var cells = IPython.notebook.get_cells();\n","    var ncells = cells.length;\n","    for (var i = 0; i < ncells; i++) {\n","        var cell = cells[i];\n","        if (cell.cell_type === 'code') {\n","            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n","                var data = cell.output_area.outputs[j];\n","                if (data.data) {\n","                    // IPython >= 3 moved mimebundle to data attribute of output\n","                    data = data.data;\n","                }\n","                if (data['text/html'] === html_output) {\n","                    return [cell, data, j];\n","                }\n","            }\n","        }\n","    }\n","};\n","\n","// Register the function which deals with the matplotlib target/channel.\n","// The kernel may be null if the page has been refreshed.\n","if (IPython.notebook.kernel !== null) {\n","    IPython.notebook.kernel.comm_manager.register_target(\n","        'matplotlib',\n","        mpl.mpl_figure_comm\n","    );\n","}\n"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div id='1d2bfe50-857a-4c9d-a4a6-da52555d811d'></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|| 8114/8114 [45:51<00:00,  2.95it/s]\n","/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"name":"stdout","output_type":"stream","text":["Running  DLC_resnet50_HorsesMay8shuffle1_58500  with # of training iterations: 58500\n","Running evaluation ...\n"]},{"name":"stderr","output_type":"stream","text":["5576it [33:48,  2.75it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-cc0d8c46af34>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#so be sure your labels are good! (And you have trained enough ;)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/deeplabcut/pose_estimation_tensorflow/core/evaluate.py\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(config, Shuffles, trainingsetindex, plotting, show_errors, comparisonbodyparts, gputouse, rescale, modelprefix)\u001b[0m\n\u001b[1;32m    830\u001b[0m                             \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                             \u001b[0;31m# Compute prediction with the CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                             outputs_np = sess.run(\n\u001b[0m\u001b[1;32m    833\u001b[0m                                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                             )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["%matplotlib notebook\n","deeplabcut.evaluate_network(path_config_file,plotting=True)\n","\n","# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\n","#so be sure your labels are good! (And you have trained enough ;)"]},{"cell_type":"markdown","metadata":{"id":"BaLBl3TQtrfB"},"source":["## There is an optional refinement step you can do outside of Colab:\n","- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n","- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos...\n","- Please see the repo and protocol instructions on how to refine your data!"]},{"cell_type":"markdown","metadata":{"id":"OVFLSKKfoEJk"},"source":["## Start Analyzing videos:\n","This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n","\n","The results are stored in hd5 file in the same directory where the video resides."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSZDNkGoMSZG"},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/MyDrive/DeepLabCut2/outv2\")\n","videofile_path='/content/drive/MyDrive/DeepLabCut2/outv2/B.mp4'\n","VideoType='mp4'\n","#path_config_file='/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset10shuffle1/train/pose_cfg.yaml'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":112350,"status":"ok","timestamp":1682274088170,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-120},"id":"Y_LZiS_0oEJl","outputId":"39435e99-84b4-4d75-975f-a60eb0316a40"},"outputs":[{"output_type":"stream","name":"stderr","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21]],\n"," 'all_joints_names': ['Nose',\n","                      'Eye',\n","                      'Nearknee',\n","                      'Nearfrontfetlock',\n","                      'Nearfrontfoot',\n","                      'Offknee',\n","                      'Offfrontfetlock',\n","                      'Offfrontfoot',\n","                      'Shoulder',\n","                      'Midshoulder',\n","                      'Elbow',\n","                      'Girth',\n","                      'Wither',\n","                      'Nearhindhock',\n","                      'Nearhindfetlock',\n","                      'Nearhindfoot',\n","                      'Hip',\n","                      'Stifle',\n","                      'Offhindhock',\n","                      'Offhindfetlock',\n","                      'Offhindfoot',\n","                      'Ischium'],\n"," 'batch_size': 1,\n"," 'crop_pad': 0,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_HorsesMay8/Horses_Byron5shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deterministic': False,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/usr/local/lib/python3.9/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 1.0,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'mirror': False,\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 22,\n"," 'optimizer': 'sgd',\n"," 'pairwise_huber_loss': True,\n"," 'pairwise_predict': False,\n"," 'partaffinityfield_predict': False,\n"," 'regularize': False,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1/test/snapshot',\n"," 'stride': 8.0,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n","/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!\n","Using snapshot-205500 for model /content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/dlc-models/iteration-0/HorsesMay8-trainset5shuffle1\n","Starting to analyze %  /content/drive/MyDrive/DeepLabCut2/outv2/B.mp4\n","Loading  /content/drive/MyDrive/DeepLabCut2/outv2/B.mp4\n","Duration of video [s]:  49.04 , recorded with  25.0 fps!\n","Overall # of frames:  1226  found with (before cropping) frame dimensions:  1280 720\n","Starting to extract posture\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 1226/1226 [01:44<00:00, 11.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saving results in /content/drive/MyDrive/DeepLabCut2/outv2...\n","Saving csv poses!\n","The videos are analyzed. Now your research can truly start! \n"," You can create labeled videos with 'create_labeled_video'\n","If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"]},{"output_type":"execute_result","data":{"text/plain":["'DLC_resnet50_HorsesMay8shuffle1_205500'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType,save_as_csv=True,)"]},{"cell_type":"markdown","source":["# Learning From Mistakes"],"metadata":{"id":"iaZ23LX5Xy96"}},{"cell_type":"code","source":["#deeplabcut.extract_outlier_frames(path_config_file,videofile_path, videotype=VideoType,numframes2pick:5 ,automatic=True)"],"metadata":{"id":"2966pGKnOzqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deeplabcut.add_new_videos(path_config_file, videofile_path, videotype=VideoType, copy_videos=False)"],"metadata":{"id":"iJ0qwvDfR1lC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deeplabcut.refine_labels(path_config_file)"],"metadata":{"id":"losvTjGgSRbN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8GTiuJESoEKH"},"source":["## Plot the trajectories of the analyzed videos:\n","This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3267,"status":"ok","timestamp":1682274091432,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-120},"id":"gX21zZbXoEKJ","outputId":"481d2013-e6dc-4c82-9b82-2eb3378f9f7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Changing snapshotindext to the last one -- plotting, videomaking, etc. should not be performed for all indices. For more selectivity enter the ordinal number of the snapshot you want (ie. 4 for the fifth) in the config file.\n","Loading  /content/drive/MyDrive/DeepLabCut2/outv2/B.mp4 and data.\n","Plots created! Please check the directory \"plot-poses\" within the video directory\n"]}],"source":["deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType ,linewidth=0.5)\n","\n","#deeplabcut.plot_trajectories(config_path, videofile_path, trajectory_color=[0.5, 0.5, 0.5], filtered=True, showfigures=True, destfolder=None, displayedbodyparts='all', linewidth=1.0, alpha=1.0, zoom=1.0)"]},{"cell_type":"markdown","metadata":{"id":"pqaCw15v8EmB"},"source":["Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4."]},{"cell_type":"markdown","metadata":{"id":"pCrUvQIvoEKD"},"source":["## Create labeled video:\n","This function is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18966,"status":"ok","timestamp":1682274110382,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"},"user_tz":-120},"id":"6aDF7Q7KoEKE","outputId":"c51a8fae-2ab7-497d-cb4f-7ffdb23ed846"},"outputs":[{"output_type":"stream","name":"stdout","text":["Changing snapshotindext to the last one -- plotting, videomaking, etc. should not be performed for all indices. For more selectivity enter the ordinal number of the snapshot you want (ie. 4 for the fifth) in the config file.\n","Starting to process video: /content/drive/MyDrive/DeepLabCut2/outv2/B.mp4\n","Loading /content/drive/MyDrive/DeepLabCut2/outv2/B.mp4 and data.\n","Duration of video [s]: 49.04, recorded with 25.0 fps!\n","Overall # of frames: 1226 with cropped frame dimensions: 1280 720\n","Generating frames and creating video.\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 1226/1226 [00:19<00:00, 64.39it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[True]"]},"metadata":{},"execution_count":26}],"source":["deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"]},{"cell_type":"code","source":[],"metadata":{"id":"CMPZIcO5Jc2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deeplabcut.evaluate_network(path_config_file, plotting=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"eCJTJ5dVJcqK","executionInfo":{"status":"error","timestamp":1687371921694,"user_tz":-180,"elapsed":720618,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}},"outputId":"7bebe587-aff5-4ca6-a7ea-cdc5b9fedab4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Running  DLC_resnet50_HorsesMay8shuffle1_57500  with # of training iterations: 57500\n","This net has already been evaluated!\n","Plots already exist for this snapshot... Skipping to the next one.\n","Running  DLC_resnet50_HorsesMay8shuffle1_58000  with # of training iterations: 58000\n","This net has already been evaluated!\n","Plots already exist for this snapshot... Skipping to the next one.\n","Running  DLC_resnet50_HorsesMay8shuffle1_58500  with # of training iterations: 58500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["Running evaluation ...\n"]},{"output_type":"stream","name":"stderr","text":["740it [11:32,  1.07it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-0a2b44389952>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/core/evaluate.py\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(config, Shuffles, trainingsetindex, plotting, show_errors, comparisonbodyparts, gputouse, rescale, modelprefix)\u001b[0m\n\u001b[1;32m    821\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running evaluation ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mimageindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                             image = imread(\n\u001b[0m\u001b[1;32m    824\u001b[0m                                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"project_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mimagename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skimage\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/utils/auxfun_videos.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(image_path, mode)\u001b[0m\n\u001b[1;32m    369\u001b[0m     Returns frame in uint with 3 color channels.\"\"\"\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skimage\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                (plugin, kind))\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/v2.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mimopen_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"legacy_mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimopen_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                     \u001b[0mplugin_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mInitializationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;31m# file extension doesn't match file type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/config/plugins.py\u001b[0m in \u001b[0;36mpartial_legacy_plugin\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpartial_legacy_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mLegacyPlugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegacy_plugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mclazz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_legacy_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/legacy_plugin_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, request, legacy_plugin)\u001b[0m\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mIOMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 raise InitializationError(\n\u001b[1;32m     82\u001b[0m                     \u001b[0;34mf\"`{self._format.name}`\"\u001b[0m \u001b[0;34mf\" can not read `{source}`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mcan_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mformat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mread\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \"\"\"\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcan_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/plugins/pillow_legacy.py\u001b[0m in \u001b[0;36m_can_read\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPEN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugin_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstbytes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36mfirstbytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_first_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_read_first_bytes\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_firstbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_n_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m             \u001b[0;31m# Set back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36mread_n_bytes\u001b[0;34m(f, N)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mextra_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextra_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Convert To TFlite Model"],"metadata":{"id":"8F-nVLIVuvBc"}},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/DeepLabCut2/TFlite\")"],"metadata":{"id":"KNq-cFy0vNi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflowjs\n","#!tensorflowjs_converter --input_format=tfjs_layers_model --output_format=keras_saved_model /path/to/my_model.json /path/to/save/model/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sE7m86_LLK2E","executionInfo":{"status":"ok","timestamp":1682237698486,"user_tz":-120,"elapsed":60949,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}},"outputId":"19c82e66-6059-44c8-91e8-c0e93f416951"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflowjs\n","  Downloading tensorflowjs-4.4.0-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting packaging~=20.9\n","  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-decision-forests>=1.0.1\n","  Downloading tensorflow_decision_forests-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (5.12.0)\n","Collecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (1.16.0)\n","Requirement already satisfied: flax>=0.6.2 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (0.6.8)\n","Requirement already satisfied: tensorflow<3,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (2.12.0)\n","Collecting tensorflow-hub<0.13,>=0.7.0\n","  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.9/dist-packages (from tensorflowjs) (0.4.8)\n","Requirement already satisfied: tensorstore in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.35)\n","Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (6.0)\n","Requirement already satisfied: orbax in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.7)\n","Requirement already satisfied: optax in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (0.1.4)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (1.22.4)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (13.3.4)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (1.0.5)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from flax>=0.6.2->tensorflowjs) (4.5.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib_resources>=5.9.0->tensorflowjs) (3.15.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.16->tensorflowjs) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.16->tensorflowjs) (0.1.0)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.12.0)\n","Collecting tensorflow<3,>=2.10.0\n","  Downloading tensorflow-2.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.53.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.32.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.6.3)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (16.0.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (1.4.0)\n","Collecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.2.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (0.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (67.6.1)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (23.3.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (3.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3,>=2.10.0->tensorflowjs) (2.2.0)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from tensorflow-decision-forests>=1.0.1->tensorflowjs) (0.40.0)\n","Collecting wurlitzer\n","  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from tensorflow-decision-forests>=1.0.1->tensorflowjs) (1.5.3)\n","INFO: pip is looking at multiple versions of tensorflow-decision-forests to determine which version is compatible with other requirements. This could take a while.\n","Collecting tensorflow-decision-forests>=1.0.1\n","  Downloading tensorflow_decision_forests-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax>=0.6.2->tensorflowjs) (2.14.0)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.1->flax>=0.6.2->tensorflowjs) (2.2.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.17.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.27.1)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.4.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.2.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax->flax>=0.6.2->tensorflowjs) (0.4.7+cuda11.cudnn86)\n","Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from optax->flax>=0.6.2->tensorflowjs) (0.1.7)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.5.6)\n","Requirement already satisfied: etils in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.2.0)\n","Requirement already satisfied: cached_property in /usr/local/lib/python3.9/dist-packages (from orbax->flax>=0.6.2->tensorflowjs) (1.5.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->tensorflow-decision-forests>=1.0.1->tensorflowjs) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->tensorflow-decision-forests>=1.0.1->tensorflowjs) (2.8.2)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax>=0.6.2->tensorflowjs) (0.12.0)\n","Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax->flax>=0.6.2->tensorflowjs) (0.1.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (6.4.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax>=0.6.2->tensorflowjs) (0.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (2.1.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.10.0->tensorflowjs) (3.2.2)\n","Installing collected packages: wurlitzer, tensorflow-estimator, tensorboard-data-server, protobuf, packaging, keras, tensorflow-hub, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-decision-forests, tensorflowjs\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 23.1\n","    Uninstalling packaging-23.1:\n","      Successfully uninstalled packaging-23.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: tensorflow-hub\n","    Found existing installation: tensorflow-hub 0.13.0\n","    Uninstalling tensorflow-hub-0.13.0:\n","      Successfully uninstalled tensorflow-hub-0.13.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n","statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 packaging-20.9 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-decision-forests-1.2.0 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflowjs-4.4.0 wurlitzer-3.0.3\n"]}]},{"cell_type":"code","source":["deeplabcut.export_model(path_config_file, shuffle=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CjTZ5OxyWush","executionInfo":{"status":"ok","timestamp":1682240683883,"user_tz":-120,"elapsed":28334,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}},"outputId":"326e3149-5e45-4b8f-f3cc-45cb932cc1f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Snapshotindex is set to 'all' in the config.yaml file. Changing snapshot index to -1!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]}]},{"cell_type":"code","source":["\n","import tensorflow as tf\n","\n","def print_layers(graph_def):\n","    def _imports_graph_def():\n","        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n","\n","    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\n","    import_graph = wrapped_import.graph\n","\n","    print(\"-\" * 50)\n","    print(\"Frozen model layers: \")\n","    layers = [op.name for op in import_graph.get_operations()]\n","    ops = import_graph.get_operations()\n","    print(ops[0])\n","    print(\"Input layer: \", layers[0])\n","    print(\"Output layer: \", layers[-1])\n","    #print(\"-------------------------\")\n","    #for l in layers:\n","      #print(l)\n","    #print(layers)\n","    #print(\"-------------------------\")\n","    print(\"-\" * 50)\n","\n","# Load frozen graph using TensorFlow 1.x functions\n","with tf.io.gfile.GFile(\"/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/exported-models/DLC_Horses_resnet_50_iteration-0_shuffle-1/snapshot-105000.pb\", \"rb\") as f:\n","    graph_def = tf.compat.v1.GraphDef()\n","    loaded = graph_def.ParseFromString(f.read())\n","\n","frozen_func = print_layers(graph_def=graph_def)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sttzvcfieLQ2","executionInfo":{"status":"ok","timestamp":1682243679162,"user_tz":-120,"elapsed":2457,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}},"outputId":"e0f20994-662e-4251-e2d0-471c89e5928f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","Frozen model layers: \n","name: \"Placeholder\"\n","op: \"Placeholder\"\n","attr {\n","  key: \"dtype\"\n","  value {\n","    type: DT_FLOAT\n","  }\n","}\n","attr {\n","  key: \"shape\"\n","  value {\n","    shape {\n","      dim {\n","        size: -1\n","      }\n","      dim {\n","        size: -1\n","      }\n","      dim {\n","        size: -1\n","      }\n","      dim {\n","        size: 3\n","      }\n","    }\n","  }\n","}\n","\n","Input layer:  Placeholder\n","Output layer:  concat_1\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n","    graph_def_file = '/content/drive/MyDrive/DeepLabCut2/Horses-Byron-2019-05-08/exported-models/DLC_Horses_resnet_50_iteration-0_shuffle-1/snapshot-105000.pb',\n","    input_arrays = ['Placeholder'],\n","    output_arrays = ['concat_1']\n",")\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [\n","  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n","  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n","]\n","\n","tflite_model = converter.convert()\n","with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n","  f.write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"id":"yPAy0onJgynN","executionInfo":{"status":"error","timestamp":1682243861672,"user_tz":-120,"elapsed":12059,"user":{"displayName":"Abdullah Amin Marzok Rezk Ibrahim Madi","userId":"08210666173134863398"}},"outputId":"df68b392-4adf-4959-aabd-4aa207b75bc9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ConverterError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-486f96526a8d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2804\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \"\"\"\n\u001b[0;32m-> 2806\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2444\u001b[0m             \u001b[0;34m\"input_arrays_with_shape and output_arrays|control_output_arrays \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m             \"must be defined.\")\n\u001b[0;32m-> 2446\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLiteFrozenGraphConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[0;31m# Converts model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m       result = _convert_graphdef(\n\u001b[0m\u001b[1;32m   2084\u001b[0m           \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimized_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert_graphdef\u001b[0;34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m   data = convert(\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0mconversion_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0merror_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_metrics_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_collected_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   return _run_deprecated_conversion_binary(model_flags_str,\n","\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(fused[\"UnravelIndex:\", \"UnravelIndex\"]): 'tf.UnravelIndex' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"UnravelIndex:\", \"UnravelIndex\"]): Error code: ERROR_NEEDS_CUSTOM_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \nCustom ops: UnravelIndex\nDetails:\n\ttf.UnravelIndex(tensor<?xi64>, tensor<2xi64>) -> (tensor<2x?xi64>) : {Tidx = i64, device = \"\"}\n\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb","timestamp":1681940059168}],"collapsed_sections":["8F-nVLIVuvBc"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.12 ('dlc')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.12"},"vscode":{"interpreter":{"hash":"70cad038f2bddb56e8a0ba66c48b76ebce20579892bf83e71733a81977e3ceea"}}},"nbformat":4,"nbformat_minor":0}